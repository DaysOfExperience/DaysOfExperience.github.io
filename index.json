[{"categories":null,"content":"1_常见指令以及权限理解 ","date":"0001-01-01","objectID":"/linux1_%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E7%90%86%E8%A7%A3/:0:0","tags":null,"title":"","uri":"/linux1_%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E7%90%86%E8%A7%A3/"},{"categories":null,"content":"Linux 基础指令 ls pwd cd 改变工作目录 touch touch命令参数可更改文档或目录的日期时间，包括存取时间和更改时间，或者新建一个不存在的文件。 mkdir [-p] rm man 1 是普通的命令 2 是系统调用,如open,write之类的 3 是库函数,如printf,fread cp cp [选项] 源文件或目录 目标文件或目录 mv 移动文件/目录 重命名文件/目录 cat 查看目标文件的内容 less 功能更强大的浏览文件内容的工具 head tail find find pathname -options -name 按照文件名查找文件 grep [选项] 搜寻字符串 文件 -i ：忽略大小写的不同，所以大小写视为相同 -n ：顺便输出行号 -v ：反向选择，亦即显示出没有 ‘搜寻字符串’ 内容的那一行 zip [选项] 压缩文件.zip 目录或文件 -r 递归处理，将指定目录下的所有文件和子目录一并处理 tar 打包/解包 = = 暂时略了 ","date":"0001-01-01","objectID":"/linux1_%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E7%90%86%E8%A7%A3/:1:0","tags":null,"title":"","uri":"/linux1_%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E7%90%86%E8%A7%A3/"},{"categories":null,"content":"Linux 扩展指令 top ps su su [用户名] 切换用户 su root 时root可省略（Linux下的两种用户：超级用户root和普通用户） who chmod chown chgrp netstat ipconfig ","date":"0001-01-01","objectID":"/linux1_%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E7%90%86%E8%A7%A3/:2:0","tags":null,"title":"","uri":"/linux1_%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E7%90%86%E8%A7%A3/"},{"categories":null,"content":"Shell命令及其运行原理（Bash） ","date":"0001-01-01","objectID":"/linux1_%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E7%90%86%E8%A7%A3/:3:0","tags":null,"title":"","uri":"/linux1_%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E7%90%86%E8%A7%A3/"},{"categories":null,"content":"Linux权限 ","date":"0001-01-01","objectID":"/linux1_%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E7%90%86%E8%A7%A3/:4:0","tags":null,"title":"","uri":"/linux1_%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E7%90%86%E8%A7%A3/"},{"categories":null,"content":"2_Linux环境基础开发工具使用 ","date":"0001-01-01","objectID":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:0:0","tags":null,"title":"","uri":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"Linux 软件包管理器 yum ","date":"0001-01-01","objectID":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:1:0","tags":null,"title":"","uri":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"Linux开发工具 ","date":"0001-01-01","objectID":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:0","tags":null,"title":"","uri":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"Linux编辑器-vim使用 ","date":"0001-01-01","objectID":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:1","tags":null,"title":"","uri":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"Linux编译器-gcc/g++使用 预处理 编译 汇编 连接 函数库：静态库/动态库 ","date":"0001-01-01","objectID":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:2","tags":null,"title":"","uri":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"Linux调试器-gdb使用 ","date":"0001-01-01","objectID":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:3","tags":null,"title":"","uri":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"Linux项目自动化构建工具-make/Makefile Linux第一个小程序－进度条 ","date":"0001-01-01","objectID":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:4","tags":null,"title":"","uri":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"使用 git 命令行 ","date":"0001-01-01","objectID":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:5","tags":null,"title":"","uri":"/linux2_linux%E7%8E%AF%E5%A2%83%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"3_Linux进程概念 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:0:0","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"冯诺依曼体系结构 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:1:0","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"操作系统(Operator System) ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:2:0","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"进程 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:0","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"进程的基本概念 进程是正在运行的程序的实例（an instance of a computer program that is being executed） 在操作系统中运行的程序。程序的一个执行实例。 内核观点：承担分配系统资源（CPU时间，内存）的实体。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:1","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"描述进程 - PCB PCB ：Process Control Block 进程控制块 作用 : 用于描述操作系统中的进程的数据结构 为了描述控制进程的运行，系统中存放进程的管理和控制信息的数据结构称为进程控制块（PCB Process Control Block），它是进程实体的一部分，是操作系统中最重要的记录性数据结构。它是进程管理和控制的最重要的数据结构，每一个进程均有一个PCB，在创建进程时，建立PCB，伴随进程运行的全过程，直到进程撤消而撤消。- from百度百科 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:2","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"为什么要有PCB？PCB的作用？ 六个字：先描述，再组织。 这是一个管理的观念，比如我们写C++程序时，写的类就是为了描述某一个事物，而创建了类的实例化\u003e对象之后，用某一个数据结构将其组织起来，就叫做再组织。之后再使用某些算法就可以达到管理对象。 这个观念在程序开发和很\u003e多场景下都适用，而C++中的STL就是为了帮助我们更方便地组织实例化对象。 所以，在操作系统中，进程也是一个事物，操作系统要管理进程，那么最好的方法就是先描述，再组织。而描述，采用的方法就是利用一个strcut（Linux是用C写的，使用的是struct，其他操作系统未知）去存储进程的各种属性来描述进程。之后再在内存中，用数据结构将这些PCB（结构体）组织起来，来达到更好地管理进程的目的。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:3","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"task_struct - Linux下的PCB 在Linux操作系统中，描述进程的结构体叫做task_struct，Linux的PCB名为task_struct。它们的关系就像是：警察是一种职业，task_struct是一种PCB。 task_struct是Linux内核的一种数据结构，它会被装载到RAM(内存)里并且包含着进程的信息。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:4","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"task_ struct内容分类 标示符: 描述本进程的唯一标示符，用来区别其他进程。 状态: 任务状态，退出代码，退出信号等。 优先级: 相对于其他进程的优先级。 程序计数器: 程序中即将被执行的下一条指令的地址。 内存指针: 包括程序代码和进程相关数据的指针，还有和其他进程共享的内存块的指针 上下文数据: 进程执行时处理器的寄存器中的数据[休学例子，要加图CPU，寄存器]。 I／O状态信息: 包括显示的I/O请求,分配给进程的I／O设备和被进程使用的文件列表。 记账信息: 可能包括处理器时间总和，使用的时钟数总和，时间限制，记账号等。 其他信息 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:5","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"Linux下查看进程的方式 top ：Linux下的任务管理器，通常用来查看资源占用比较高的进程。对于查看某个进程的具体信息数据，使用ps更方便。 ps ：ps -ajx | head -1 \u0026\u0026 ps -ajx | grep process_name while :; do ps axj | head -1 \u0026\u0026 ps ajx | grep myproc | grep -v grep; sleep 1; done 循环进行ps查看进程，用于实时监控某个进程的信息。 /proc /proc 是Linux系统下的一个目录，存储着所有进程的信息。以每个进程的PID作为目录名来标识每个进程。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:6","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"PID/PPID - 进程标示符 每个进程都有一个ID，PID就类似于人的身份证号码。而PPID是该进程的父进程的PID。 pid_t getpid(void) get process id pid_t getppid(void) get parent process id (系统调用函数) bash：一种shell，是Linux下的命令行解释器，我们执行命令，./运行可执行程序都是bash通过创建子进程的方式进行的。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:7","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"通过系统调用创建进程-fork fork是一个操作系统接口函数，作用是创建子进程。fork函数，用于创建子进程，返回值是pid_t，如果创建子进程成功，则返回给父进程子进程的PID，返回给子进程0。如果创建子进程失败，则返回给父进程-1，errno被恰当地设置。 父子进程代码共享，数据各自开辟空间，私有一份（采用写时拷贝） 在fork函数内部，主体代码肯定是创建子进程，而return的时候，子进程已经创建出来了，那么，哪个进程先执行return呢？ 其实这个是不确定的。因为CPU执行一个进程的代码，是执行上若干时间，再换其他进程。只是这个若干时间非常非常小，并且速度非常块，凭人的主观意识感觉到的好像是多个进程同时执行。 所以，到底哪个进程先执行return是不确定的。这个是由操作系统的调度器决定的。 当然，创建子进程的时候，一定会在内存中创建与之对应的task_struct结构体。并且新建的子进程是以父进程为模板创建出来的，除了一些特有的值比如pid等属性，其他都是拷贝过来的。这里还有写时拷贝的知识…. ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:8","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"fork之后发生了什么？父子进程的资源如何划分？ ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:9","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"bash的作用，与shell的关系，工作原理??? ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:10","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"写时拷贝??? ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:3:11","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"进程状态 进程状态本质也是PCB中的一个字段。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:4:0","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"操作系统中的进程状态 新建：指的是进程刚被创建好，还没有被CPU调度的状态，事实上，Linux下并没有这种状态，而操作系统中有此状态也是为了整体更加严谨，完善。 运行：进程的task_struct被CPU调度或者在CPU的runqueue（运行队列）中等待被调度，都称为运行态（另一种说法是，被CPU调度为运行态，在运行队列中是就绪态） 阻塞：等待非CPU资源就绪的状态，例1，比如要进行文件读取，在硬盘的执行队列中等待硬盘的响应，就是阻塞状态。 例2，scanf时等待键盘的输入，也是阻塞状态，其实这个的场景还是非常多的。 挂起：类似于阻塞，情景比较少见，比较极端。当内存不足时，OS会置换长期不运行的进程的代码和数据到磁盘，而内存中只留进程的PCB，此时进程被称为挂起状态。 退出：进程结束的状态。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:4:1","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"Linux操作系统：进程状态 R 运行状态（running） 并不意味着进程一定在运行中(CPU执行进程代码)，它表明进程要么是在运行中要么在运行队列里。对应上方的运行态。 只运行死循环即可，但是前提是，循环内不可以有打印语句。 S 睡眠状态 (sleeping) 意味着进程在等待非CPU资源的就绪（有时候也叫做可中断睡眠，（interruptible sleep））。 当while循环不断printf时，显示的是睡眠状态，这里可能并不是我们所预料的，因为看上去这个进程一直在运行，事实上显示睡眠态的原因是，可能这个进程等待显示器资源就绪的时间比上CPU执行的时间是50：1，也就是CPU的执行速度是非常快的，大多数时间都是等待显示器的资源就绪。（当一直查看这个进程状态时，少数会显示出R） （对应的就是上方的阻塞状态） D磁盘休眠状态（Disk sleep） 有时候也叫不可中断睡眠状态（uninterruptible sleep），在这个状态的 进程通常会等待IO的结束。 可以理解为，有些进程当在和磁盘进行数据读写时（IO），不可以被中断，因为意外中断可能导致数据丢失等问题。 T停止状态（stopped） 可以通过发送 19号信号 SIGSTOP 给进程来停止（T）进程。这个被暂停的进程可以通过发送 SIGCONT 信号让进程继续运行。 t 停止状态(tracing stop) 本质上也是暂停状态，只是如果使用gdb调试某进程，并使进程在某断点处停下，进程就处于t状态，而本质上，断点就是程序在运行到某一行处时暂停了。 X死亡状态（dead） 这个状态只是一个返回状态，瞬时性非常强，你不会在任务列表里看到这个状态。它仅表示此进程已结束，告知OS，这个进程的数据可以回收了。 Z(zombie)-僵尸状态，僵尸进程 僵死状态（Zombies）是一个比较特殊的状态。当子进程退出并且父进程没有读取子进程退出码时子进程就是一个僵死(尸)进程 僵死进程会以终止状态保持在进程表中，并且会一直在等待父进程读取退出状态代码。 所以，只要子进程退出，父进程还在运行，但父进程没有读取子进程状态，子进程即进入Z状态。 事实上，每个进程正常退出时，都会经历很小的一段僵尸状态，这段时间内等待父进程读取这个子进程的退出码。 僵尸进程危害 父进程一直不读取子进程状态，子进程的Z状态就会一直维持下去。子进程Z状态一直不退出，这个进程的PCB就一直要维护。 如果永远不回收，就会造成内存泄漏，因为PCB就会一直存在于内存中，不被释放。 僵尸进程如何避免？ 进程等待，通过wait/waitpid来读取子进程退出码以及回收子进程资源。 SIGCHLD ???? 孤儿进程 子进程退出，父进程不读取子进程的状态。子进程为僵尸进程。 父进程退出，子进程变成孤儿进程。那么，如果这个进程不被领养，则此子进程退出后就会变成僵尸进程，且永远不会被释放内存导致内存泄漏。所以，子进程必须被领养，且是被1号进程领养。 可以看到，经过5s后，父进程退出，子进程被1号进程领养。 而当子进程退出后，就会由新的父进程：1号进程回收这个子进程。（图略 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:4:2","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"进程状态之间的转换流程 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:4:3","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"程序地址空间（虚拟地址空间） 虚拟地址空间的引出： 代码略了，子进程和父进程不断打印，子进程改变某全局变量的值，之后子进程与父进程打印全局变量的值不同，地址相同。因此引出父子进程打印的地址并非物理地址而是一种虚拟地址。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:5:0","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"Linux下进程虚拟地址空间分布 虚拟地址空间使得每个进程看待内存时都有一个统一的视角，并且在他们看来，内存的分布是井然有序的。 注：初始化数据和未初始化数据是全局数据区，这里存储的都是全局的初始化/未初始化数据。正文代码的内部的高地址处有一小段的字符常量区。这块数据（正文代码）都是只读的。 栈堆相向增长，堆向高地址增长，栈向低地址增长。这两个区域是动态变化的。 虚拟地址空间分为两个空间：1. 内核空间，在32位下占1G 2. 用户空间，在32位下占3G 即[0, 3GB] 用户空间 [3GB, 4GB] 内核空间 static修饰局部变量，本质上是将此变量属性变为全局属性，存储在全局区。而语法的限制使得此static变量仅能在局部可见。 上图虚拟内存分布仅适用于Linux操作系统，不适于Windows。 一个有关堆区的知识：当C语言使用malloc函数时，申请10字节空间，实际在内存中会占用大于10字节的空间，多出的空间用于存储一些属性。这也是为什么free时传首地址即可，而不需要传空间大小。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:5:1","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"什么是虚拟地址空间？（灵魂拷问 是一个mm_struct结构体，属于进程的内核数据结构，用于操作系统向进程描述一个完整的连续的线性地址空间。 虚拟地址空间（进程地址空间）在操作系统内核中是一个数据结构，在Linux内核中，就是一个struct结构体 在Linux下，进程地址空间是一个名为mm_struct的结构体，主要存储各区域（堆，栈，全局数据区，只读代码区等）的范围，即start和end，用于划分各个区域。 页表是和虚拟地址空间结构体配套的内核数据结构，页表的作用是：保存对应进程中每一个虚拟地址到物理内存中的物理地址的映射关系。即起一个映射配对的作用。 因为实际上数据，代码，变量等肯定最终要存储在物理内存中。 页表起映射作用，就类似于C++中的map数据结构，key 是虚拟地址， value是对应的物理地址 每一个进程都有一份地址空间结构体变量mm_struct和页表实例化对象。在磁盘中的二进制可执行程序加载到内存中时，要创建对应的PCB结构体，同时，也会创建对应的地址空间结构体变量和页表。 虚拟地址出现之前：进程直接访问物理内存 在早期计算机操作系统内部，进程直接访问物理内存。这样做有很多弊端。在说弊端之前，有一个点需要明确：内存本身是可以随意读写的，物理内存是不存在只读的情况的。 比如，最典型的野指针问题，一个进程的野指针很容易破坏其他进程，甚至影响操作系统内的安全数据。其次，进程直接访问物理内存，使得进程和物理内存耦合度很大，内存管理变得不方便，从而内存碎片等问题也变得更难处理。 基于进程直接访问物理内存的弊端，衍生出虚拟地址空间。 PCB中有一个struct mm_struct* mm指针数据成员指向这个进程对应的mm_struct结构体（进程地址空间） 因为内存本身是随意读写的，所以，在地址空间+页表的作用下，可以在某些虚拟地址与物理地址的映射关系中，用某些数据（比如页表中存储）表明这个内存是只读的。以此来保护某些数据。这些都是地址空间+页表的作用，而非内存的权限控制。 基于第二点，地址空间+页表可以对某些内存进行权限管理，比如常量代码区设为只读。同时，对于某些内存的非法访问，也可以及时禁止。从而保护物理内存。 我们知道，进程是具有独立性的，那么，在地址空间+页表的作用下，只要使得各个进程的虚拟地址通过页表映射的物理内存是不同的，则可以保证进程之间互不干扰，即进程独立性。 虚拟地址空间结构体是如何进行区域划分的呢？ 通过定义栈区，堆区，常量代码区，全局数据区等区域的start，end。来对这些区域进行划分。 比如栈区，堆区是动态变化的。那么只需要增大或者减小end，即可对栈区堆区的空间大小进行控制。再比如只读代码区，在源文件编译之后，可执行程序内部已经有了虚拟地址。若此文件加载到内存中变为进程，则mm_struct中的常量代码区的start和end即可通过这些编译生成的虚拟地址来确定start和end 最初，父进程与子进程打印出同一个变量地址相同，但是值不同。我们现在知道了，这个地址其实是虚拟地址，而非物理地址。 一个事实：父进程创建子进程时，除了一些子进程独有的属性，比如典型的pid。其余大部分属性和数据都是从父进程那里拷贝过来的。包括mm_struct 和 页表 的数据！ 所以，起初，在子进程执行g_val = 200;之前，也就是修改这个全局变量之前。因为子进程的mm_struct和页表是直接从父进程那里拷贝过来的。故父子进程的g_val的虚拟地址，以及这个虚拟地址映射的物理内存中的数据都是一样的。这样做的原因是：如果有某些数据，父子进程都是只读的，也就是不会修改，那么这份数据在内存中只保存一份即可，没必要给子进程在内存中再创建一份相同的，只读的数据。 而当子进程执行g_val = 200;时，这是子进程对这个全局数据执行写操作。因为父子进程访问的g_val不应该互相干扰。故此时，OS在内存中的其他区域，拷贝了一个新的，子进程的g_val，赋值为200，并改变子进程的页表的映射关系（不需要改变g_val的虚拟地址）。从而当子进程改变g_val后，父子进程打印的这个全局数据的虚拟地址相同，但是映射到物理内存不同区域，值不相同。才有了最初的现象。 这种子进程写数据时进行拷贝的操作，称为写时拷贝！ 本质上，fork的返回值，由一个变量来存储，但是在父子进程内部存储的值是不同的（父进程存储子进程id，子进程存储0），其实这里也是写时拷贝！ ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:5:2","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"为什么存在虚拟地址空间？（虚拟地址空间的好处 一、保护物理内存，增加内存访问控制 凡是非法的访问或者映射，OS都会识别到，并终止你的进程。 原因就是：比如const char* p = “abcd”; p指针保存的是虚拟地址，且这个虚拟地址所映射的物理内存不可被写。这都是基于地址空间+页表。除了这种保护只读的数据，当存在野指针或者非法访问时，地址空间+页表也能以某种方式告诉OS，从而OS可以发信号终止这个进程。这样一来，物理内存的访问都在OS的监管之下。保护了物理内存，物理内存中的数据，其他进程，以及内核的相关有效数据。 二、内存管理 和 进程管理 低耦合，降低维护成本 因为有虚拟地址空间+页表，物理内存中的数据可以随意存储。只要保证虚拟地址可以通过映射找到对应的数据即可 物理内存管理 和 进程管理因此可以做到关联性很低 内存管理模块和进程管理模块 完成了解耦合。在操作系统层面，这两个模块关联性很低，维护成本也会降低 三、实现延时分配，提高整机效率。同时提高内存利用率 我们在C语言中进行malloc时，申请内存本质是在虚拟地址空间中申请，并不会因为malloc，就立即申请内存空间。 原因是：如果我malloc时就立刻申请物理内存，且不立刻使用，则这就是一种内存资源浪费。 所以，因为有地址空间存在，上层申请内存，其实是在地址空间中申请。而当你进行对物理内存的访问时，才执行相关的内存管理算法，帮你申请内存，构建页表映射关系。然后再让你进行内存访问。 （这些是由操作系统完成的，进程0感知） 虚拟地址存在，但是物理内存中没有对应的空间。称为缺页中断。 这样延时分配产生的好处就是：确保物理内存中的有效使用是100%的，并不会出现物理内存中申请空间但不使用的情况。提高内存的利用率。延时分配的策略，提高整机效率。 四、使内存分布有序化、实现进程独立性 第二点中：物理内存可以随意存储。但是因为页表的存在，可以建立虚拟地址和物理地址的映射关系。**从而使得从每个进程的视角看来，内存分布都是有序的。**地址空间+页表的存在，可以使得内存分布有序化！ 通过让不同内存的虚拟地址映射到物理内存的不同区域, 实现进程独立性。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:5:3","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"如何通过虚拟地址找到物理地址？ ….???????? 重新理解挂起 创建进程时，并非一定需要将程序所有的代码和数据都加载到内存中，并创建内核数据结构，才能执行。 极端情况下，当只有内核数据结构被创建出来时，就是新建状态。 理论上，可以实现对程序的分批加载。同样可以实现程序的数据/代码的分批唤出。 当一个进程等待某些响应时，或者短时间内不会再执行了，比如阻塞了。 此时将进程的数据和代码换出，就叫做挂起。 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:5:4","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"进程优先级 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:6:0","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"。。。？？？？？？ ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:6:1","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"环境变量 ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:7:0","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"。。。。？？？？？？ ","date":"0001-01-01","objectID":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/:7:1","tags":null,"title":"","uri":"/linux3_linux%E8%BF%9B%E7%A8%8B%E6%A6%82%E5%BF%B5/"},{"categories":null,"content":"4_Linux进程控制 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:0:0","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"进程创建 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:1:0","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"fork创建子进程做了什么？ 进程 = 内核数据结构 + 进程的代码和数据 =\u003e 内核数据结构是操作系统创建的。而进程的代码和数据，一般是从磁盘中加载到内存而来。=\u003e fork创建子进程，由操作系统为子进程创建对应的内核数据结构：PCB，进程地址空间对象，页表等。虽然子进程内核数据结构中的大部分数据是从父进程那里拷贝来的，但是因为进程的独立性，这部分内核数据结构确实是子进程独有的。=\u003e 而子进程对应的代码和数据，并不是从磁盘中加载而来。而是直接和父进程”共享一份代码和数据“，可是，基于进程的独立性，父子进程共享一份代码和数据是如何实现的呢？=\u003e 对于代码：代码具有只读属性，也就是父子进程都不会修改，那么共享一份代码是完全可以的，同时这样还节省了内存空间 =\u003e 对于数据：事实上，子进程在创建初期，确实和父进程共享一份数据。 基于进程独立性，本身，父子进程的数据必须分离。那么在子进程创建初期，直接在内存中拷贝一份父进程的数据是完全可以的。 但是这样有很大的内存浪费的风险。=\u003e 父进程的数据，并非所有子进程都需要使用。 即使需要使用，也不会立刻使用。 即使立刻使用，也不一定是写操作。=\u003e 基于高效和节省内存空间的理念，我们不能采取直接拷贝一份父进程的数据。=\u003e 故我们在子进程创建时，是和父进程共享同一份数据。=\u003e 对于只读的，父子进程共享数据完全可以。对于父进程写的，或者子进程写的数据。基于进程独立性，必须进行数据分离。 操作系统采用的即 写时拷贝技术，来将父子进程数据分离。 进程调用fork，当控制转移到内核中的fork代码后，内核做： 分配新的内存块和内核数据结构给子进程 将父进程部分数据结构内容拷贝至子进程 添加子进程到系统进程列表当中 fork返回，开始调度器调度 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:1:1","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"父子进程分离数据的方式：写时拷贝 写时拷贝，在fork创建子进程这里的具体操作是：创建子进程之后，当父子进程的任何一方以写方式访问某个数据时，操作系统在内存的其他区域拷贝一份这个数据，并修改写此数据的那个进程的页表映射关系，使父子进程访问的是物理内存中的不同数据。以达到父子进程数据分离的效果，避免互相干扰。 写时拷贝好处： 在对数据进行写入时，再拷贝数据，是高效使用内存的一种表现，提高内存使用率，可以提高整机的运行效率。 写时拷贝还是一种延时申请的技术。提高内存使用率。 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:1:2","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"有关程序计数器和EIP寄存器： 一个事实：在fork创建子进程之后，父子进程的代码是共享的。那么为什么子进程是从fork内部的return或者说fork之后的代码开始执行呢？ 我们知道，子进程的创建是以父进程为模板的，比如大部分内核数据结构，代码，数据等。包括进程地址空间，页表。都是从父进程那里拷贝过来的。=\u003e 进程中的代码指令，每条代码指令都有地址，在CPU内部的寄存器中，有一个名为EIP的寄存器，称为程序计数器，Program Counter，简称PC。用于存放下一条需要执行的指令所在单元的地址。而CPU执行一个进程的代码指令就是依靠程序计数器。大致过程为：取指令（依据PC），分析指令（指令集架构ISA），执行指令。=\u003e 同时，进程并非一次就将全部的指令都执行完，而是执行若干时长，就需要切换为其他进程。切换前，之前的进程需要保存好程序计数器中的数据和其他相关数据。称为进程的上下文数据。以便下次切换为此进程时，可以继续上次CPU执行指令的进度。=\u003e 而fork创建子进程时，子进程的上下文数据都是从父进程那里拷贝过来的。所以，当子进程执行时，程序计数器中保存的指令地址致使子进程从父进程执行fork那里开始执行。同时fork之前的代码子进程是可见的，只是不是从头开始执行。 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:1:3","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"fork常规用法 一个父进程希望复制自己，使父子进程同时执行不同的代码段。例如，父进程等待客户端请求，生成子 进程来处理请求。 一个进程要执行一个不同的程序。例如子进程从fork返回后，调用exec函数。（比如OJ项目中，子进程通过execl来进行g++或者运行生成的可执行程序来获取运行结果） ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:1:4","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"fork什么时候会失败？ 系统中有太多的进程 实际用户的进程数超过了限制 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:1:5","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"vfork 创建一个子进程，共享同一个虚拟地址空间 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:1:6","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"进程终止 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:2:0","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"进程终止时，操作系统做了什么？ 释放进程相关内核数据结构和对应的代码和数据。 即释放进程占用的资源。 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:2:1","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"进程终止的分类情况 代码跑完，运行结果正确（符合预期） 代码跑完，运行结果不正确（不符合预期） 代码没有跑完，程序异常退出（崩溃） ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:2:2","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"进程退出码 进程退出码，是当进程执行结束后，用于标识运行结果的。 通常情况下，0标识success，非0标识结果不正确。 非零值有无数个，用不同的非零值来标识不同的错误原因。所以，依据进程退出码，可以判断上方情况的1 和 2 当进程异常退出时（崩溃），是由操作系统发送信号给进程以实现的。此时，退出码没有意义。 进程的退出码是返回给上一级进程（父进程）的，用于上一级进程获取该进程执行结果用的。（如果上一级进程不关心，则可以忽略） C语言中定义了一套将退出码/错误码转化为字符串描述的方案，你可以设定一套自己的用于程序中。strerror库函数可以查询C语言规定的退出码对应的退出原因。 Linux下，echo $?可查询上一个进程执行的退出码 其中C/C++程序中，main函数的return值就是该进程（程序）的退出码，我们通常写的return 0即表示当main函数运行结束时，退出码为0标识结果成功。 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:2:3","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"用代码如何终止一个进程 main函数中的return语句，即标识终止此进程。（其余函数的return仅表示此函数的结束，或者返回一个值给该函数的函数调用处） void exit(int status); C语言库函数，返回status作为退出码，并终止此进程 void _exit(int status); 操作系统系统接口函数，返回status作为退出码，并终止此进程 main函数中执行return n等同于执行exit(n)，因为调用main的运行时函数会将main的返回值当做exit的参数。 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:2:4","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"exit vs _exit 一个是C语言库函数，一个是操作系统接口函数。 区别：exit在程序退出之前，会刷新C标准库设定的缓冲区。执行用户通过atexit或on_exit定义的清理函数。关闭所有打开的流，所有的缓存数据均被写入。 最后调用_exit！ ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:2:5","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"进程等待 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:3:0","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"需要进行进程等待的原因 一、子进程退出之后，父进程不读取子进程的退出状态，不回收其资源。子进程变为僵尸进程。则子进程会造成内存泄漏。 二、 除了回收子进程的资源，如果父进程想读取子进程的退出码。也需要通过进程等待的方式获取。（子进程的运行结果正确与否） 总之：父进程通过进程等待的方式，回收子进程资源，获取子进程退出信息。 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:3:1","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"父进程进程等待的方式 pid_t wait(int *status); pid_ t waitpid(pid_t pid, int *status, int options); wait只能阻塞式等待，waitpid可以选择等待方式，依据的是最后一个参数：0 or WNOHANG。 status是输出型参数，由操作系统填充，用于得到子进程的执行结果：正常 or 崩溃（看信号），正常的话，是否正确（看退出码） status参数类型是int*，但是，并非以整型整体来获取子进程执行结果。而是按照比特位的方式。 若正常终止，次低八位，表示退出码。 (status»8) \u0026 0xff 若崩溃，则低7位为终止信号。status \u0026 0x7f 0x7f = 1111111 通过以上方式，即可得到status所标识的子进程退出信号 or 子进程退出码 进程PCB中是有对应的字段来保存进程的退出码和退出信号的，int exit_code, exit_signal; 多进程程序基本方式就是 fork + wait/waitpid ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:3:2","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"进程程序替换 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:4:0","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"进程程序替换的概念+原理 用fork创建子进程后，可以用进程替换的方式，通过函数调用让子进程去执行一个全新的进程。当进程调用一种exec函数时，物理内存中保存的该进程的数据和代码将全部被替换为新的进程的代码和数据。之后，子进程会将新的进程完整执行一遍！ 进程替换，并没有创建新的进程，而是完全替换了一个已存在进程的代码+数据为新进程的代码+数据。所以进程的id，优先级等不会变 并且，fork之后父子进程的代码共享，数据写时拷贝。那么，子进程进行进程替换时，数据和代码被替换，本质就是数据和代码被写入。所以此时代码也会发生写时拷贝。再将子进程的代码和数据替换为新进程的代码和数据。以此保证父子进程的独立性！！！ exec系列函数本质上，就是把新的进程的代码和数据加载到内存中！ ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:4:1","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"exec系列-进程替换接口函数 int execl(const char *path, const char *arg, ...); int execlp(const char *file, const char *arg, ...); int execle(const char *path, const char *arg, ..., char * const envp[]); int execv(const char *path, char *const argv[]); int execvp(const char *file, char *const argv[]); int execvpe(const char *file, char *const argv[], char *const envp[]); int execve(const char *path, char *const argv[], char *const envp[]); l(list) : 表示arg命令行参数采用列表形式 v(vector) : 表示arg命令行参数采用数组形式 p(path) : 有p自动搜索环境变量PATH e(env) : 表示自己维护组装环境变量 第一个参数，传递要替换为的进程的路径+目标文件名，相对路径或绝对路径。 而如果函数名有p表示，第一个参数还可以传递环境变量里保存好的，比如ls，pwd。若没p，则必须传递绝对/相对路径 第二个参数，为命令行参数，在命令行上怎么执行这个进程，这里就怎么填。以列表形式 or 数组形式 第三个参数：若函数名中有e，则表示自己维护环境变量（有关这里环境变量的作用，不太理解，只能勉强用一用）exec系列函数若调用失败返回-1，若调用成功没有返回值。（也无法返回） execve是真正的系统调用函数（man 2），上面的六个函数都是对execve的封装，适用于不同的使用场景。（man 3） 本质上，第一个参数，传路径是为了让exec找到这个程序在哪里。后面args为命令行参数，意思是怎么执行这个参数。而命令行参数，比如以NULL结尾。 一个C程序可以fork/exec另一个程序，并传给它一些参数。这个被调用的程序执行一定的操作，然后通过exit(n)来 返回值。调用它的进程可以通过wait（\u0026ret）来获取exit的返回值。 ","date":"0001-01-01","objectID":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/:4:2","tags":null,"title":"","uri":"/linux4_linux%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/"},{"categories":null,"content":"C程序编译链接的过程 程序翻译的过程 源文件生成可执行程序的过程 ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:0:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"1、预处理 预处理的作用/工作： 宏替换，头文件展开，条件编译，去注释。 [yzl@VM-4-5-centos testdir]$ ll total 4 -rw-rw-r-- 1 yzl yzl 314 Jul 30 17:14 test.c [yzl@VM-4-5-centos testdir]$ gcc -E test.c -o test.i [yzl@VM-4-5-centos testdir]$ ll total 24 -rw-rw-r-- 1 yzl yzl 314 Jul 30 17:14 test.c -rw-rw-r-- 1 yzl yzl 17030 Jul 30 17:15 test.i gcc -E test.c -o test.i 选项“-E”，该选项的作用是让 gcc 在预处理结束后停止编译过程。选项“-o”是指定目标文件,“.i”文件为已经过预处理的C原始程序。 上图中左边为预处理之后的test.i文件，可以看出预处理做了哪些操作。 即宏替换，头文件展开，条件编译，去注释。 ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:1:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"2、编译 在这个阶段中,gcc 首先要检查代码的规范性、是否有语法错误等，以确定代码的实际要做的工作,在检查无误后 gcc 把代码翻译成汇编语言。 gcc -S test.i -o test.s 进行程序的翻译，如果编译完成，就停下来，并生成test.s文件 -S 该选项只进行编译而不进行汇编,生成汇编代码。 [yzl@VM-4-5-centos testdir]$ gcc -S test.i -o test.s [yzl@VM-4-5-centos testdir]$ ll total 28 -rw-rw-r-- 1 yzl yzl 328 Jul 30 17:42 test.c -rw-rw-r-- 1 yzl yzl 16991 Jul 30 17:50 test.i -rw-rw-r-- 1 yzl yzl 745 Jul 30 17:51 test.s ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:2:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"3、汇编 汇编 ： 把汇编语言test.s 翻译为可重定向二进制目标文件 test.o，里面存储的是二进制目标代码。 gcc -c test.s -o test.o 进行程序的翻译，如果汇编完成，就停下来，并生成test.o文件 [yzl@VM-4-5-centos testdir]$ gcc -c test.s -o test.o [yzl@VM-4-5-centos testdir]$ ll total 32 -rw-rw-r-- 1 yzl yzl 328 Jul 30 17:42 test.c -rw-rw-r-- 1 yzl yzl 16991 Jul 30 17:50 test.i -rw-rw-r-- 1 yzl yzl 1576 Jul 30 17:53 test.o -rw-rw-r-- 1 yzl yzl 745 Jul 30 17:52 test.s 利用hexdump工具，可以看到test.o里的二进制内容，此时test.o是一个可重定向二进制目标文件 ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:3:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"4、链接 成功编译之后，进入链接阶段。 链接是将多个.o(linux).obj(win)可重定向二进制目标文件和静态库/动态库链接起来，生成可执行程序 gcc test.o -o test 也可以 gcc test.c -o test 只是一个从test.o目标文件开始翻译，一个是从.c文件开始从头翻译。最终都生成可执行文件 ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:4:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"总结 以.c程序为例，无论是Linux下，还是Windows的vs2019下，都要遵循这个程序翻译的过程： 预处理，编译，汇编，链接。我们的头文件在预处理之后，就已经在.c/.cpp文件中展开了。而gcc这个工具可以做到做完某个流程之后停下来。指令就是-E -S -c 分别对应完成预处理，完成编译，完成汇编。 生成的文件依次为.i .s .o ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:5:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"动态库 / 静态库 任何一个C程序或者C++程序都离不开动态库或者静态库。在之前VS2019这样的IDE中，其实每次编译运行程序都用到了动态库，只是我们把关注点放在了编写代码上，对于程序的翻译过程没有研究，从而忽略了这个点。 当我们在.c文件中使用printf这样的库函数时，需要包头文件，我们知道，头文件提供库函数的声明，而库函数的定义就是在动态库/静态库中。 静态库是指编译链接时,把库文件的代码全部加入到可执行文件中,因此生成的文件比较大,但在运行时可以独立运行，不再需要库文件。Linux下静态库后缀名一般为“.a” 动态库后缀名为“.so” 动态库与之相反,在编译链接时并没有把库文件的代码加入到可执行文件中,而是在程序执行时由运行时链接文件 加载库,这样可以节省系统的开销。动态库一般后缀名为“.so”, gcc 在编译时默认使用动态库。 ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:6:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"gcc / g++ 上面讲的是文件的编译链接过程，中间穿插了gcc的命令行使用方法。 gcc/g++ 默认形成的可执行程序是动态链接的！ [yzl@VM-4-5-centos testdir]$ ldd test linux-vdso.so.1 =\u003e (0x00007ffdd6fb0000) /$LIB/libonion.so =\u003e /lib64/libonion.so (0x00007f695bc3c000) libc.so.6 =\u003e /lib64/libc.so.6 (0x00007f695b755000) libdl.so.2 =\u003e /lib64/libdl.so.2 (0x00007f695b551000) /lib64/ld-linux-x86-64.so.2 (0x00007f695bb23000) [yzl@VM-4-5-centos testdir]$ file test test: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.32, BuildID[sha1]=adae662af334b0bcb43e4bfacc2fe0559dd66250, not stripped 如上，.so就是Linux下动态库文件的后缀，下方的dynamically linked就是表明动态链接。 指定生成静态链接的可执行程序 加-static即可指定采用静态链接的方式编译链接源文件。可以明显看出静态链接的可执行程序比动态链接的可执行程序大很多。 [yzl@VM-4-5-centos testdir]$ gcc test.c -o test_static -static [yzl@VM-4-5-centos testdir]$ ll total 888 -rwxrwxr-x 1 yzl yzl 8384 Jul 30 21:04 test -rw-rw-r-- 1 yzl yzl 328 Jul 30 17:42 test.c -rw-rw-r-- 1 yzl yzl 16991 Jul 30 17:50 test.i -rw-rw-r-- 1 yzl yzl 1576 Jul 30 17:53 test.o -rw-rw-r-- 1 yzl yzl 745 Jul 30 17:52 test.s -rwxrwxr-x 1 yzl yzl 861240 Jul 30 21:50 test_static [yzl@VM-4-5-centos testdir]$ ldd test_static not a dynamic executable [yzl@VM-4-5-centos testdir]$ file test_static test_static: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 2.6.32, BuildID[sha1]=aab0a9e371c993a62c024b75d99c488c7bd1d3e0, not stripped 静态库与动态库的概念 静态库（.a）：程序在编译链接的时候把静态库库的二进制代码拷贝到程序代码中。程序运行的时候将不再需要链接外部静态库。 生成的可执行程序中自己的代码和静态库的代码是一体的，进程运行起来之后，全部都在地址空间的代码区。 动态库（.so）：程序在运行的时候才去链接动态库的代码，不需要将动态库拷贝到程序的代码中，多个程序可以共享使用动态库的代码。 静态链接与动态链接 静态链接： 首先要明白的是：main函数所在的.o文件可以和其他.o文件一起编译（链接），生成可执行文件。**而静态库中有多个obj文件，将这些obj文件与main函数所在的obj文件一起编译，这个过程就叫做静态链接。**静态链接需要编译除自己之外的其他obj文件，生成的可执行文件会占用更多的空间 动态链接： 使用动态库的可执行文件含有一张动态符号表，表中包含了需要用到的函数符号。动态库也有一张动态符号表，包含了动态库的函数符号。 程序运行时，系统将动态库从磁盘加载到内存。同时，动态链接器根据符号表将程序与动态库之间建立绑定。一个动态库可以绑定多个程序的动态符号表，也就是可以被多个程序使用。使用动态库的程序，编译生成的可执行文件将占用更少的空间。 关于动态链接器的链接过程：符号解析 + 重定位 动态链接器根据程序中的符号引用和动态库中的符号定义生成一张符号表，用来记录程序中每个符号在内存中的地址 动态链接器遍历程序的重定位条目，根据符号表中的地址信息，修改程序中符号的引用，使它们指向正确的内存位置 动态链接器调用动态库的初始化函数，完成必要的初始化工作 动态链接器将控制权交给程序的入口点，使程序开始执行 结合虚拟地址空间 理解静态库和动态库的使用 静态库在你的源文件编译汇编，最后链接时，已经进入可执行程序内部了，生成可执行程序之后，后期执行此可执行程序，不再需要静态库文件。可以独立运行，所有代码都存在于进程地址空间的代码段中。 动态库和可执行程序可以分批加载，先运行可执行，若需要执行动态库中的函数代码时，从磁盘加载动态库代码到内存中，建立进程地址空间中共享区（栈堆之间的那部分区域），页表，内存中的动态库代码的映射关系。执行动态库代码时，从地址空间中的代码段跳转到共享区，然后通过页表映射执行内存中的动态库代码即可。 这也是为什么动态库的.o文件编译时，要加-fPIC选项，fPIC表示生成的.o文件中的地址，是一种与位置无关的地址，相对地址。编动态库代码的地址时，都是从0开始编址。 因为动态库的使用机制。动态库是一种共享库，动态库可以被多个进程共享，同时使用。所以动态链接使得可执行文件更小，节省了磁盘空间。操作系统采用虚拟内存机制允许物理内存中的一份动态库被要用到该库的所有进程共用，节省了内存和磁盘空间。 即，如果多个进程需要用到内存中的一个共享库，则利用虚拟地址空间机制，建立映射关系即可。 而静态库则不是，若有十个进程使用同一个静态库，同时运行，则内存中将会有10份一样的静态库代码。这无疑是一种内存资源浪费。 结合动态库是一种共享库，以及用gcc -c 汇编动态库源文件时要加-fPIC选项，这个选项表示动态库编址是一种相对地址。如何理解呢？ 多个进程使用同一个动态库时，每一次动态库被加载到内存，映射到进程的地址空间的共享区后，映射的位置可能是不一样的。 但是，因为动态库内使用的是相对地址，所以，函数定位可以采用 库在地址空间中的起始虚拟地址 + 函数偏移量的方式，确定函数的虚拟地址。也就是动态库的编址为一种相对地址。 而静态库和自己的.o文件链接合并，使用的虚拟地址是一种绝对地址。 重在理解。 gcc规则使用动静态库的规则： gcc默认使用动态库，进行动态链接，若只有静态库，gcc只能对该库进行静态链接。 若动静态库同时存在，则gcc默认使用动态库 若动静态库同时存在，-static 即可指定使用静态库，进行静态链接。 制作静态库 如上，两个.c 两个.h .h头文件包含库里面的函数声明 .c文件包含函数定义。 将源文件，预处理，编译，汇编为.o可重定向二进制目标文件。 gcc -c mymath.c -o mymath.o gcc -c myprint.c -o myprint.o 将所有.o文件归档为静态库（静态库文件）至此，静态库就生成好了 ar -rc *.o -o libplus.a （静态库文件的格式: libxxx.a） 创建一个文件夹（目录），比如名为plus，将所有的头文件放在文件夹的include子目录下，将所有归档打包好的静态库放在plus的lib子目录下。 至此，这个plus文件就可以用来发布或者发给别人使用了。 制作动态库 还是基于 myprint.h myprint.c mymath.h mymath.c gcc -fPIC -c myprint.c -o myprint_d.o gcc -fPIC -c mymath.c -o mymath_d.o 编译.c文件生成.o可重定向二进制目标文件时，加-fPIC指令，表示与位置无关地生成.o文件（这块和动态库使用的本质有关） gcc -shared mymath_d.o myprint_d.o -o libplus.so 不用ar，用gcc -shared即可。将.o文件合并归档打包生成libxxx.so动态库 （这里若不加-shared，就等于生成可执行程序了） 可以将.h放在plus/include/下 将.so放在plus/lib/下，这样plus文件就可以用来发布或者传给使用库的人了。 使用静态/动态库 首先，无论是动态库还是静态库，源程序都需要包含头文件（函数的声明）。这样做可以使得程序编译成功，但是无法进行链接。 ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:7:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"静态库 编译器在链接时，会在/usr/lib64这个默认路径下搜索源程序需要链接的库，在/usr/include这个路径下搜索需要使用的头文件。通常我们会用到标准库，所以编译器会默认在/usr/include和/usr/lib64这两个路径下查找头文件与标准库并且链接标准库。若源程序需要使用第三方库，就需要告知编译器：头文件所在路径、需要链接的库及其所在路径。 若第三方库和第三方头文件已经添加到/usr/lib64和/usr/include目录下，那么编译源文件时，只要带上-l 选项，告知编译器要静态链接的库。比如gcc test.c -l myfirst -o test 要注意的是：一般库文件名称都含有\"lib\"的前导和\".so\"或\".a\" 的后缀。比如libmyfirst.so和libmyfirst.a，使用-l选项时，需要去除\"lib\"前导与\".*“的后缀。比如libmyfirst.so写为myfirst （我们将库拷贝到系统默认路径下，就叫做库的安装。） 若第三方库和第三方头文件没有添加到默认路径下，那么就需要使用-I (头文件所在路径)和-L (库文件所在路径)告知编译器库文件与头文件所在的位置。比如gcc test.c -L ../lib-static/lib -I ../lib-static/include -l myfirst依次为库文件所在路径，头文件所在路径，要链接的库。（不会污染系统默认路径下的环境） Linux下，头文件gcc默认搜索路径：/usr/include 库文件默认搜索路径： /lib64 or /usr/lib64 ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:8:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"动态库 同样，使用动态库有两种方法，一种是简单的向默认路径添加库文件与头文件，编译时只使用-l选项。另一种使用-L、-I、-l三个选项进行编译。注意：对于后者，源程序依然可以编译成功，但是程序无法执行，一旦执行就会崩溃。 ./haha_dong: error while loading shared libraries: libplus.so: cannot open shared object file: No such file or directory 原因是：虽然编译器知道源程序依赖的库文件所在路径，编译器根据库文件生成符号表。但是程序加载时，操作系统需要进行动态链接 ，这个过程需要加载动态库到内存并修改符号表，使其指向正确的内存位置。然而进行动态链接的动态链接器不知道动态库所在的位置（磁盘中的位置），动态链接的过程无法进行。 运行时动态链接出错的解决方法： 采用第一种方法，将动态库文件添加到默认路径下。因为程序运行过程中，需要动态库时，会自动去系统默认路径下查找动态库。这里不需要头文件，因为生成可执行的时候（预处理），头文件已经在你的源文件内部展开的。 向环境变量LD_LIBRARY_PATH导入动态库路径（用export设置）。动态链接时，链接器除了会在默认路径下查找库文件，还会解析LD_LIBRARY_PATH中的路径，查找变量中的路径。不过系统一旦重新，环境变量重置，需要重新配置才能使程序正确运行。因为环境变量的初值是通过系统的配置文件获取的。 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/yzl/aandso/plus2/lib 修改系统配置文件。/etc/ld.conf.so.d/目录下存储了系统的配置文件，每个配置文件存储了一个路径，将库文件所在的绝对路径为内容创建我们的配置文件，并添加到该路径下。添加完成使用sudo ldconfig指令，加载配置文件即可 与第一种方式类似，向默认路径下以绝对路径的方式添加软连接。使用指令sudo ln -s /home/xx/xxx/lib-dynamic/libmyfirst.so /usr/lib64/libmyfirst.so 这两个路径以此为：库文件的绝对路径 需要在系统中创建的软链接文件 综上，其实所有方法的目的都是为了在程序运行过程中能够找到动态库，而使用静态库的程序只要链接好之后，就不用再找静态库了，因为静态库已经在可执行程序内部了，目标文件生成后，静态库删掉，程序照样可以运行。 程序加载时，链接器查找动态库文件的顺序？ 动态链接器会根据以下顺序查找动态库文件，若都无法找到，动态加载失败，程序将崩溃 在环境变量LD_LIBRARY_PATH中存储的指定路径中查找，使用export设置 在/etc/ld.conf.so.d/目录下根据配置文件存储的路径查找，使用ldconfig使配置生效 默认的/usr/lib64路径 其实一开始，动态链接器会根据程序硬编码的RPATH或RUNPATH选项指定的路径查找。不过我们一般都不会设置这两个选项，因为这会降低程序的可移植性和灵活性，所以我没有在上面提及。 怎么从进程地址空间的角度理解动态库？ 加载程序时，系统就要为其分配进程地址空间，堆区往上栈区往下之间有一个区域：共享区，共享区有很多作用，其中一个是：存储动态库函数的映射地址。 映射地址只是虚拟地址，由于动态库函数的地址是在程序运行时才确定的，而共享区在程序加载时就需要进行创建。所以操作系统会在动态库函数地址确定之后修改页表，使共享区的映射地址指向正确的动态库函数地址。 ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:9:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"运行使用动态库的程序的主要过程： 系统将程序对应的可执行文件和需要使用的动态库加载到内存中（可分批加载） 系统创建为可执行文件创建进程，建立进程地址空间，页表。同时调用动态链接器，进行符号解析和重定位 当系统为动态库函数分配好了空间，系统将修改进程的页表，使共享区的动态库函数映射地址指向正确的地址（建立进程地址空间中的共享区与内存中的动态库在页表中的映射） 当进程调用动态库函数时，会跳转到共享区，获取函数的映射地址，经过页表的转换找到函数并且执行。函数执行完，程序会从共享区跳转回原来的区域，继续往下执行代码 对比动态库与静态库的优缺点 利用静态函数库编译成的文件比较大（因为整个函数库的所有数据都会被整合进目标代码中） 他的优点就显而易见了，即编译后的执行程序不需要外部的函数库支持，因为所有使用的函数都已经被编译进去了。 当然这也会成为他的缺点，因为如果静态函数库改变了，那么你的程序必须重新编译。 动态函数库所产生的可执行文件比较小。 由于函数库没有被整合进你的程序，而是程序运行时动态的申请并调用，所以程序的运行环境中必须提供相应的库。 动态函数库的改变并不影响你的程序，所以动态函数库的升级比较方便。 操作系统采用虚 拟内存机制允许物理内存中的一份动态库被要用到该库的所有进程共用，节省了内存和磁盘空间。 库依赖的查看 使用ldd命令来查看执行文件依赖于哪些库。 ","date":"0001-01-01","objectID":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/:10:0","tags":null,"title":"","uri":"/linux5_gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8A%A8%E9%9D%99%E6%80%81%E5%BA%93/"},{"categories":null,"content":"5_基础IO ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:0:0","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"C标准库IO接口 int main() { FILE* fp = fopen(\"log.txt\", \"w+\"); if(fp == NULL) { perror(\"fopen\"); return 1; } // C output fprintf(fp, \"hello fprintf\\n\"); fputs(\"hello fputs\\n\", fp); fwrite(\"hello fwrite\\n\", strlen(\"hello fwrite\\n\"), 1, fp); fclose(fp); fp = fopen(\"log.txt\", \"r\"); char arr1[64]; char arr2[64]; char arr3[64]; fscanf(fp, \"%s\", arr1); fgets(arr2, sizeof(arr2), fp); fread(arr3, sizeof(arr3), 1, fp); printf(\"%s\", arr1); printf(\"%s\", arr2); printf(\"%s\", arr3); fclose(fp); return 0; } 打开文件fopen，关闭文件fclose，output输出类函数 fwrite fputs fprintf，intput输入类函数fread fscanf fgets 观察可以发现，这里的所有的文件IO的C库函数都和一个FILE*的类型有关，比如fopen的返回值就是一个FILE*类型，剩余所有文件IO类函数都和FILE*有关。 这里的FILE实际上是C标准库在语言层面用来描述一个文件的结构体。 fopen的打开模式：fopen的第二个参数为打开模式，有 r r+ w w+ a a+，不同参数对应不同的打开方式，比如r 为只读，r+为读写；w为写，文件存在则清空内容，不存在则创建，w+为读写，存在则清空，不存在则创建；a为追加模式写，也就是打开文件时会定位到文件末尾，不存在则创建，a+为追加模式读写，不存在则创建； 这里主要是为了对应后面要学习的文件IO的系统接口open，理解库函数和系统调用的关系 C标准库默认打开的三个输入输出流 默认打开的三个输入输出流分别为：标准输入：stdin，标准输出：stdout，标准错误stderr 它们的类型都是FILE*，也就是文件结构体FILE的指针。 这三个文件，所对应的硬件外设分别为键盘，显示器，显示器（一切皆文件） 理解当前路径 FILE *fopen(const char *path, const char *mode); 上方为fopen的函数声明，第一个参数path为你要打开的文件的相对路径或绝对路径。 每一个进程运行起来的时候，都会记录自己当前所在的工作路径，称为cwd 可以理解为，exe为这个进程对应可执行程序的绝对路径，可执行程序名为myfile。而工作路径cwd为这个进程执行起来的时候所处的路径。 如果我们写 fopen(“log.txt”, “w”); 则这里的log.txt 自动转换为 cwd/log.txt 这里和可执行程序所处的位置有关。 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:1:0","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"IO接口 - 系统调用 int main() { // blog，再次使用文件类系统接口 int fd = open(\"log.txt\", O_RDWR | O_CREAT | O_TRUNC, 0666); if(fd \u003c 0) { perror(\"open\"); return 1; } const char* p = \"hello write\\n\"; write(fd, p, strlen(p)); close(fd); fd = open(\"log.txt\", O_RDWR | O_CREAT | O_APPEND, 0666); char arr[64]; //memset(arr, '\\0', sizeof arr); read(fd, arr, sizeof(arr)); printf(\"%s\", arr); close(fd); return 0; } 有关文件IO的系统接口调用，简单学习一下open write read close，我们会发现它们和C标准库的文件IO函数相似度非常高，比如 fopen fwrite fread fclose open系统调用简介 open函数int open(const char *pathname, int flags, mode_t mode); 第一个参数为要打开的文件的路径，相对路径或绝对路径 第二个参数为标记位，这里是通过宏的方式传递的，比如O_WRONLY 表示只写， O_RDONLY表示只读，O_RDWR表示读写，O_CREAT表示若文件不存在，创建该文件，O_TRUNC表示打开文件时，清空文件…. 这里的open函数的第二个参数是以二进制标记位方式传递的，比如 O_RDWR为000001 O_WRONLY为000010，经过|运算符后，为0000011，组成一个int。函数内会根据每一个二进制标记位的01情况判断文件的打开方式 第三个参数为，创建文件时文件的权限设置，比如0666这里为8进制，转换为二进制表示110110110 – rw-rw-rw- ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:2:0","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"文件描述符 通过文件IO的系统调用接口可以发现，操作系统内核中标识一个进程打开的某一个文件使用的是一个整型int，称为文件描述符 - file descriptor - fd 我们需要学习的是这个int是如何在操作系统中标识一个文件的。 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:3:0","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"文件描述符的本质 进程打开一个文件，需要将文件加载到内存中 -\u003e 文件分为进程打开的内存文件，和没有被任何进程打开的磁盘文件。 每一个进程都可能打开一些文件，所以操作系统中就会有很多内存级文件。那么操作系统就必须管理这些文件。管理就代表着先描述，再组织（类似操作系统管理进程，用PCB描述进程）。而Linux操作系统内核中描述一个被进程打开的文件使用的是 struct file结构体，它会存储文件相关的inode元信息 操作系统中，有很多进程，有很多进程打开的文件。进程和进程打开的每一个文件需要匹配起来。 每一个进程的进程控制块PCB结构体 sturct task_struct中都有一个struct files_struct* flles的结构体指针数据成员 struct files_struct存储的就是这个进程打开的所有文件的各种信息。(Open file table structure) struct files_struct结构体内有一个struct file* fd_array[] 指针数组。我们知道，操作系统内部描述一个打开的文件就是用的struct file。 所以，这个数组就是存储着该进程打开的所有内存级文件在操作系统内部的file结构体的地址。而文件描述符fd就是这个数组的下标！ 操作系统内核使用int类型的文件描述符来标识该进程打开的每一个文件，实际上这个整型就是一个结构体（struct file）指针数组的下标。来实现进程和进程打开的每一个文件的配对。因此，那些系统调用接口也自然使用文件描述符来标识某一个文件。 而现在知道，文件描述符就是从0开始的小整数。当我们打开文件时，操作系统在内存中要创建相应的数据结构来描述目标文件。于是就有了file结构体。表示一个已经打开的文件对象。而进程执行open系统调用，所以必须让进程和文件关联起来。每个进程都有一个指针*files, 指向一张表files_struct,该表最重要的部分就是包涵一个指针数组，每个元素都是一个指向OS内描述某打开文件的结构体的指针！所以，本质上，文件描述符就是该数组的下标。所以，只要拿着文件描述符，就可以找到对应的文件 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:3:1","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"文件描述符的分配规则 int main() { close(0); int f1 = open(\"log1.txt\", O_WRONLY|O_CREAT|O_TRUNC, 0666); int f2 = open(\"log2.txt\", O_WRONLY|O_CREAT|O_TRUNC, 0666); int f3 = open(\"log3.txt\", O_WRONLY|O_CREAT|O_TRUNC, 0666); int f4 = open(\"log4.txt\", O_WRONLY|O_CREAT|O_TRUNC, 0666); printf(\"%d\\n\", f1); printf(\"%d\\n\", f2); printf(\"%d\\n\", f3); printf(\"%d\\n\", f4); close(f1); close(f2); close(f3); close(f4); } fd 0 1 2 分别对应的是标准输入 标准输出 标准错误。关闭0号标准输入之后，打开四个文件，发现是0345，所以文件描述符的分配规则为： 在files_struct结构体内部的文件结构体指针数组当中，找到当前没有被使用的最小的一个下标，作为新的文件描述符。 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:3:2","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"三个默认打开的文件 Linux进程默认情况下会有3个缺省打开的文件描述符，分别是标准输入0， 标准输出1， 标准错误2. 0,1,2对应的物理设备一般是：键盘，显示器，显示器。（Linux一切皆文件，外设在OS内也是对应一个文件，通过这个文件来与外设进行交互） 其实这和C语言默认打开的三个输入输出流是相对应的。那么，是谁配合的谁呢？当然是语言层配合系统层喽 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:4:0","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"C库函数和系统调用接口的关系 文件存在磁盘中，磁盘为硬件，要想读写文件，就要向硬件写入，只有操作系统可以通过驱动直接访问并管理硬件 操作系统给出了一些系统调用接口，比如文件IO类系统调用接口，open close read write，可以供给打开，关闭，读写文件 C标准库作为上层语言，要想访问硬件，读写文件，就必须使用操作系统提供的系统调用接口 所以，C标准库函数实际上就是系统调用接口的封装，比如fopen 封装open fclose封装close… 比如，fopen(“log.txt”, w); 封装 open(“log.txt”, O_WRONLY | O_CREAT | O_TRUNC, 0666); 再比如，fopen(“log.txt”, a); 封装 open(“log.txt”, O_WRONLY | O_CREAT | O_APPEND, 0666); 操作系统中，描述一个被打开的文件，用的是struct file，而将进程和打开的文件配对起来使用的是文件描述符fd，系统调用接口中也都是用的是文件描述符来标识文件 而C标准库在语言层面描述一个文件使用的是FILE结构体。综上，FILE结构体中一定封装了文件描述符fd stdout 这个FILE*指向的FILE内就有一个数据成员_fileno，这就是系统调用接口使用的文件描述符fd，在C标准库中定义的FILE结构体内，名为_fileno。 fprintf(stdout, \"%d\\n\", stdin-\u003e_fileno); // 0 fprintf(stdout, \"%d\\n\", stdout-\u003e_fileno); // 1 fprintf(stdout, \"%d\\n\", stderr-\u003e_fileno); // 2 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:5:0","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"重定向 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:6:0","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"输出重定向 int main() { close(1); // 1就是标准输出,stdout-\u003efileno = 1 int fd = open(\"log.txt\", O_CREAT|O_TRUNC|O_WRONLY, 0666); // 写打开，且清空 if(fd \u003c 0) { perror(\"open\"); return 1; } // 现在其实fd就是1 printf(\"fd:%d\\n\", fd); fprintf(stdout, \"hello fprintf\\n\"); fwrite(\"hahaha\\n\", strlen(\"hahaha\\n\"), 1, stdout); // 缓冲区相关 fflush(stdout); close(fd); return 0; } 运行结果：显示器上没有打印内容，数据全部输出到了文件log.txt内。 代码分析：先把1号文件描述符close了，再打开一个文件时，根据文件描述符的分配规则，选取的是文件描述符表中最小的且没有被占用的下标。所以给log.txt分配的文件描述符就是1。 stdout为标准输出，类型为FILE*，而FILE是语言层面的，内部封装的fd为1，这是语言层面没有改变的。 此时，在OS内核中的该进程对应的文件描述符表的1号下标处存储的struct file*已经不是显示器的内存文件结构体的地址了(struct file*)，而变为了log.txt对应的内核struct file结构体的地址，所以，printf fprintf fwrite都会将数据output到log.txt内，因为1号文件描述符处的指针指向的是log.txt对应的struct file结构体。 这就是输出重定向！ 同理，如果把O_TRUNC改为O_APPEND，就变为了追加重定向。 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:6:1","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"输入重定向 close(0); int fd = open(\"log.txt\", O_RDONLY); char arr[64]; fgets(arr, sizeof(arr), stdin); printf(\"%s\", arr); close(fd); 此时log.txt的fd = 0，文件描述符表的0下标处存储的是log.txt的内核struct file结构体的地址。 所有本应从标准输入读取的数据，变为了从其他文件中读取，即输入重定向。 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:6:2","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"重定向的本质 凡是往原本1号文件描述符对应的标准输出中写的内容，都写到了其他文件中，不再写到标准输出！这就是输出重定向。 凡是从原本0号文件描述符对应的标准输入读取的内容，变为了从其他文件中读取，即输入重定向。 重定向的本质：在OS内部，更改fd对应的struct file*指针内容的指向。 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:6:3","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"dup2系统调用 上方利用文件描述符的分配规则，改变0号或1号文件描述符处内容，使其指向其他文件，完成重定向。 事实上，系统调用中有一个dup2系统调用可以更方便地完成这一操作。 #include \u003cunistd.h\u003e int dup2(int oldfd, int newfd); //dup2() makes newfd be the copy of oldfd, closing newfd first if necessary, but note the following: //* If oldfd is not a valid file descriptor, then the call fails, and newfd is not closed. //* If oldfd is a valid file descriptor, and newfd has the same value as oldfd, then dup2() does nothing, and returns newfd. 介绍：**dup2是一个系统调用接口，将newfd文件描述符下标处的内容改为oldfd文件描述符下标处内容的拷贝。**使newfd和oldfd文件描述符指向同一个文件。（如果newfd处的文件是打开状态，则拷贝之前会先关闭掉newfd处的文件） ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:6:4","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"dup2示例代码 int main(int argc, char* argv[]) { int fd = open(\"log.txt\", O_CREAT | O_TRUNC | O_WRONLY, 0666); dup2(fd, 1); // 此时1和fd处的内容指向同一个文件 printf(\"fd:%d\\n\", fd); printf(\"test message\\n\"); int n = 5; while(n--) { fprintf(stderr, \"%d\\n\", n); sleep(1); } close(fd); fflush(stdout); sleep(5); printf(\"after close\\n\"); return 0; } 这段代码，一方面是使用dup2来演示输出重定向。另一方面是探究两个问题。 1、 1 和 fd都指向同一文件，那么close(fd)之后，文件描述符为1处的文件访问还有效吗？ 2.、缓冲区问题 事实证明，fd和1文件描述符指向同一文件，在close(fd)之后，fd = 1处的文件访问依然有效，所以后面的\"after close\"成功输出到log.txt内。 缓冲区问题：重定向导致刷新策略改变，前两个printf中的\\n失效，必须fflush。才能将语言层缓冲区内的数据刷新到fd=1对应的普通文件中 若将第一个fflush注释掉，则 前10s，数据都存储在stdout缓冲区内。最后进程退出强制刷新stdout语言级别缓冲区，使数据刷新到stdout -\u003e fd = 1 -\u003e log.txt文件内。（这里是缓冲区问题，已经不属于上方研究的dup2的范畴了，具体缓冲区问题见下方） 其实现在看来很简单，只是重定向导致fd=1处不再是标准输出（行刷新）而是普通文件（满刷新） ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:6:5","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"缓冲区 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:7:0","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"缓冲区是什么 缓冲区就是一段内存空间 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:7:1","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"为什么要有缓冲区？ 比如用户使用fprintf往文件中输出数据时，实际上需要向磁盘中写入。如果每次fprintf都直接将数据通过操作系统写入到硬件外设内，则会非常慢，原因是内存 与 外设的处理数据速度差距非常大并且与外设预备IO的过程是非常耗时间的，可能是1000倍的差距，导致我们往硬件写入时，需要非常长的处理和响应时间。 因此，我们在内存中设计一段缓冲区，将需要多次与外设IO的数据先暂存在内存的缓冲区内，等待刷新条件达成时，再一次性全部刷新写入到外设中，这样减少了与外设的访问IO次数，则可以提高整机的效率。更重要的是提高用户的响应速度，因为只需要将数据拷贝到内存的另一区域即可！ 总而言之：提高效率，提高用户的响应速度。 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:7:2","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"缓冲区的刷新策略 通常刷新策略分为1. 立即刷新 2. 行刷新（行缓冲）3. 满刷新（全缓冲） 很容易理解的是，全缓冲的效率是最高的，因为意味着最少的外设IO次数，所以，所有的设备文件都倾向于全缓冲。但是，具体的刷新策略需要考量具体的外设使用需求。 通常，显示器的内存文件采用行缓冲（遇到\\n则刷新（指的是刷新到外设中，这里就是显示器中）），磁盘文件通常采用全缓冲（缓冲区满了则刷新）。 同时，我们也可以通过fflush强制刷新，或者进程结束时，缓冲区内的数据也会强制刷新。 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:7:3","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"探究缓冲区的示例代码 // 示例代码1 printf(\"hello printf\\n\"); fprintf(stdout, \"hello fprintf\\n\"); fputs(\"hello fputs\\n\", stdout); write(1, \"hello write\\n\", strlen(\"hello write\\n\")); fork(); // 示例代码2 int fd = open(\"log.txt\", O_WRONLY | O_TRUNC | O_CREAT, 0666); dup2(fd, 1); printf(\"hello printf\\n\"); fprintf(stdout, \"hello fprintf\\n\"); fputs(\"hello fputs\\n\", stdout); write(1, \"hello write\\n\", strlen(\"hello write\\n\")); //fflush(stdout); fork(); // 示例代码3 int fd = open(\"log.txt\", O_WRONLY | O_TRUNC | O_CREAT, 0666); dup2(fd, 1); printf(\"hello printf\\n\"); fprintf(stdout, \"hello fprintf\\n\"); fputs(\"hello fputs\\n\", stdout); write(1, \"hello write\\n\", strlen(\"hello write\\n\")); fflush(stdout); fork(); 运行结果 示例代码1：显示器中显示全部内容 示例代码2：log.txt文件中C标准库的output函数的输出内容有两份，系统调用接口write函数的输出内容有一份 示例代码3：文件中显示全部内容 结论 FILE结构体是语言层C标准库提供的，C标准库在每一个FILE结构体内都有一个语言层面的缓冲区（可以简单理解为一个数组，一段内存空间），缓冲区刷新策略由FILE结构体内封装的fd指向的文件类型决定，如果是显示器，则为行缓冲，如果是磁盘文件，则为全缓冲。 操作系统内核中在每一个struct file结构体内都有一个内核级缓冲区，事实上，C标准库IO函数fprintf，fputs，是将数据先存储到FILE结构体内的语言层面缓冲区，等待刷新条件达成或者发生强制刷新，再将数据刷新到OS内核中struct file内的内核级缓冲区中，最终由OS将数据从内核struct file内的缓冲区output到外设硬件中。（我们不关心内核缓冲区到硬件的刷新，只关心语言层面缓冲区到内核缓冲区的刷新，也就是C标准库的缓冲区何时刷新） 示例代码的分析 示例代码1中，stdout的类型为FILE*，指向FILE为标准输出，内部封装了语言层面缓冲区，此时fd = 1对应的外设为显示器，故刷新策略为行缓冲。 printf fprintf fputs将数据刷新到stdout指向的FILE内的缓冲区内，因为此时对应的是显示器，所以为行刷新，于是，当fork时，数据已经刷新到OS内了。 而对于write，这是一个系统调用接口，是直接将数据写入到OS中的，对于write，并没有语言层面的缓冲区。因此，最终所有数据都只有一份，写入到了显示器中。 示例代码2中，由于dup2，使得stdout对应的fd实际指向的文件为磁盘文件，所以，刷新策略由行缓冲变为全缓冲。致使fork前，数据并没有由FILE内的语言层缓冲区刷新到OS内。fork创建子进程，之后，进程结束，会强制刷新缓冲区，此时缓冲区内的数据需要刷新到OS内，清空缓冲区，即一种写操作，故缓冲区内的数据发生写时拷贝，父子进程各一份。最后父子进程都将数据刷新到OS内，所以最终log.txt文件中，C标准库的IO函数的数据有两份。而对于write，是直接将数据写入到OS内的，所以只有一份。 示例代码3中，如果在fork之前，fflush(stdout)，则会强制将stdout指向的FILE内的缓冲区数据刷新到OS内（刷新策略外的强制刷新）。所以，在fork创建子进程时，父进程的C标准库的缓冲区内已经没有缓冲数据了（因为已经刷新到log.txt的内核struct file结构体内的缓冲区中了，属于OS，不属于进程），自然不会发生写时拷贝后有两份数据等情况了。 上方缓冲区相关知识，揭示了缓冲区的概念，缓冲区存在的意义，缓冲区的刷新策略，语言层缓冲区和内核级缓冲区。那个示例代码也不是很复杂，只是对不同的文件/设备有不同的刷新策略，当发生重定向时，刷新策略也随之改变，致使数据存留在了语言层，即C标准库提供的缓冲区内，属于进程的数据。再结合fork创建子进程，进程结束时强制刷新缓冲区，以及写时拷贝。就产生了最终现象~ ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:7:4","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"一切皆文件 - Linux设计哲学 一切皆文件指的是，在操作系统层面，所有的外设硬件，以及磁盘内的文件，都可以看作文件，都可以统一以文件来处理。这是体现在操作系统的软件设计层面的。 如何实现一切皆文件呢？ Linux操作系统是用C语言写的，它要将所有外设都看作统一的文件，就需要以统一的方式来描述和管理。 最好的就是使用结构体，这里可以通过让结构体：struct file内存储一些属性数据，函数指针。 从而使用C语言结构体达到面向对象机制和运行时多态。 上方仅是一个简单示例。 我们可以将磁盘，显示器，键盘，网卡，显卡等硬件外设都用这个struct file结构体在操作系统内部描述管理起来。属性数据存储在结构体的数据成员中。而函数指针，可以指向不同外设的具体read write函数实现， 因为底层不同的外设，具体读写方法肯定是不同的，但是我们可以通过函数指针机制，达到在struct file的使用管理层面上，调用读写方法的方式都相同的效果。 这样一来，在操作系统层面来看，硬件之间已经没有任何差别了，都可以统一看作struct file，且可以使用struct file内的函数指针，调用对应外设的具体的读写方法。 这就是一切皆文件，这种机制称为VFS，Virtual File System，虚拟文件系统。 ","date":"0001-01-01","objectID":"/linux5_%E5%9F%BA%E7%A1%80io/:8:0","tags":null,"title":"","uri":"/linux5_%E5%9F%BA%E7%A1%80io/"},{"categories":null,"content":"6_进程间通信 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:0:0","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"进程间通信的作用，应用场景，必要性 进程间通信，是建立在多进程之上的。如果是单进程，则无法利用并发能力，更加无法进行多进程协同。多进程要想实现多进程协同（目的），就必须进行进程间通信（手段）。 具体的进程间通信的目的（实例）：比如：1. 数据传输：一个进程需要将它的数据发送给另一个进程。2. 资源共享：多个进程之间共享同样的资源。 3. 通知事件：一个进程需要向另一个或一组进程发送消息，通知它（它们）发生了某种事件（如进程终止时要通知父进程）。4. 进程控制：有些进程希望完全控制另一个进程的执行（如Debug进程），此时控制进程希望能够拦截另一个进程的所有陷入和异常，并能够及时知道它的状态改变。 多进程通过进程间通信，比如具体的上方几个目的，实现多进程协同。（鉴于现在代码经验和知识的匮乏，可能无法切实理解到什么情况下需要多进程协同，但是这样的需求和场景肯定是存在的，需要后面不断的学习） ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:1:0","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"进程间通信的技术背景 进程是具有独立性的，这是在学习进程时，进程的一大特点。 进程 = 进程的内核数据结构 + 进程对应的代码和数据。不管是两个独立的进程，还是父进程fork创建子进程，进程的内核数据结构还有代码和数据都是有独立性的。因为虚拟地址空间+页表的存在，进程的虚拟地址通过页表映射到物理内存的不同区域，即使是父进程fork创建子进程，子进程的内核数据结构也会有独立的一份，而代码和数据在创建之初和父进程共享，但是因为写时拷贝技术的存在，子进程的代码和数据仍然是具有独立性的（比如，非常量全局数据，在父子进程之一写时，会拷贝一份。而代码，比如调用execl函数进行进程切换时，也会发生代码的写时拷贝） 因此，基于进程独立性，进程间如果想进行通信，成本是比较高的。 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:2:0","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"进程间通信的本质理解 因为进程是具有独立性的 所以，要想实现进程间通信，首先要让不同进程看到同一份资源（同一块\"内存\"，这个内存是特定的结构组织的），这个内存资源，不能隶属于任何一个进程，而更应该强调共享（其实就是属于操作系统管理的） ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:3:0","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"管道IPC：匿名管道 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:4:0","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"匿名管道的本质原理 在父进程fork创建子进程时，子进程会有自己独立的内核数据结构（如页表，虚拟地址空间，PCB，文件描述符表等），而这其中的文件描述符表中每一个元素存储的是该进程打开的所有文件对应的内核struct file结构体的地址。 当父进程fork创建子进程，子进程的大部分内核数据结构的数据都是从父进程那里直接拷贝过来的（包括虚拟地址空间，页表等），当然，部分字段还是需要修改的（如pid等），而其中的文件描述符表的数据是与父进程完全一致的。 因此，当父进程以读和写方式打开某一个文件之后，进行fork，子进程继承了父进程的文件描述符表，子进程也以读和写方式打开了这个文件。因为管道是单向通信的，故父子进程关闭自己不需要的一端之后，就可以通过该匿名管道文件进行通信。 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:4:1","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"demo示例代码： #include \u003cunistd.h\u003e #include \u003ciostream\u003e #include \u003cstring\u003e #include \u003ccstring\u003e #include \u003ccassert\u003e #include \u003csys/types.h\u003e #include \u003csys/wait.h\u003e using namespace std; int main() { // 创建管道 int pipefd[2] = {0}; // pipefd[0] 读端fd， pipefd[1] 写端fd int ret = pipe(pipefd); // On success, zero is returned. On error, -1 is returned, and errno is set appropriately. assert(ret != -1); (void)ret; #ifdef DEBUG cout \u003c\u003c \"pipefd[0](管道读端) : \" \u003c\u003c pipefd[0] \u003c\u003c endl; cout \u003c\u003c \"pipefd[1](管道写端) : \" \u003c\u003c pipefd[1] \u003c\u003c endl; #endif // 创建子进程 pid_t id = fork(); assert(id != -1); if (id == 0) { // 子进程，用于读取，不写入 close(pipefd[1]); // 子进程读数据时的缓冲区 char buff[1024]; int count = 0; while (true) { ssize_t sz = read(pipefd[0], buff, sizeof(buff) - 1); if(sz \u003e 0) { buff[sz] = '\\0'; cout \u003c\u003c \"child pid :\" \u003c\u003c getpid() \u003c\u003c \" \" \u003c\u003c buff \u003c\u003c \"haha\" \u003c\u003c endl; // cout \u003c\u003c \"test\" \u003c\u003cendl; } else if(sz == 0) { cout \u003c\u003c \"Pipe close, child quit!!!\\n\"; break; } // 测试管道的读端关闭，写继续写。则OS终止写进程。 // if(count++ == 5) // { // cout \u003c\u003c \"read close\" \u003c\u003c endl; // close(pipefd[0]); // break; // } } exit(0); } // 父进程，用于写入，不读取 close(pipefd[0]); string s = \"I am father process\"; int count = 0; char buff[100]; while (true) { // 现在制造一个，父进程持续向管道内写入的程序 snprintf(buff, sizeof(buff), \"%s : %d\", s.c_str(), count++); write(pipefd[1], buff, strlen(buff)); sleep(1); // 写端关闭，读端read返回值为0，标志读到文件结尾。 if (7 == count) { close(pipefd[1]); // 父进程关闭管道的写端 cout \u003c\u003c \"Pipe close!!!\\n\"; break; } } // 父进程回收子进程 int status = 0; pid_t result = waitpid(id, \u0026status, 0); // 阻塞式等待，等待成功返回0，调用失败返回-1 if (WIFEXITED(status)) { cout \u003c\u003c \"Child process \" \u003c\u003c result \u003c\u003c \" quit code : \" \u003c\u003c WEXITSTATUS(status) \u003c\u003c endl; } else { cout \u003c\u003c \"Child process quit abnormally\" \u003c\u003c endl; } assert(result \u003e 0); return 0; } ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:4:2","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"pipe 系统调用 创建匿名管道时，是有专门的系统调用的，因为管道文件和普通文件在性质上是有差别的。 int pipe(int pipefd[2]) 该系统调用用于创建一个纯内存级的匿名管道文件，参数为输出型参数，pipefd[0]保存读端文件描述符，pipefd[1]保存写端文件描述符（整型，文件描述符表的下标）。 管道是一种单向通信的进程间通信方式，父子进程需要关闭自己不需要的一端的文件描述符（其实不关闭也不影响，但是从严谨和避免资源浪费的角度考虑，最好关闭）。 **这里的内核中匿名管道文件是一种纯内存级文件，在磁盘中没有对应的文件实体，不会把内核缓冲区中的数据进行落盘和持久化。**所有的进程间通信都是内存级通信 fork创建子进程，父进程的内核数据结构会拷贝一份，因为进程具有独立性，且这样做本身也是很有必要的。但是一个文件在OS内核中只会有一个对应的struct file，故父子进程文件描述符表中struct file*指向的是同一个struct file，这里struct file只有一份。这是OS的进程管理和文件管理，属于两个模块。 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:4:3","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"管道读写的4种情况： 写慢，读快，则将管道里数据读完就会read阻塞等待，需要等待写端写入数据 读慢，写快，管道写满就不能再写了，需要等待读端读取数据 写端关闭，read不会阻塞，read读到管道结尾，返回0 读关，写仍然在写，则OS会终止写端进程 （这里可以用demo代码进行测试验证） ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:4:4","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"管道的特点： 这里的匿名管道适用于具有亲缘关系的进程之间进行进程间通信 - 常用于父子进程。（后面的命名管道文件可用于无关联的两个进程之间进行IPC） 管道提供了访问控制，内核会对管道操作进行同步与互斥 管道提供流式的通信服务 - 面向字节流 管道的本质就是文件，管道的生命周期随进程 管道是单向通信的，半双工的（一方读，一方写），数据只能向一个方向传输。如果需要进程双向通信，则需创建两个管道。 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:4:5","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"管道IPC：命名管道 上方讲的匿名管道适用于有亲缘关系的两个进程之间进行IPC。其实利用的就是文件描述符表继承机制，从而让双方进程看到同一份资源（文件）。 而对于没有亲缘关系的两个进程来说，无法利用文件描述符表的继承机制，但是也可以直接打开磁盘中的同一个文件，这样在OS内核中就只有一个struct file结构体，也实现了让两个进程看到同一份资源。 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:5:0","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"mkfifo - C标准库函数\u0026\u0026Linux命令 用于创建命名管道 mkfifo是Linux下的一个命令，可用于创建命名管道文件 $ mkfifo filename 同时，mkfifo也是一个C标准库函数 int mkfifo(const char *filename,mode_t mode); 第一个参数为命名管道的绝对/相对路径，第二个参数为管道文件的权限设置。 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:5:1","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"命名管道的本质理解 mkfifo C标准库函数，可用于创建一个命名管道（文件），此文件在磁盘中有对应文件实体，也就有了路径，而路径是具有唯一性的。 让两个进程打开磁盘中的同一命名管道文件，则路径的唯一性使得这两个进程使用open打开此fifo之后，文件描述符表中的struct file*指向的是OS内核中的同一个struct file（对应那个命名管道文件），这样就让两个进程看到同一份资源了。 注意： 此命名管道文件在内核缓冲区中的数据，依旧不会刷新到磁盘中，磁盘中那个命名管道文件，仅仅是一个文件名+属性，没有内容。 进程间利用命名管道文件进行IPC仍然是纯内存级的，因为不会进行落盘/持久化数据。 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:5:2","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"命名管道示例代码 comm.hpp #ifndef _COMM_H_ #define _COMM_H_ #include \u003ciostream\u003e #include \u003cstring\u003e #include \u003ccassert\u003e #include \u003csys/types.h\u003e #include \u003csys/stat.h\u003e #include \u003csys/wait.h\u003e #include \u003cfcntl.h\u003e #include \u003cunistd.h\u003e #include \"Log.hpp\" #define PATHNAME \"/home/yzl/InterprocessCommunication/namedPipe/fifo.ipc\" #endif Log.hpp #ifndef _LOG_H_ #define _LOG_H_ #include \u003ciostream\u003e #include \u003cctime\u003e #define Debug 0 #define Notice 1 #define Warning 2 #define Error 3 const std::string msg[] = { \"Debug\", \"Notice\", \"Warning\", \"Error\" }; // 输出日志信息，其实就是 时间戳+日志等级(日志用于做什么的标记)+信息。 std::ostream \u0026Log(std::string message, int level) { std::cout \u003c\u003c \" | \" \u003c\u003c (unsigned)time(nullptr) \u003c\u003c \" | \" \u003c\u003c msg[level] \u003c\u003c \" | \" \u003c\u003c message; return std::cout; } #endif mutiServer.cxx #include \"comm.hpp\" void getMessage(int fd) { // 三个子进程读取管道文件数据，随机一个子进程读取成功，也只会有一个。 char buffer[1024]; while (true) { ssize_t sz = read(fd, buffer, sizeof(buffer) - 1); if (sz \u003e 0) { buffer[sz] = '\\0'; std::cout \u003c\u003c \"[ \" \u003c\u003c getpid() \u003c\u003c \" ] \" \u003c\u003c \"client say : \" \u003c\u003c buffer \u003c\u003c std::endl; } else if (sz == 0) { std::cout \u003c\u003c \"read end of file, client quit, server quit too\" \u003c\u003c std::endl; break; } else { perror(\"server::read\"); exit(3); } } } int main() { // 1. 创建命名管道 int ret = mkfifo(PATHNAME, 0666); // 000 110110110 rw-rw-rw- 受文件掩码影响 if (ret == -1) { perror(\"mkfifo\"); exit(1); } Log(\"创建命名管道文件成功\", Debug) \u003c\u003c \" step1\" \u003c\u003c std::endl; // 2. 然后就是文件的常规操作了 // 因为这里是管道，所以，有一些和普通磁盘文件不同的地方 // 这里当写端进程没有打开管道文件时，这里open会阻塞 int fd = open(PATHNAME, O_RDONLY); // 只读打开 if (fd == -1) { perror(\"open\"); exit(2); } Log(\"server只读打开命名管道文件成功\", Debug) \u003c\u003c \" step2\" \u003c\u003c std::endl; // 3. 利用命名管道文件进行通信 for(int i = 0; i \u003c 3; ++i) { pid_t id = fork(); if(id == 0) { // 子进程，继承父进程的文件描述符，已经打开了命名管道文件，进行数据读取 getMessage(fd); exit(0); } } // 父进程等待 for(int i = 0; i \u003c 3; ++i) { waitpid(-1, nullptr, 0); // 阻塞式等待随机一个子进程 } Log(\"父进程等待三个子进程退出成功\", Debug) \u003c\u003c std::endl; // 4. 读取结束，关闭文件 close(fd); Log(\"关闭命名管道文件成功\", Debug) \u003c\u003c \" step 3\" \u003c\u003c std::endl; // 5. ipc结束，删除命名管道文件 unlink(PATHNAME); Log(\"删除命名管道文件成功\", Debug) \u003c\u003c \" step 4\" \u003c\u003c std::endl; return 0; } client.cxx #include \"comm.hpp\" int main() { // client只写打开管道文件，管道文件由server提供 int fd = open(PATHNAME, O_WRONLY); if(fd == -1) { perror(\"client::open\"); exit(1); } Log(\"client open fifo.ipc success\", Debug) \u003c\u003c std::endl; // 写数据 std::string str; std::cout \u003c\u003c \"Please input the message that you want to sent to server by fifo.ipc\" \u003c\u003c std::endl; while(std::getline(std::cin, str)) { write(fd, str.c_str(), str.size()); } // 关闭文件 int n = close(fd); assert(n == 0); Log(\"client close fifo.ipc success(reader of named pipe)\", Debug) \u003c\u003c std::endl; return 0; } ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:5:3","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"命名管道 vs 匿名管道 匿名管道和命名管道都是基于文件的，本质都是利用文件，让两个进程看到同一份资源从而进行IPC。 FIFO（命名管道）与pipe（匿名管道）之间的区别在它们创建与打开的方式不同，一但这些工作完成之后，它们具有相同的语义。 匿名管道使用的pipe可以创建并直接以读和写方式打开匿名管道文件（纯内存级，磁盘无对应实体）并利用文件描述符表的继承机制 让进程看到同一份资源从而IPC。 而命名管道文件是用mkfifo创建命名管道文件，之后，双方进程使用系统调用open分别以读和写方式打开这个fifo，实现IPC。同时，命名管道文件在磁盘中有对应的实体。但是命名管道文件在内核缓冲区中的数据，依旧不会刷新到磁盘中，磁盘中那个命名管道文件，仅仅是一个文件名+属性，没有内容。进程间利用命名管道文件进行IPC仍然是纯内存级的，因为不会进行落盘/持久化数据。 匿名管道文件：pipe -\u003e ipc 命名管道文件：mkfifo -\u003e open -\u003e ipc 对于管道的访问控制（同步与互斥），生命周期随进程，提供字节流服务，单向通信，半双工的特点，命名管道同样具有，因为本质都是管道 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:6:0","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"System V 共享内存 IPC ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:7:0","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"如何通过共享内存进行进程间通信 进程间通信的前提是让不同进程看到同一份资源。 SHM：先在物理内存中创建（申请）一段共享内存（内存空间），再通过页表建立起这段内存空间（SHM）与进程的虚拟地址空间之间的映射关系，这样进程就可以使用虚拟地址通过页表映射直接访问这段物理内存，即可在多个进程之间进行进程间通信。 这段物理内存会被映射进虚拟地址空间中栈区与堆区之间的共享区（回顾，动态库的加载链接也是会被加载到共享区。） 在申请SHM，attachSHM之后，进程就可以像使用一段malloc出的内存空间一样使用这段SHM。这里和管道的使用方式和本质是有区别的，具体看下方。 共享内存的生命周期随操作系统，而不是随进程 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:7:1","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"共享内存本质理解（OS管理的角度） 对于共享内存的理解不能只理解为一段物理内存中的内存空间。共享内存的提供/管理者是操作系统，OS内可能会有很多共享内存，则OS必须要管理这些共享内存，管理的本质就是：先描述，再组织。所以，OS必须要对这些共享内存建立对应的内核数据结构，去描述这些SHM。 故，共享内存 = 共享内存块 + 共享内存对应的内核数据结构 所以，如果进程申请4096字节的共享内存空间，则OS为了这段共享内存所占用的内存空间一定大于4096字节（因为还有内核数据结构）（其实管道也是需要管理的，只是管道的本质就是文件，所以管道管理 = 文件管理。而这里的共享内存是OS为了进程间通信单独设立的一个模块。） ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:7:2","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"共享内存IPC示例代码 comm.hpp #pragma once #include \u003ciostream\u003e #include \u003cstring\u003e #include \u003ccassert\u003e #include \u003ccstring\u003e #include \u003cunistd.h\u003e #include \u003cfcntl.h\u003e #include \u003csys/types.h\u003e #include \u003csys/ipc.h\u003e #include \u003csys/shm.h\u003e #include \u003csys/stat.h\u003e #include \"Log.hpp\" #define PATH_NAME \".\" #define PROJ_ID 306 #define SHM_SIZE 4096 #define WRITE O_WRONLY #define READ O_RDONLY #define FIFO_PATH \"./fifo\" std::string transToHex(key_t key) { char buff[32]; snprintf(buff, sizeof(buff), \"0x%x\", key); return buff; } // 利用命名管道进行访问控制 // 这里包含了命名管道文件的创建和删除，创建和析构对象时自动执行。还剩下打开，读写，关闭行为。 class Fifo { public: Fifo() { int n = mkfifo(FIFO_PATH, 0666); assert(n != -1); (void)n; Log(\"create FIFO success\", Notice) \u003c\u003c std::endl; } ~Fifo() { unlink(FIFO_PATH); Log(\"delete FIFO success\", Notice) \u003c\u003c std::endl; } }; // 打开一个文件的包装函数 int OpenFIFO(std::string pathname, int flags) { int fd = open(pathname.c_str(), flags); assert(fd != -1); return fd; } // 共享内存的读方调用Wait，利用管道的访问控制进行等待。 void Wait(int fd) { Log(\"等待中...\", Notice) \u003c\u003c std::endl; uint32_t temp = 0; ssize_t sz = read(fd, \u0026temp, sizeof(uint32_t)); assert(sz == sizeof(uint32_t)); (void)sz; } // 共享内存的写方调用Awaken，利用管道的访问控制，唤醒管道读方，从而让共享内存的读方读取数据。 void Awaken(int fd) { Log(\"唤醒中...\", Notice) \u003c\u003c std::endl; uint32_t temp = 0; ssize_t sz = write(fd, \u0026temp, sizeof(uint32_t)); assert(sz != -1); (void)sz; } void CloseFIFO(int fd) { close(fd); } shmServer.cc #include \"comm.hpp\" // 共享内存的server，创建共享内存，attach detach 删除 Fifo fifo; int main() { // 1. 先创建Key key_t key = ftok(PATH_NAME, PROJ_ID); assert(key != -1); Log(\"create key done\", Debug) \u003c\u003c \" server key : \" \u003c\u003c transToHex(key) \u003c\u003c std::endl; // 2. 创建共享内存，建议创建一个全新的共享内存 // key所对应的共享内存不存在则创建，存在则报错，目的：创建一个全新的共享内存，要带权限 int shmid = shmget(key, SHM_SIZE, IPC_CREAT | IPC_EXCL | 0666); // 返回这个共享内存的标识符id，此时还没有和进程关联起来 if(shmid == -1) { perror(\"shmget\"); exit(1); } Log(\"create shm done\", Debug) \u003c\u003c \" shmid : \" \u003c\u003c shmid \u003c\u003c std::endl; // sleep(10); // 3. attach共享内存 （上面shmget只是获取了共享内存的id，这里进行attach，将物理内存中的共享内存和自己的地址空间的共享区建立映射关系） char* shmaddr = (char*)shmat(shmid, nullptr, 0); if(shmaddr == (void*)-1) { perror(\"server::shmat\"); exit(2); } Log(\"attach shm done\", Debug) \u003c\u003c \" shmid : \" \u003c\u003c shmid \u003c\u003c std::endl; // sleep(10); // 4. 通过shm，进行ipc int fd = OpenFIFO(FIFO_PATH, READ); while(true) { Wait(fd); // 利用管道进行访问控制，等待写入！写入数据后，再进行读取 printf(\"%s\\n\", shmaddr); if(strcmp(shmaddr, \"quit\") == 0) break; // sleep(1); } CloseFIFO(fd); // 5. detach共享内存 int ret = shmdt(shmaddr); assert(ret != -1); (void)ret; Log(\"detach shm done\", Debug) \u003c\u003c \" shmid : \" \u003c\u003c shmid \u003c\u003c std::endl; // sleep(10); // 6. 删除共享内存 // 共享内存的生命周期随OS，而不是随进程。并非attach的进程数到0就自动销毁。 // IPC_RMID即便是有进程和当下的shm挂接，依旧删除共享内存 ret = shmctl(shmid, IPC_RMID, nullptr); assert(ret != -1); (void)ret; Log(\"delete shm done\", Debug) \u003c\u003c \" shmid : \" \u003c\u003c shmid \u003c\u003c std::endl; return 0; } shmClient.cc #include \"comm.hpp\" int main() { // 创建key key_t key = ftok(PATH_NAME, PROJ_ID); Log(\"create key done\", Debug) \u003c\u003c \" client key : \" \u003c\u003c transToHex(key) \u003c\u003c std::endl; // 获取（创建、申请）共享内存 // int shmid = shmget(key, SHM_SIZE, IPC_CREAT); int shmid = shmget(key, SHM_SIZE, 0); if(shmid == -1) { perror(\"shmget\"); exit(1); } Log(\"get shm done\", Debug) \u003c\u003c \" shmid : \" \u003c\u003c shmid \u003c\u003c std::endl; // sleep(10); // attach共享内存 char* shmaddr = (char*)shmat(shmid, nullptr, 0); if(shmaddr == (void*)-1) { perror(\"shmat\"); exit(2); } Log(\"attach shm done\", Debug) \u003c\u003c \" shmid : \" \u003c\u003c shmid \u003c\u003c std::endl; // sleep(10); // 利用共享内存进行ipc int fd = OpenFIFO(FIFO_PATH, WRITE); // test3 while(true) { // 用键盘输入数据 // 事实证明，输入abc\\n 则 sz = 4 ssize_t sz = read(0, shmaddr, SHM_SIZE - 1); if(sz \u003e 0) { Awaken(fd); // 确认写入共享内存数据后，利用命名管道的访问控制，唤醒读端 shmaddr[sz - 1] = '\\0'; if(strcmp(shmaddr, \"quit\") == 0) break; } } CloseFIFO(fd); // // test2 // // 果然，共享内存这块内存空间是完全类似于一段数组的，读写操作随意，且读操作只是读，不会进行清空操作。 // std::string s = \"aaaaaaaaa\"; // sprintf(shmaddr, \"%s\", s.c_str()); // aaaaaaaaa\\0 // sleep(3); // std::string s2 = \"ccc\"; // sprintf(shmaddr, \"%s\", s2.c_str()); // ccc\\0 // shmaddr[s2.size()] = 'b'; // sleep(3); // test1 // char c = 'a'","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:7:3","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"创建，attach，detach，删除共享内存的相关系统调用 key_t ftok(const char *pathname, int proj_id); 用于创建key值，第一个为随意一个文件路径，第二个为随意一个整数，创建的key值用于在内核层面标识每一个共享内存，创建共享内存shmget需要用到。 ftok函数内部就是算法，用于生成一个唯一的关键值。（类似哈希函数的道理），值无所谓，重点是要唯一。 int shmget(key_t key, size_t size, int shmflg); 用于创建/获取共享内存，即在物理内存中申请一段共享内存空间。第一个参数为ftok创建的key值，第二个为共享内存段空间大小（最好是页的整数倍（4kb，4096字节），避免资源浪费，因为你申请4097，则OS创建的SHM段大小也为8kb），第三个参数为宏的组成，IPC_CREAT IPC_EXCL 共享内存权限设置的组合。 IPC_CREAT | IPC_EXCL：若key对应的SHM不存在，则创建，并返回SHM的用户层shmid，若已经存在，则出错返回（SHM的创建者可以使用这个，因为这样可以保证SHM一定是全新的） IPC_CREAT：若key对应的SHM已经存在，则返回，不存在，则创建并返回。 注意SHM创建进程要加SHM的权限设置。 void *shmat(int shmid, const void *shmaddr, int shmflg); 建立共享内存和进程虚拟地址空间的映射关系，关联/attach起来。shmid即shmget返回的SHM在用户层的标识符（类似文件描述符），后面两个设置为nullptr和0即可… 略了.. 返回值即共享内存attach之后对应的虚拟地址。 int shmdt(const void *shmaddr); 解除共享内存和进程虚拟地址空间的映射关系，进行detach。参数为shmat的返回值，即共享内存的虚拟地址。 int shmctl(int shmid, int cmd, struct shmid_ds *buf); 用代码，系统调用删除共享内存。 shmctl(shmid, IPC_RMID, nullptr)即可 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:7:4","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"shmget的第一个参数 key值 和 返回值shmid的关系 key值，是当进程间想通过共享内存进行IPC时，用key值在OS内核层面标识每个共享内存的唯一性。 这样，两个进程通过同一个key值，就可以获取到同一个共享内存，这里shmget的返回值是用户层共享内存的一个id值，类似open的返回值文件描述符。 后面进行shmat shmctl都是使用shmid，而key，仅仅是在shmget，创建/获取该共享内存时，帮助进程间获取到同一个共享内存。 key是在内核层面标识共享内存。shmid是在用户层标识共享内存。 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:7:5","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"Linux关于共享内存的命令 ipcs -m 查看当前OS中所有的共享内存 key shmid owner perms(权限) bytes(共享内存大小) nattach(该SHM挂接的进程数量) ipcsrm -m shmid 使用命令删除一个共享内存。 共享内存的生命周期随OS，而不是随进程，若某进程创建共享内存，attach，detach，而没有删除，则进程结束，该共享内存不会自动销毁。 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:7:6","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"从虚拟地址空间角度理解共享内存和管道的区别 共享内存机制：在物理内存中申请一段内存空间，通过用户空间页表（页表分为用户空间页表和内核空间页表）建立起物理内存和进程的虚拟地址空间之间的映射关系，具体映射到虚拟地址空间中的堆栈间的共享区。 这样，进程就可以使用shmat返回的虚拟地址通过用户空间页表映射直接访问这段物理内存，这是不需要OS内核的 而虚拟地址空间分为0~3G的用户空间和3~4G的内核空间，上方的共享区就是在用户空间中。进程可以直接访问用户空间，但是不能直接访问内核空间，访问内核空间需要通过系统调用。 管道机制：管道机制的本质是文件机制，进程间通过访问内核中的同一个文件，从而看到同一份资源进行IPC，而不管是匿名管道还是命名管道，本质都是内核中的struct file以及文件的内核缓冲区。这部分数据本身是存储在物理内存中的，通过内核空间页表映射到虚拟地址空间的内核空间中，这就是为什么进程间通过管道通信需要使用write，read这样的系统调用，因为内核空间的访问必须通过系统调用。（这里也是虚拟地址空间机制对于物理内存的保护的体现） ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:7:7","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"共享内存的特点 共享内存是最快的进程间通信方式，一旦这样的内存映射到共享它的进程的地址空间，这些进程间数据传递不再涉及到内核，换句话说是进程不再通过执行进入内核的系统调用（如read，write）来传递彼此的数据 比如：管道这样的ipc，写端：键盘输入数据，到我们用户空间的缓冲区中，之后用write写入到内核中的管道文件缓冲区中。读端：通过read系统调用读取管道文件缓冲区中的数据，到字符数组中（用户层缓冲区），再用printf打印出来。 这里面经过了4次拷贝（不考虑文件的用户层缓冲区，即标准库定义的缓冲区） 而共享内存，我们可以直接用键盘向共享内存中写入，再用printf直接打印出共享内存中的数据，这是2次拷贝。 4次与2次拷贝直接决定了共享内存是最快的ipc方式，本质还是因为通过共享内存进行IPC不用通过操作系统内核传递数据 共享内存缺乏访问控制，共享内存没有进行同步与互斥 上方示例代码中，server端运行起来后，不管client端有没有执行，不管这个共享内存的attach进程数是几个，有没有人写入，server端都是一直读取，这是缺乏访问控制的表现。（上方代码中，利用管道，增加了访问控制） 共享内存的生命周期随操作系统，不同于管道的生命周期随进程 如果不进行shmctl 和 命令行上的ipcsrm -m，则进程结束后，共享内存一直存在。 ","date":"0001-01-01","objectID":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/:7:8","tags":null,"title":"","uri":"/linux6_%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"categories":null,"content":"7_进程信号 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:0:0","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"信号的基本认识 Linux信号机制： 它是一种异步的通知机制，用来提醒进程一个事件已经发生。 Linux操作系统中，有编号为1~31的31个普通信号，编号为34~64的31个实时信号，共62个信号。日常中只会涉及和使用到普通信号。故下方对信号的学习仅对于1~31的普通信号。 每个信号都有一个编号和一个宏定义名称，本质上，这些都是通过#define的形式定义的。也就是用一个int型变量去代替某特定信号。（编译之后，这些宏定义都会变为int整型） ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:1:0","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"进程处理信号的三种方式 信号的接收方一定是进程，因为信号就是OS用来提醒进程某个事件已经发生。故，进程在接收到信号之后，一定要处理这个信号。 执行该信号的默认处理动作。 忽略该信号。 提供一个信号处理函数，要求内核在处理该信号时切换到用户态执行这个处理函数，这种方式称为捕捉（Catch）一个信号。 （其实就是程序员自定义进程对某信号的处理方法，该方法存储在用户代码中） 简单来说就是默认，忽略，自定义捕捉。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:2:0","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"信号产生：产生信号的若干种方式 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:3:0","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"1.通过键盘产生信号： ctrl + c：通过键盘组合键向前台进程发送2号SIGINT信号。 ctrl + \\：通过键盘组合键向前台进程发送3号SIGQUIT信号。 理解：键盘是通过中断的方式工作的。输入某组合键-\u003eOS解释识别组合键-\u003e查找进程列表-\u003e前台运行的进程-\u003eOS将前台运行进程的PCB(task_struct)中的pending信号集位图中组合键对应的信号所对应的比特位由0置1（pending 信号集本质是一个位图结构，见信号保存。）（其实最后一步一句话就是：OS向前台进程发送对应信号，发送信号的本质就是如此） 有关前台进程与后台进程： Ctrl-C 产生的信号只能发给前台进程。一个命令后面加个\u0026可以放到后台运行（如 ./mysignal \u0026）,这样Shell不必等待进程 结束就可以接受新的命令,启动新的进程。Shell可以同时运行一个前台进程和任意多个后台进程,只有前台进程才能接到像 Ctrl-C 这种控制键产生的信号 前台进程在运行过程中用户随时可能按下 Ctrl-C 而产生一个信号,也就是说该进程的用户空间代码执行 到任何地方都有可能收到 SIGINT 信号而终止,所以信号相对于进程的控制流程来说是**异步(Asynchronous)**的。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:3:1","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"2.通过系统调用接口产生信号： int kill(pid_t pid, int signo); 向指定进程发送指定信号 int raise(int signo); 向当前进程发送指定信号 void abort(void); 向当前进程发送6号SIGABRT信号 Linux还有一个kill命令，就是通过调用kill函数实现的。 // 通过系统调用发送信号 void handler(int signo) { std::cout \u003c\u003c \"进程收到了一个\" \u003c\u003c signo \u003c\u003c \"号信号\" \u003c\u003c std::endl; } int main() { signal(2, handler); // 捕捉下方kill 和 raise发送的信号 signal(SIGABRT, handler); // 捕捉abort发送的6号SIGABRT信号 kill(getpid(), 2); raise(2); abort(); // 向当前进程发送SIGABRT信号，使其异常终止（默认） while(true) sleep(1); return 0; } 如上图，对6号SIGABRT信号明明捕捉了但是还是中止了。 这是一个很奇怪的现象，查了stack overflow：However, I cannot find any corroborating evidence of that behavior in the signal man page, which clearly states that The signals SIGKILL and SIGSTOP cannot be caught, blocked, or ignored but makes no similar mention for SIGABRT. 也就是信号手册种明确说了9和19号信号不能被捕捉，阻塞，或者忽略，但是没有说SIGABRT信号不能被捕捉。其实这个是因为调用了abort，abort不仅发送了SIGABRT信号，还做了其他事情。所以如果把abort();换成kill(getpid(), SIGABRT); 就会捕捉SIGABRT且进程不会退出。 如何理解通过调用系统接口产生信号：其实很简单，系统调用接口通过提取参数，获取进程pid，信号编号，然后OS向进程PCB内写信号，也就是修改对应进程的pending位图的特定位，后续进程处理该信号。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:3:2","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"3.由软件条件产生信号： 例一：进程间通过管道通信时，因软件条件而产生SIGPIPE信号。 进程间通过管道通信时，不管是命名管道还是匿名管道，都会提供访问控制。也就是管道读写的四种情况。情况之一是：当管道的读端关闭，写端继续写，则OS会向写端进程发送SIGPIPE信号从而终止写端进程。（SIGPIPE信号的默认处理方式就是terminate process，终止进程。产生条件：Broken pipe：write to pipe with no readers） // 通过软件条件发送信号，比如管道读端关闭，写端继续写，OS会终止写端进程 void handler(int signo) { std::cout \u003c\u003c \"进程收到了\" \u003c\u003c signo \u003c\u003c \"号信号\" \u003c\u003c std::endl; } int main() { int pipefd[2] = {0}; pipe(pipefd); // 创建匿名管道 pid_t id = fork(); if(id == 0) { // 子进程，读端，关闭写端 close(pipefd[1]); sleep(3); close(pipefd[0]); // 三秒后关闭读端 } // 父进程写端 signal(SIGPIPE, handler); close(pipefd[0]); // 关闭读端 while(true) { const char* message = \"haha\"; write(pipefd[1], message, strlen(message)); sleep(1); // 每隔一秒写一次 } return 0; } 如上，对SIGPIPE信号进行捕捉，当OS发送SIGPIPE信号时，会执行handler方法。下方打印1s进行一次。 SIGPIPE是一种由软件条件产生的信号 例二：alarm函数，因软件条件产生SIGALRM信号。 #include \u003cunistd.h\u003e unsigned int alarm(unsigned int seconds); 调用alarm函数可以设定一个闹钟,也就是告诉内核在seconds秒之后给当前进程发SIGALRM信号, 该信号的默认处理动作是终止当前进程。 这个函数的返回值是0或者是以前设定的闹钟时间还余下的秒数。（也就是一个进程在同一时刻只会有一个闹钟） 下方是一个基于alarm和SIGALRM信号的简单定时执行某任务的程序。 // 因软件条件产生信号：alarm函数，SIGALRM信号 typedef std::function\u003cvoid()\u003e func; std::vector\u003cfunc\u003e callbacks; uint64_t count = 0; void showCount() { std::cout \u003c\u003c \"current count is : \" \u003c\u003c count \u003c\u003c std::endl; } void showLog() { std::cout \u003c\u003c \"Log...\" \u003c\u003c std::endl; } void logUser() { if(fork() == 0) { // 子进程 execl(\"/usr/bin/who\", \"who\", nullptr); exit(1); } // 父进程等待回收一下 wait(nullptr); } void catchSIGALRM(int signo) { std::cout \u003c\u003c \"进程收到了SIGALRM信号\" \u003c\u003c std::endl; // 对于SIGALRM的自定义捕捉 for(auto\u0026 func : callbacks) { func(); } alarm(1); // 再定一个定时器 } int main() { callbacks.push_back(std::function\u003cvoid()\u003e(showCount)); callbacks.push_back(std::function\u003cvoid()\u003e(showLog)); callbacks.push_back(std::function\u003cvoid()\u003e(logUser)); signal(SIGALRM, catchSIGALRM); // 捕捉SIGALRM信号 alarm(1); // 定一个1s的定时器 while(true) ++count; return 0; } 之前的SIGPIPE可以理解为是在管道IPC中，软件条件不满足从而OS发了信号。这里的SIGALRM可以理解为是闹钟软件条件满足从而发了信号。 这里的定时器闹钟，一定是OS管理的，因为整个程序只有一个执行流。OS中有很多进程，每个进程都有可能通过alarm设定定时器，所以，OS对于这些闹钟，一定是要管理的。所以需要先描述，再组织。就像struct file，task_struct一样。OS定期检查操作系统内所有的闹钟哪个到时间了，就向指定进程发送SIGALRM信号。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:3:3","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"4.因硬件异常产生信号： 例一：除零操作，触发硬件异常，OS发送8号SIGFPE信号 OS发送SIGFPE信号是当进程犯了一个floating point exception（浮点异常），本质上是由于硬件异常引起的，这种异常通常发生在进程进行浮点运算时出现了无效操作，如除以零。 具体来说，CPU内是由浮点单元(FPU，Floating Point Unit)来执行浮点运算指令的，浮点单元主要由两部分组成：算术逻辑单元(ALU)和浮点寄存器。算术逻辑单元负责实现浮点运算指令，而浮点寄存器则用于保存运算中间结果和最终结果。当CPU内的浮点单元（FPU）发生除零操作时，会将这种情况标记在状态寄存器中。具体来说，这个状态寄存器通常被称为浮点单元状态寄存器（FPU status register）或者协处理器状态寄存器（Co-processor status register）。这个寄存器用于存储浮点单元的状态和结果状态，包括运算的结果状态（如是否为非数字（NaN）、无穷大）、被除数状态、除数状态、精度状态等。（注：ChatGPT真他妈好用） 其实就是，当发生除零操作时，CPU内的状态寄存器的状态标记位会标记这次除零操作（如将比特位置为1），OS会自动进行计算完毕之后的检测，若OS识别到某标记位异常，就会向当前执行的进程发送对应的信号，除零操作对应的就是SIGFPE信号。（为什么是当前进程呢？因为此时的状态标记位一定标记的是当前进程的某指令的运算结果） void handler(int signo) { std::cout \u003c\u003c \"进程收到了一个\" \u003c\u003c signo \u003c\u003c \"号信号\" \u003c\u003c std::endl; } int main() { signal(SIGFPE, handler); int i = 1/0; // 除零 return 0; } 上方实验现象是：handler方法无限调用，不是只调用一次的原因是：这里的硬件异常并没有解决，比如状态寄存器中的某状态标记位一直为1，异常一直存在，OS就会一直检测到硬件异常，并发送对应信号。 例二：进程访问非法内存地址，触发MMU硬件异常，OS发送11号SIGSEGV信号 又涉及到进程地址空间了…我们所写程序中的地址都是虚拟地址，是要通过页表转换为对应的物理地址的。野指针，数组的越界访问，总之就是进程试图访问不属于它的内存空间，或者试图读取或写入受保护的内存时，这时的虚拟地址一定是非法的，这个虚拟地址无效，内存管理单元(MMU)（硬件！）发出页面错误中断，操作系统会察觉到这个硬件异常，并发生SIGSEGV信号给对应进程。 （之前一直说通过页表进行虚拟地址到物理地址的转换，实际上，页表是软件，而这种地址转换发生的频率非常高，故即使是哈希这种O(1)的结构，也太慢了，所以，实际上进行地址转换是通过页表+MMU进行的，MMU是硬件，这里非法地址访问就是MMU硬件异常所引发OS发送SIGSEGV信号） void handler(int signo) { std::cout \u003c\u003c \"进程收到了一个\" \u003c\u003c signo \u003c\u003c \"号信号\" \u003c\u003c std::endl; } int main() { signal(SIGFPE, handler); signal(SIGSEGV, handler); // int i = 1/0; // 除零 int* p = nullptr; *p = 10; return 0; } 同上方除零异常一样，这里也是无限调用handler方法，原因就是这种硬件异常没有解决，一直存在。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:3:4","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"信号产生总结 上方所说多种产生信号的方式，不管方式是什么，最终一定是由OS向进程发送信号，而发送信号的本质为OS向进程PCB中的pending位图的对应比特位由0置1。最终都是由OS来执行，因为OS是进程的管理者！ ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:3:5","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"核心转储-Core Dump ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:4:0","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"核心转储概念 如上，Term表示这个信号的默认动作是终止这个进程，Core表示这个信号的默认动作是终止这个进程并核心转储（Core Dump）。它们的区别就是是否进行核心转储。 当一个进程要异常终止时,可以选择把进程的用户空间内存数据全部 保存到磁 盘上,文件名通常是core,这叫做Core Dump。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:4:1","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"核心转储作用 进程异常终止通常是因为有Bug,比如非法内存访问导致段错误, 事后可以用调试器检查core文件以查清错误原因,这叫做Post-mortem Debug（事后调试）。核心转储的作用就是为了方便调试。 注意 通常在云服务器这样的生产环境中核心转储功能是关闭的，也就是默认不允许产生core文件，因为core文件中可能包含用户密码等敏感信息,不安全。其次，core文件体积较大，每次进程因Core信号而异常终止时如果都会进行核心转储生成core文件，时间长了是很耗费磁盘空间的。 可以通过ulimit -c 10240命令开启core dump功能。 使用core.pid文件：gdp调试-\u003ecore-file core.pid即可。（配合gdb） ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:4:2","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"进程等待wait/waitpid的core dump标记位 之前在进程控制中的进程等待那里，可以传一个int* status的输出型参数获取子进程退出信息，当子进程被信号所杀时，status的第8个比特位就是core dump标记位（前7个比特位为终止信号的编号），若真的生成了core文件，则该标记位为1，否则为0。 注意：若被Action为Core的信号所杀，但是环境不允许生成core文件，则该标记位为0。也就是它标记的是当进程出现某种异常的时候，是否由OS将当前进程在内存中的相关核心数据，转存（dump）到磁盘中。 int main() { if(fork() == 0) { sleep(3); int* p = nullptr; *p = 10; // SIGSEGV默认处理动作为Core } int status = 0; waitpid(-1, \u0026status, 0); // 阻塞式等待 std::cout \u003c\u003c \"signal num : \" \u003c\u003c (status \u0026 0x7f) \u003c\u003c std::endl; std::cout \u003c\u003c \"core dump flag : \" \u003c\u003c ((status \u003e\u003e 7) \u0026 1) \u003c\u003c std::endl; return 0; } ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:4:3","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"信号保存：进程如何保存收到的信号 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:5:0","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"为什么要进行信号保存 在OS因某种原因给某进程发送了某信号之后，进程必须处理该信号，时机：进程是在合适的时候处理该信号的（由内核态转为用户态时，详见信号处理），所以，进程处理信号可能不是立即的，故进程需要将收到的信号保存起来。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:5:1","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"信号相关概念 实际执行信号的处理动作称为信号递达(Delivery) （默认，忽略，自定义捕捉） 信号从产生到递达之间的状态,称为信号未决(Pending) 进程可以选择**阻塞（block）**某个信号。 被阻塞的信号产生时将保持在未决状态,直到进程解除对此信号的阻塞,才执行递达的动作。 注意,阻塞和忽略是不同的,只要信号被阻塞就不会递达,而忽略是信号递达的一种具体方式。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:5:2","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"进程保存信号的方式（内核中的表示） block信号集（阻塞信号集，信号屏蔽字）：在进程PCB中本质是一个位图结构（sigset_t)。 每一个bit位用于表示对应信号是否被阻塞（屏蔽） pending信号集（未决信号集）：在进程PCB中本质是一个位图结构（sigset_t） 每一个bit位用于表示对应信号是否处于未决状态（简单理解就是进程是否收到了这个信号，等处理状态，具体处理不处理还要看是否被block） handler方法处理表：一个函数指针数组。void (* handlerArray[32]) (int); 信号处理三种方式：默认，忽略，自定义捕捉。这里存储的就是对应信号的具体处理方法，若存储的是SIG_DFL（本质是一个宏定义），就是该信号要执行默认处理逻辑，若SIG_IGN，就是忽略该信号，若自定义捕捉，则这里存的就是自定义捕捉方法的函数地址。（signal函数修改的就是handler方法处理表） 综上，也就是，进程收到信号之后，不一定会递达该信号，还要看该信号是否被该进程屏蔽（block）了。具体为什么要增加block信号这个功能呢？肯定是因为需要这个功能啊，有需求和使用场景… block信号集和pending信号集的结构完全一样，就是一个位图结构。只是具体比特位的意义不同，分别表示对应信号是否被阻塞和是否处于未决状态。故每个信号都有两个标志位分别表示阻塞(block)和未决(pending) pending信号集只能存储对应信号处于或不处于未决状态，因此若一个进程block某个信号，然后接收到了多次这个信号，实际上只会存储一次。解除block之后也只会递达一次。普通信号是这样的。而实时信号产生多次会以此放进一个队列中…不讨论 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:5:3","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"sigset_t 由上，我们已知pending信号集和block信号集本质就是一个位图结构，它们的类型其实就是sigset_t类型，sigset_t就是一个位图结构。 sigset_t是操作系统提供的自定义类型（语言会提供.h，.hpp以及语言的自定义类型，如int，double，OS也会提供.h和自定义类型，比如pid_t，sigset_t） sigset_t类型对于每种信号会用一个bit表示“有效”或“无效”状态。具体这个有效和无效在阻塞信号集和pending信号集中含义不同，阻塞信号集中有效和无效指的是该信号是否被阻塞，在未决信号集中指的是该信号是否处于未决状态… OS给我们提供了操作sigset_t（位图）的方法。也就是这个位图结构不建议我们去直接操作和修改，建议使用操作系统提供的接口。（因为linux内核是C语言写的，所以其实这个sigset_t位图就是一个struct，我们是可以获取到它的成员的） 位图信号集sigset_t操作函数 int sigemptyset(sigset_t *set); // 初始化信号集，所有信号对应bit清零 int sigfillset(sigset_t *set); // 初始化信号集，所有信号对应bit置位 int sigaddset (sigset_t *set, int signo); // 添加某种信号 int sigdelset(sigset_t *set, int signo); // 删除某种信号 int sigismember（const sigset_t *set, int signo); // 判断某信号在信号集中是否有效 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:5:4","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"pending信号集，block信号集，handler方法处理表相关的函数接口 // 0. 读取或更改进程的信号屏蔽字 int sigprocmask(int how, const sigset_t *set, sigset_t *oset); // oset为输出型参数，用于获取旧的信号屏蔽字 // 有关sigprocmask的how参数：传入下方宏定义 // SIG_BLOCK ：set包含了我们希望添加到当前信号屏蔽字的信号，相当于mask = mask | set // SIG_UNBLOCK : set包含了我们希望解除阻塞的信号，相当于 mask = mask \u0026 ~set // SIG_SETMASK : 设置当前信号屏蔽字为set所指向的sigset_t，相当于mask = set // 1. 读取当前调用进程的pending信号集（未决信号集） int sigpending(sigset_t *set); // 2. 自定义信号捕捉方法 typedef void (*sighandler_t)(int); sighandler_t signal(int signum, sighandler_t handler); // handler是回调函数，通过回调的方式，修改对应的信号捕捉方法。 signal系统调用：本质就是OS将调用进程的PCB中的handler方法表中signum对应函数指针修改为handler，即完成了signum信号的自定义捕捉，之后该进程进行signum信号递达时，就会调用handler方法（注意该方法在用户空间中，详见信号处理） 有block信号集的读写接口，有pending信号集的读接口，但是没有pending信号集的写接口。实际上，信号产生的方式就是写pending信号集的方式。kill -x pid ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:5:5","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"编码验证 若进程将所有信号都捕捉，则进程无敌？ void handler(int sig) { std::cout \u003c\u003c \"进程收到了一个\" \u003c\u003c sig \u003c\u003c \"号信号\" \u003c\u003c std::endl; } int main() { std::cout \u003c\u003c getpid() \u003c\u003c std::endl; for(int sig = 1; sig \u003c= 31; ++sig) { signal(sig, handler); } while(true) sleep(1); return 0; } 然后给进程发送1~31号信号，结果：9和19号信号无法被捕捉，且19号SIGSTOP暂停进程之后，发送18号SIGCONT信号，18号信号被捕捉了，会调用18号信号的捕捉方法，同时也会continue process，也就是生效了…这些其实不是很重要，大概率是一些特殊设计。只需要注意并非所有的普通信号都可以被捕捉。 验证将某信号block之后，向进程发送该信号，是否pending信号集中会有对应的1标记位出现. void showSigset(sigset_t* sigset) { // 此处信号集可能为pending信号集可能是block信号集。 for(int sig = 1; sig \u003c= 31; ++sig) { if(sigismember(sigset, sig) == 1) std::cout \u003c\u003c \"1\"; else std::cout \u003c\u003c \"0\"; } std::cout \u003c\u003c std::endl; } void handler(int sig) { std::cout \u003c\u003c \"进程收到了一个\" \u003c\u003c sig \u003c\u003c \"号信号\" \u003c\u003c std::endl; } int main() { for(int sig = 1; sig \u003c= 31; ++sig) { signal(sig, handler); } sigset_t sigset; sigprocmask(SIG_BLOCK, nullptr, \u0026sigset); // 获取进程初始时的block信号集 std::cout \u003c\u003c \"初始时，进程的block信号集为 : \"; showSigset(\u0026sigset); sigpending(\u0026sigset); std::cout \u003c\u003c \"初始时，进程的pending信号集为 : \"; showSigset(\u0026sigset); sigfillset(\u0026sigset); sigprocmask(SIG_BLOCK, \u0026sigset, nullptr); // 试图将全部信号进行block sigprocmask(SIG_BLOCK, nullptr, \u0026sigset); // 获取新的block信号集 std::cout \u003c\u003c \"试图将全部信号进行block之后，block信号集为 : \"; showSigset(\u0026sigset); sleep(5); int count = 0; while(true) { // 每秒打印一次pending信号集 sigpending(\u0026sigset); std::cout \u003c\u003c \"此时，pending信号集为 : \"; showSigset(\u0026sigset); sleep(1); count++; // if(count == 10) // { // // 将全部信号解除block，看是否会递达 // sigemptyset(\u0026sigset); // sigprocmask(SIG_SETMASK, \u0026sigset, nullptr); // 解除全部信号block // } } return 0; } 上方，试图将1~31号信号都block，然后查看进程的信号屏蔽字，发现9号SIGKILL和19号SIGSTOP信号无法被阻塞，然后按编号顺序向进程每隔1s发送一个信号，不发9和19，结果如图，确实pending信号集中会存储因为被block而处于未决态的信号。 唯一超出预期的是18号信号，18号信号确实被block了，直接向进程发送18号信号，18号的pending信号集的比特位确实会变为1，但是当向进程发送19，20，21，22号时（这些信号的默认动作都是STOP），18号信号的比特位会变为0，也就是进行了信号递达…其实在上一个示例中就说过了SIGCONT的特殊，应该是特殊处理了，大致了解即可。 故，9号SIGKILL和19号SIGSTOP信号无法被block，无法被捕捉，无法被忽略。这也是为了防止，若恶意进程将所有信号block，则user和OS将无法杀死进程。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:5:6","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"信号处理：进程处理信号的时机和流程 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:6:0","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"进程什么时候处理信号 OS向进程发送信号之后（pending信号集中某信号处于未决状态），那么，进程不是立即处理该信号的，而是在合适的时候（这也是为什么要把信号保存起来）。那么这个合适的时候是什么时候呢？ 注意，有关信号的内核数据结构，如pending信号集，block信号集和handler方法处理表都在PCB中，这是内核数据，属于内核范畴。故，想要处理信号，就必须访问这些内核数据，则此时CPU的状态必须处于内核态（因为只有内核态才有权访问内核数据）。相对的，CPU运行用户程序，执行用户代码时，就处于用户态。 用户态和内核态的概念：内核态和用户态是操作系统内核与用户进程之间的两种不同状态。在这两种状态中，操作系统内核和用户进程对于系统资源和硬件设备的访问权限是不同的。 **内核态是 CPU 执行操作系统内核代码的状态。**在内核态中， CPU 拥有所有特权，可以执行所有底层操作，并具有访问所有内存地址和设备的权限。这种状态下，内核能够执行所有系统调用和硬件中断处理。所有关于操作系统内核的操作都需要在内核态下执行。 **用户态是 CPU 执行用户进程代码的状态。**在用户态中，进程只能访问被分配给它的内存，并且不能直接访问硬件和其他进程的内存。**当一个进程需要进行一些特殊操作，如请求系统调用、执行系统调用，则需要从用户态切换到内核态。**因此用户态下的进程不能进行一些底层的操作，对系统资源和硬件设备的访问也有限制，这些操作需要通过系统调用来完成。这种方式可以保证系统安全。 划分内核态和用户态的主要目的是为了保护操作系统和其他进程免受用户进程的损害。 在内核态下，操作系统内核拥有所有特权，可以执行所有底层操作和访问所有内存地址和设备，因此可以保证系统的稳定性和安全性。而在用户态下，用户进程只能访问被分配给它的内存，并且不能直接访问硬件和其他进程的内存，这样可以防止用户进程对其他进程和操作系统产生影响。 通过这种方式，可以将操作系统的核心代码和用户进程的代码分离开来，保护操作系统免受用户进程的损害。当然这样也带来了一些性能的损失，因为要在两种状态间不断地切换, 但相对于安全性来说是值得的 结合进程地址空间 进程地址空间分为0~3G用户地址空间和3~4G内核地址空间（32位），用户空间通过用户级页表映射到物理内存中的用户数据和用户代码。内核空间通过内核级页表映射到物理内存中的内核数据和内核代码。 通常，用户级页表每个进程都有一个，因为它们映射的物理内存不同，实现了进程独立性。而操作系统在物理内存中只有一个，内核级页表也只需要一个即可，内核级页表映射到物理内存中的内核数据和内核代码，这个内核级页表是所有进程共享的。 结合，内核态就是CPU执行内核代码的状态，用户态就是CPU执行用户代码的状态。比如用户代码中调用了系统调用接口，就会从从用户态转为内核态（接口中有转换的指令），否则用户态无法执行内核代码。其实这个和调用动态库内的函数接口没什么本质区别，只是动态库代码在共享区，而系统接口代码在内核空间中。都是在进程地址空间内不断跳转完成的。用户代码可以直接跳转执行动态库中的代码，因为都属于用户空间的代码。 哪些情况下会从用户态转为内核态？ 最典型的：用户代码调用系统接口，这个系统调用接口就属于内核代码，执行内核代码必须处于内核态，而系统调用接口编译之后形成的汇编指令中就有从用户态向内核态转变的指令，比如int 80（这里的int并非C语言的基本类型，而是一种汇编指令，意为interrupt）。这也就说明了，OS可以访问并管理硬件，而进程若想访问硬件，则必须调用系统调用接口通过OS访问，因为只有合法的系统调用接口里面才有向内核态的转换，否则用户态是无法直接执行内核代码或访问内核数据的，因为没有权限。这也是OS对硬件保护的体现。 其次：若某进程的调度时间到了，OS需要进行进程切换，此时执行的一定是内核代码 故并非一个进程一直执行用户代码就不会进入内核态，像这种进程调度切换是无法避免的。 还有：缺陷，中断，异常等的发生也会使CPU进入内核态执行OS内核代码。 关于内核态和用户态我的理解… 内存中有很多代码，都是二进制序列，CPU会不断地执行代码，代码分为用户代码和内核代码。CPU在执行代码时会记录此时执行的代码是用户的代码还是内核的代码，对应的就是用户态和内核态，做此区分的主要目的就是为了限制用户态的权限，也就是如果此时是用户的代码，则此代码不能访问那些用户没有权限访问的数据和代码（其实代码也属于数据的一种），比如内核数据，其他进程的数据。这其实就是为了保护OS，保护硬件，保护其他进程，提高稳定性。 而用户的代码若想完成某些功能，访问某些硬件或数据，则必须通过系统调用，系统调用属于内核代码，且其中就有用户态向内核态的转换指令 综上，了解了用户态和内核态概念，区别，划分目的，结合进程地址空间，以及转换场景。 进程进行信号处理的时间为：从内核态转回用户态时，进行信号检测和处理。 也就是因某种原因进入了内核态，执行完内核代码之后，需要返回到用户代码中继续执行之前，进行信号检测和处理。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:6:1","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"进程处理信号的流程 OS给进程发送信号之后（修改pending信号集）-\u003e进程因某原因陷入内核，进入内核态-\u003e返回用户态之前进行信号检测和处理-\u003e大致处理流程：检测pending信号集是否有信号处于未决状态-\u003e若有，看对应信号是否被阻塞-\u003e若阻塞，则信号处理完成（因为此时不可进行信号递达），返回用户态继续执行主控制流程的用户代码-\u003e若不阻塞，则看对应信号的handler方法处理表中函数指针 若信号处理方法为忽略（1强转为函数指针类型），则将pending信号集的对应比特位由1置0即可（忽略也是信号递达的一种方式）。信号处理结束，返回用户模式，从主控制流程中上次被中断的地方继续向下执行。 若信号处理方法为默认（0强转为函数指针类型），则执行信号的默认处理方法，如终止，暂停，继续，忽略（这里的忽略为默认处理方法）。这里因为具体处理行为不定，故要不要再返回用户态是不确定的，因为进程可能直接终止。但是，此时是处于内核态的，故，执行很多OS方法都有权限，可以直接执行。 若信号处理方法为用户自定义的捕捉方法，则此时的流程与默认，忽略不尽相同。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:6:2","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"捕捉信号的流程 因某原因进入内核，未决，未阻塞…与默认和忽略的信号处理流程的不同之处在于 若信号的handler处理方法表中存储的是用户空间中自定义捕捉方法的函数地址，则此时需转为用户态去执行捕捉函数（上图的sighandler），注意，内核态是有权执行用户代码的，但这是不安全的，因为用户代码是否有非法操作是不确定的，故需要先转为用户态，再执行捕捉方法。执行完之后，通过系统调用sigreturn再次进入内核，做一些信号处理的收尾工作，如将pending信号集的对应信号比特位由1置0（可能这个操作并非此时进行），检测是否有新的信号产生…至此，信号捕捉完成，返回用户模式，从主控制流程中上次被中断的地方继续向下执行。（此处要恢复main函数的上下文） 如上图为记忆捕捉信号流程的示意图，一共进行了四次用户态和内核态之间的转变（蓝色标记），进行信号检测的时机是从内核态转回用户态之前（红色标记），上图仅适用于handler方法表中为自定义捕捉函数时。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:6:3","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"信号捕捉方法2.0：sigaction 前面说了signal函数用于捕捉信号，设定信号的自定义捕捉方法。此处的sigaction也是这个功能，并有一些拓展功能。 #include \u003csignal.h\u003e int sigaction(int signo, const struct sigaction *act, struct sigaction *oact); // 系统提供的同名结构体类型sigaction struct sigaction { void (*sa_handler)(int); // signo的自定义捕捉方法 void (*sa_sigaction)(int, siginfo_t *, void *); // 有关实时信号，不关心 sigset_t sa_mask; // 拓展功能，设定执行信号的处理函数时，block信号集的内容。 int sa_flags; // 不关心，设为0即可 void (*sa_restorer)(void); // 不关心 }; 当某个信号的处理函数被调用时,内核自动将当前信号加入进程的信号屏蔽字,当信号处理函数返回时自动恢复原来 的信号屏蔽字,这样就保证了在处理某个信号时,如果这种信号再次产生,那么它会被阻塞到当前处理结束为止。处理结束后，会再次递达此信号。 // 验证：当信号捕捉函数处理时，OS是否会自动阻塞该信号 void showSigset(sigset_t * p) { for(int i = 0; i \u003c= 31; ++i) { if(sigismember(p, i) == 1) std::cout \u003c\u003c \"1\"; else std::cout \u003c\u003c \"0\"; } std::cout \u003c\u003c std::endl; } void handler2(int signo) { std::cout \u003c\u003c \"进程收到了一个\" \u003c\u003c signo \u003c\u003c \"号信号\" \u003c\u003c std::endl; sigset_t sigset; sigprocmask(SIG_BLOCK, nullptr, \u0026sigset); std::cout \u003c\u003c \"信号捕捉方法执行时，block信号集为 : \"; showSigset(\u0026sigset); sleep(10); std::cout \u003c\u003c \"2号信号捕捉方法执行结束\" \u003c\u003c std::endl; } void handler3(int signo) { std::cout \u003c\u003c \"执行3号信号捕捉方法\" \u003c\u003c std::endl; } int main() { sigset_t sigset; sigprocmask(SIG_BLOCK, nullptr, \u0026sigset); std::cout \u003c\u003c \"初始时，block信号集为 : \"; showSigset(\u0026sigset); signal(2, handler2); signal(3, handler3); while(true) sleep(1); return 0; } 如图，确实在信号捕捉方法执行时，会阻塞当前处理信号。但是其他信号不会被block，处理2号时，3号信号产生，会直接去处理三号。 如果在调用信号处理函数时,除了当前信号被自动屏蔽之外,还希望自动屏蔽另外一些信号,则用sigaction的sa_mask字段说明这些需要额外屏蔽的信号,当信号处理函数返回时自动恢复原来的信号屏蔽字。 void showSigset(sigset_t * p) { for(int i = 1; i \u003c= 31; ++i) { if(sigismember(p, i) == 1) std::cout \u003c\u003c \"1\"; else std::cout \u003c\u003c \"0\"; } std::cout \u003c\u003c std::endl; } void handler2(int signo) { std::cout \u003c\u003c \"进程收到了一个\" \u003c\u003c signo \u003c\u003c \"号信号\" \u003c\u003c std::endl; sigset_t sigset; sigprocmask(SIG_BLOCK, nullptr, \u0026sigset); std::cout \u003c\u003c \"信号捕捉方法执行时，block信号集为 : \"; showSigset(\u0026sigset); sleep(10); std::cout \u003c\u003c \"2号信号捕捉方法执行结束\" \u003c\u003c std::endl; } void handler3(int signo) { std::cout \u003c\u003c \"执行3号信号捕捉方法\" \u003c\u003c std::endl; } // 验证sigaction int main() { signal(3, handler3); struct sigaction sa; sa.sa_flags = 0; sigemptyset(\u0026sa.sa_mask); sigaddset(\u0026sa.sa_mask, 3); // 2号处理时，将3号也屏蔽 sa.sa_handler = handler2; // 使用sigaction捕捉2号信号 sigaction(2, \u0026sa, nullptr); // 使用sigaction捕捉2号信号 while(true) sleep(1); return 0; } 如上，在2号信号的捕捉方法执行时，发送3号信号，此时3号被block（sigaction的sa_mask字段），并没有立即递达，而是在2号捕捉方法执行完成后，恢复原来的信号屏蔽字，3号解除阻塞（此时2号也解除），递达3号。 这里也算是解释了OS设计block这个功能的本质原因（之一），也就是信号处理过程中，又来了同样的信号，怎么办？因为OS会自动阻塞该信号，故不会立即递达。处理完成后会恢复为原来的信号屏蔽字。 最后一句：信号捕捉并没有创建新的进程或线程…（突然的一句，和上方无直接关联） 这肯定是呀~….. ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:6:4","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"可重入函数vs不可重入函数 main调用insert向链表head中头插一个结点node1，insert操作分为两步，执行完第一步之后，因硬件中断使进程切换到内核，返回用户态之前进行信号检测和处理，检测到信号之后调用捕捉方法：sighandler（用户空间），sighandler中也调用了insert方法向同一个链表中插入一个node2结点。insert(\u0026node2);和sighandler都执行完之后，返回内核态，再次返回用户态就会从上次被中断的地方继续向下执行，即insert(\u0026node1)的第二步操作。最后执行完insert(\u0026node1)之后，main和sighandler先后向链表中插入两个结点，最后只有一个结点被真正插入链表中了。 可重入函数(reentrant function)是指在一个函数调用过程中，如果它被其他函数调用，并且在第二次调用结束后能够正常返回到第一次调用，那么这个函数就是可重入函数。 相反，不可重入函数(non-reentrant function)就是不能在同一时间被多个函数调用的函数。 重入一个函数就是在这个函数正在运行时再次调用它。如果这个函数是可重入的，那么它会正常处理这个重入请求；否则，可能会发生错误或者不可预料的结果。 如上，insert函数在同一时间被main和sighandler调用，因为insert访问一个全局链表，可能因为重入而造成错乱。像这样的函数就称为不可重入函数。如果一个函数只访问自己的局部变量和参数，则称为可重入函数。（想想为什么，函数栈帧！） 当一个函数访问了全局，static变量，则就是不可重入的，包括errno也是全局数据。再比如使用了malloc/free（因为malloc也是使用全局链表来管理堆的），调用了标准库IO函数（标准库IO函数很多都是以不可重入的方式使用全局数据）。则就称为不可重入函数（可重入和不可重入是函数的一种特征） ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:7:0","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"volatile 之前在C++的地方就简单学过这个关键字。修改const变量那里？reinterpret_cast // volatile int num = 0; void handler(int signo) { std::cout \u003c\u003c \"num : \" \u003c\u003c num; num = 1; std::cout \u003c\u003c \"-\u003e\" \u003c\u003c num \u003c\u003c std::endl; } int main() { signal(2, handler); while(num == 0) ; std::cout \u003c\u003c \"while已退出，num != 0 num : \" \u003c\u003c num \u003c\u003c std::endl; return 0; } 1、不加volatile，默认优化程度。2、不加volatile，-O3优化程度。3、加volatile，-O3优化程度。 第二种-O3优化程度时(更高级别的优化)，编译器检测到main函数中对于这个num全局变量没有修改的语句，就进行优化行为：将num加载到CPU内寄存器中，如edx？之后while语句取num时，不再从内存中取，而是直接取寄存器中的num。这也就导致了handler将num改变之后，while仍旧没有推出。 当volatile int num = 0;之后，用volatile声明num表示拒绝优化行为，每次取num都使用move指令去内存中取，保证了内存的可见性。 注意，编译器的优化行为是在程序编译时进行的，也就是编译器进行的优化行为，而不是程序运行时，程序运行起来之后，程序的行为就已经确定了。 ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:8:0","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"SIGCHID 进程等待那里讲过wait和waitpid函数可用于避免僵尸进程和获取子进程退出信息。父进程可以阻塞等待子进程结束,也可以非阻塞地查询是否有子进程结束等待清理(也就是轮询的方式)。采用第一种方式,父进程阻塞了就不能处理自己的工作了;采用第二种方式,父进程在处理自己的工作的同时还要记得时不时地轮询一下,程序实现复杂。 其实,子进程在终止时会给父进程发17号SIGCHLD信号，该信号的默认处理动作是忽略（IGN，这里是默认方式的忽略，并非三种处理方式中的忽略，其实大多情况下没有区别）。 利用子进程退出时向父进程发送SIGCHID信号，父进程可以捕捉此信号，父进程调用自定义处理函数去回收子进程或获取子进程退出信息。这样父进程就不必关心子进程了（避免阻塞或轮询方式处理子进程）。 void handler(int signo) { std::cout \u003c\u003c \"父进程收到了由子进程发送的\" \u003c\u003c signo \u003c\u003c \"号信号\" \u003c\u003c std::endl; pid_t id = 0; int status = 0; while((id = waitpid(-1, \u0026status, WNOHANG)) \u003e 0) { // 回收到了一个子进程 if(WIFEXITED(status)) std::cout \u003c\u003c \"子进程退出码为 : \" \u003c\u003c WEXITSTATUS(status) \u003c\u003c std::endl; else std::cout \u003c\u003c \"子进程退出信号为 : \" \u003c\u003c (status \u0026 0x7f) \u003c\u003c \"if core dump : \" \u003c\u003c (status \u003e\u003e 7 \u0026 1) \u003c\u003c std::endl; } std::cout \u003c\u003c \"子进程回收成功\" \u003c\u003c std::endl; } int main() { if(fork() == 0) { // 子进程 sleep(3); exit(1); // 退出码为1 } signal(SIGCHLD, handler); while(true) { printf(\"parent process is working\\n\"); sleep(1); } return 0; } 如上，可以通过捕捉SIGCHLD信号的方式回收子进程和获取子进程退出信息。 注意点：若父进程创建多个子进程，则在处理SIGCHLD时，此时具体退出的子进程数量不确定，故不能只调用一次waitpid（WNOHANG），因为SIGCHLD是普通信号，只能记录是否处于未决状态，信号数量不能保存。所以需要采用上方while循环方式不断回收，直到回收完全部的退出子进程。或者也可以把子进程pid放在一个全局vector中，每次都非阻塞式遍历等待子进程。（在只有一个子进程的情况下不需要） 上方while循环，若10个子进程中6个退出了，则会循环7次。第七次返回负数，条件不满足。 子进程并非只有终止时发送SIGCHLD给父进程，暂停时也会，故父进程在捕捉SIGCHLD的方法中必须非阻塞式等待，若阻塞式等待子进程可能会影响父进程的执行。 若我们想更方便地回收子进程避免僵尸进程，且不关心子进程退出情况（也就是不想在处理函数中调用wait/waitpid），有什么更便捷的方式吗？ 事实上，由于UNIX 的历史原因，父进程调用signal或sigaction将SIGCHLD的处理方式设为SIG_IGN（忽略），这样，fork出的子进程在终止时会自动清理掉，不会产生僵尸进程。 注意，系统默认处理方式的忽略和用户用sigaction函数设定的SIG_IGN忽略通常没有区别，但这是一个特例。此方法对于Linux可用,但不保证 在其它UNIX系统上都可用。 int main() { if(fork() == 0) { std::cout \u003c\u003c \"child process pid : \" \u003c\u003c getpid() \u003c\u003c std::endl; sleep(5); exit(0); } signal(SIGCHLD, SIG_IGN); // 忽略SIGCHLD while(true) ; return 0; } 5s后子进程自动终止且没有生成僵尸进程。 若不将SIGCHLD的处理方式设为SIG_IGNs 则子进程会变为僵尸进程defunct ","date":"0001-01-01","objectID":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/:9:0","tags":null,"title":"","uri":"/linux7_%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/"},{"categories":null,"content":"线程概念 线程在进程内部执行，是OS调度的基本单位。 线程是进程的一个执行分支，是在进程内部运行的一个执行流。 在一个程序里的一个执行路线就叫做线程（thread）。更准确的定义是：线程是“一个进程内部的控制序列”。 线程（英语：thread）是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。 历史上，早期，操作系统中是没有线程的，也就是只有进程这个概念，一个进程内只有一个执行流。60年代，在OS中能拥有资源和独立运行的基本单位是进程，然而随着计算机技术的发展，进程出现了很多弊端，一是由于进程是资源拥有者，创建、撤消与切换存在较大的时空开销，因此需要引入轻型进程；二是由于对称多处理机（SMP）出现，可以满足多个运行单位，而多个进程并行开销过大。 因此在80年代，出现了能独立运行的基本单位——线程（Threads）。 线程有着自己的特点，比如执行粒度更细，更轻量化，调度切换的开销更小等等，而不同操作系统设计线程时有着不同的方案，Windows对于线程，设立了全新的数据结构，进程和线程划分的很清晰，这是比较复杂的。而因为线程创建，执行，切换，销毁等等很多行为都和进程有着很大的相似性，因此Linux采用了用进程模拟线程的设计方案（实现了进程内核代码的复用），这样的设计方案虽然没有为线程设计全新的数据结构，但是最终设计出的“轻量级进程”依旧符合线程的要求。 上图为学习线程之前，进程加载运行的示意图，每个进程都有一个task_struct（Linux），即进程PCB。 Linux线程的原理 Linux线程原理：OS内，如果我们创建“进程”时，不创建新的地址空间，用户级页表，不进行IO将程序的代码和数据加载到内存 只创建task_struct，让这个新的PCB指向旧的PCB（创建此新线程的主线程)指向的地址空间mm_struct，再通过一定的技术手段，将当前进程的资源合理划分给不同的task_struct，此时，这里的每一个task_struct，就称为一个线程。 线程在进程内部执行，指的是线程在进程的地址空间内运行。 每个进程内至少有一个执行线程（一个执行流） 在Linux系统中，在CPU眼中，看到的PCB都要比传统的进程的PCB更加轻量化，因此将Linux下的进程统一称之为轻量级进程。（多执行流时，PCB task_struct占用整个进程的一部分资源，当然轻量化） 之前一个进程，代码执行流程一定是按顺序执行的，多线程之后，就可以进行资源划分，所有线程共享一个地址空间，一个页表。此时的执行就由串型执行变为并发执行，效率更高。(??? 我们之前所写的程序为内部只有一个执行流的进程。而多线程即内部具有多个执行流的进程。 重新理解定义task_struct：进程内部的一个执行流（Linux下） CPU执行的基本单位是线程，OS调度的基本单位是线程。但并不是OS只能调度线程，实际上OS也是可以以进程整体为单位进行调度的。（见下方疑问） 重新理解进程的概念 如上红色区域即进程全部。 从用户视角来说：进程=内核数据结构+进程对应的代码和数据（内核数据结构中PCB的数量\u003e= 1） 从内核视角来说：进程：承担分配系统资源的基本实体。（因为在进程创建时，系统给这个进程分配资源。而线程是使用创建此线程的进程的部分资源，进程进行资源分配，分配给线程。故进程才是承担分配系统资源的基本实体） 疑问 对于线程这块我有个疑惑，既然CPU和OS调度的基本单位是线程，且一个进程内至少有一个执行线程（一个执行流），那能不能说CPU和OS只能调度线程呢，也就是理解为CPU调度一个单执行流进程时，本质上也是调度此进程内的一个执行线程？ 一个CSDN链接，可供参考 调度是一方面，另一方面是调度的目的是什么。如果调度的目的是为了让执行流去执行，那肯定是让线程去跑。而如果目的是以进程整体为单位进行资源分配，则OS也是可以做到调度整个进程的。对于CPU来说，特别是Linux下，CPU并不关心线程还是进程，它只关心task_struct，因为在Linux下，只存在轻量级进程，不存在线程。只是用轻量级进程去模拟线程。 线程优缺点 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:0:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程的优点 创建一个新线程的代价要比创建一个新进程小得多（创建时的成本更小 与进程之间的切换相比，线程之间的切换需要操作系统做的工作要少很多（切换的成本更小 线程占用的资源要比进程少很多（占用的资源更少 能充分利用多处理器的可并行数量（并行 在等待慢速I/O操作结束的同时，程序可执行其他的计算任务 计算密集型应用，为了能在多处理器系统上运行，将计算分解到多个线程中实现 I/O密集型应用，为了提高性能，将I/O操作重叠。线程可以同时等待不同的I/O操作。 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:1:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程的缺点 性能损失 一个很少被外部事件阻塞的计算密集型线程往往无法与其它线程共享同一个处理器。如果计算密集型线程的数量比可用的处理器多，那么可能会有较大的性能损失，这里的性能损失指的是增加了额外的同步和调度开销，而可用的资源不变。 健壮性降低 编写多线程需要更全面更深入的考虑，在一个多线程程序里，因时间分配上的细微偏差或者因共享了不该共享的变量而造成不良影响的可能性是很大的，换句话说线程之间是缺乏保护的。 缺乏访问控制 进程是访问控制的基本粒度，在一个线程中调用某些OS函数会对整个进程造成影响。 编程难度提高 编写与调试一个多线程程序比单线程程序困难得多 线程异常 单个线程如果出现除零，野指针问题导致线程崩溃，进程也会随着崩溃 线程是进程的执行分支，线程出异常，就类似进程出异常，进而触发信号机制，终止进程，进程终止，该进程内的所有线程也就随即退出 线程用途 合理的使用多线程，能提高CPU密集型程序的执行效率 合理的使用多线程，能提高IO密集型程序的用户体验（如生活中我们一边写代码一边下载开发工具，就是 多线程运行的一种表现） 进程vs线程 进程是资源分配的基本单位 线程是调度的基本单位。 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:2:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程共享进程内的部分数据 **进程内的所有线程共享同一个进程地址空间，则其中的代码区，全局数据区，共享区，命令行参数和环境变量，内核区都是共享的。**而对于堆区和栈区，根本上来说是共享的，因为一个线程可以将栈帧内的局部数据或堆区开辟空间的地址通过全局数据的方式传递给其他线程，其他线程也可以访问。但是一般情况下我们不会这样做，所以也可以认为栈区和堆区是线程私有的。 除此之外，各线程还共享进程内的文件描述符表（一个线程打开一个文件，其他线程也会自动打开），各种信号的处理方式（SIG_IGN，SIG_DFL或者自定义的信号处理函数（代码区）），当前工作目录，用户id和组id等… ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:3:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"属于线程自己的一部分数据 线程id，errno，信号屏蔽字，调度优先级。 最重要的是：每个线程私有 一组寄存器和栈。 一组寄存器：线程是CPU调度的基本单位，每个线程一定有自己的上下文。在线程被CPU调度时，上下文数据就会保存在CPU内的一组寄存器中。 栈：每个线程运行时要调用函数，一定有出栈入栈的行为，形成的临时变量会保存在栈内，故每个线程必须有自己的私有栈。 一组寄存器和栈能体现出线程的动态属性。 验证每个线程有独立的信号屏蔽字 #include \u003ciostream\u003e #include \u003cstring\u003e #include \u003cthread\u003e #include \u003csignal.h\u003e #include \u003cunistd.h\u003e #include \u003cstring\u003e using namespace std; void showSigblock(sigset_t* curSigset); void *routine(void *arg) { int cnt = 0; while (true) { cnt++; sleep(1); cout \u003c\u003c \"新线程的信号屏蔽字为 : \"; sigset_t curSigset; sigprocmask(SIG_BLOCK, nullptr, \u0026curSigset); showSigblock(\u0026curSigset); if(cnt == 5) break; } string *p = new string(\"新线程执行完毕\"); pthread_exit(reinterpret_cast\u003cvoid*\u003e(p)); } void showSigblock(sigset_t* curSigset) { for (int i = 1; i \u003c= 31; ++i) { if (sigismember(curSigset, i)) cout \u003c\u003c \"1\"; else cout \u003c\u003c \"0\"; } cout \u003c\u003c endl; } int main() { // 主线程 sigset_t sigset; sigemptyset(\u0026sigset); sigaddset(\u0026sigset, 8); sigaddset(\u0026sigset, 1); sigaddset(\u0026sigset, 11); sigprocmask(SIG_BLOCK, \u0026sigset, nullptr); sigset_t curSigset; sigprocmask(SIG_BLOCK, nullptr, \u0026curSigset); cout \u003c\u003c \"主线程此时的信号屏蔽字为 : \"; showSigblock(\u0026curSigset); pthread_t pid; pthread_create(\u0026pid, nullptr, routine, nullptr); sleep(3); sigaddset(\u0026sigset, 3); sigprocmask(SIG_BLOCK, \u0026sigset, nullptr); cout \u003c\u003c \"主线程成功将3号信号进行屏蔽 : \"; sigprocmask(SIG_BLOCK, nullptr, \u0026curSigset); showSigblock(\u0026curSigset); void* retThread = nullptr; pthread_join(pid, \u0026retThread); cout \u003c\u003c *(string*)retThread \u003c\u003c endl; delete (string*)retThread; return 0; } .[yzl@VM-4-5-centos Thread]$ ./mythread 主线程此时的信号屏蔽字为 : 1000000100100000000000000000000 新线程的信号屏蔽字为 : 1000000100100000000000000000000 新线程的信号屏蔽字为 : 1000000100100000000000000000000 主线程成功屏蔽3号信号: 1010000100100000000000000000000 新线程的信号屏蔽字为 : 1000000100100000000000000000000 新线程的信号屏蔽字为 : 1000000100100000000000000000000 新线程的信号屏蔽字为 : 1000000100100000000000000000000 新线程执行完毕 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:4:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"为什么线程切换的成本更低，而进程切换成本更高 进程内的线程之间切换时，进程地址空间和页表不需要切换（尽管它们在CPU内就是一个寄存器值），CPU内的进程范畴的状态寄存器也不需要切换。这些成本不是很高，故不是最重要的。 重点是：CPU内有L1~L3的cache（高速缓存），对内存的代码和数据，根据局部性原理，预读到CPU内部。若进程切换，则cache立即失效，切换为新进程之后，cache需要进行预热和重新缓存。这个的影响更大。 GPT：线程切换的成本比进程切换的成本低的原因是：线程是轻量级的，共享父进程的资源，切换时需要保存的状态信息相对较少；而进程是独立的，拥有自己的资源，**切换时需要保存的状态信息相对较多。**因此线程切换的速度比进程切换的速度快。 线程控制 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:5:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"pthread库介绍 Linux操作系统并没有设计真正的线程，而是用轻量级进程来模拟线程。故Linux无法直接提供针对线程的系统调用（如线程创建，终止…），最多只能提供轻量级进程的系统接口。但是Linux必须满足用户对于线程的使用需求。最终，在用户层提供了一个pthread库，这里面包含了用户对于线程的使用接口。严格来说，这个pthread库是第三方库，并不是系统库和语言库。但是所有版本的Linux操作系统都直接提供了这个线程库在系统默认路径中。（上图为动静态库和头文件） 因为这个库严格来说并不是系统库和语言库，而是第三方库。故用g++编译时要加上-lpthread指令说明要链接这个库。（g++默认使用动态库进行动态链接，头文件在源文件中已经声明了，而因为libpthread-2.17.so已经在系统默认搜索路径下了，故编译好之后可以直接运行可执行，不会出现链接错误） ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:6:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"pthread线程库API pthread_create - create a new thread int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg); pthread_join - join with a terminated thread int pthread_join(pthread_t thread, void **retval); pthread_exit - terminate calling thread void pthread_exit(void *retval); pthread_cancel - send a cancellation request to a thread int pthread_cancel(pthread_t thread); pthread_detach - detach a thread int pthread_detach(pthread_t thread); pthread_self - obtain ID of the calling thread pthread_t pthread_self(void); ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程创建：pthread_create 线程id地址（本质是一个unsigned long int类型），输出型参数。线程属性设置，直接设为nullptr即可。新线程的执行函数，函数指针类型，参数和返回值必须为void*（一般是通过强转进行的）。线程执行方法的参数。 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:1","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程等待：pthread_join 1.线程id，2.用void**接收线程执行的start_routine函数的返回值void*。这里传void**接收，是C语言典型的输出型参数指针传参问题… ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:2","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程等待的原因 假设主线程中创建了一个新线程并且分配了一个任务，如果不等待新线程，那么主线程可能在新线程未完成任务时就已经结束了，这可能导致新线程的数据不一致或内存泄漏的问题。因此，调用pthread_join方法等待新线程可以保证主线程在新线程完成任务后再继续执行。 如果主线程不需要确保在新线程将任务执行完之后再继续执行，也需要调用pthread_join等待回收一下新线程，这样可以避免产生类似于僵尸进程的问题。 已经退出的线程，其空间没有被释放，仍然在进程的地址空间内。 创建新的线程不会复用刚才退出线程的地址空间。 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:3","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程终止：pthread_exit 线程执行的start_routine函数执行return 调用pthread_exit(void* retval)进行线程退出，retval就是return的返回值。 pthread_cancel(pthread_t thread)：如果主线程调用pthread_cancel取消新线程，则新线程会返回PTHREAD_CANCELED，宏定义，-1。（一般不建议这样做） 需要注意,pthread_exit或者return返回的指针所指向的内存单元必须是全局的或者是用malloc分配的,不能在线程函 数的栈上分配,因为当其它线程得到这个返回指针时线程函数已经退出了。 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:4","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程分离：pthread_detach 若不关心一个新线程的退出结果，则join是一个负担，（若不join会造成无法释放资源，造成系统泄漏），在新线程终止后也不想join回收新线程（如果不关心退出结果，那肯定不想join啊）。则可以调用pthread_detach分离新线程。作用类似于signal(SIGCHLD, SIG_IGN) 在Linux下，调用pthread_detach函数将线程分离，使得该线程独立运行，不再需要由任何其他线程等待其终止。分离的线程不需要被回收，内存资源在线程终止时自动回收。 #include \u003ciostream\u003e #include \u003cstring\u003e #include \u003cthread\u003e #include \u003csignal.h\u003e #include \u003cunistd.h\u003e #include \u003cstring\u003e using namespace std; #define N 10 void* routine(void* arg) { long long num = 0; for(int i = 0; i \u003c N; ++i) { if(((int*)arg)[i] % 2 == 0) num++; } pthread_exit((void*)num); // return (void*)num; // 偶数个数 } int main() { int* p = new int[N]; srand(time(nullptr)); for(int i = 0; i \u003c N; ++i) { p[i] = rand()%100; } pthread_t pid = 0; pthread_create(\u0026pid, nullptr, routine, (void*)p); void* retVal; pthread_join(pid, \u0026retVal); cout \u003c\u003c \"共\" \u003c\u003c (long long)retVal \u003c\u003c \"个偶数\" \u003c\u003c endl; return 0; } ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:5","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"补充 一般情况下，不建议调用pthread_cancel进行线程取消。 新线程cancel主线程，或者主线程调用pthread_exit退出，新线程都可以继续执行，但是不建议这样做，主线程会处于defunct状态。 多进程和多线程中，都是让父进程和主线程最后退出，父进程和主线程承担起创建和回收资源的职责。通常多线程编程就是，pthread_create创建新线程，新线程pthread_exit或者return，主线程pthread_join，若不关心退出结果且不想join等待回收则pthread_detach即可。 进程中任何一个线程发生异常，收到信号。都会导致整个进程终止。进程中所有线程也会终止。同理，任何一个线程调用execl，整个进程就会发生进程替换。 joinable和分离是冲突的，一个线程不能既是joinable又是分离的。 线程id，线程的独立栈结构，线程的局部存储 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:7:6","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程id的本质 前面说了，Linux操作系统只设计提供了轻量级进程，而用户需要的是线程。于是有了用户层的pthread线程库。 因此，对于线程的管理，OS承担一部分，pthread库承担一部分。OS承担的是对于轻量级进程的创建调度等和对于内核数据结构的管理。而库需要进一步包装描述内核的轻量级进程，需要提供一些线程相关的属性字段，比如：线程id。 OS完成的是对轻量级进程的调度，管理工作。而线程的用户层属性是pthread库管理的，管理就要先描述再组织。 线程id就是这个线程在pthread库中的属性集合的起始地址。pthread库，如动态库是会加载到进程地址空间中的共享区的，因此线程id本质就是共享区的一个地址。 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:8:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程的独立栈结构 之前我们说过，线程有两个很重要的私有数据：一组寄存器和栈。 进程内的所有线程共享一个进程地址空间，进程地址空间中代码区，全局数据区，堆区，共享区，内核区都没有问题，可是如何保证每个线程有自己独立的栈结构？所有线程共享一个栈空间是不可以的。 新线程的栈空间是用户层pthread库提供并维护的。 如上图所示，主线程使用的是进程地址空间中内核级的栈区，而新线程使用的是pthread库提供的用户级栈区。从而保证每个线程有自己独立的栈结构。 上图为验证线程id的程序，发现线程id转化为地址之后确实在堆栈之间的共享区中，因为是动态链接的pthread动态库。 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:9:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"clone系统调用 Linux提供了一个clone系统调用，作用是创建一个轻量级进程，和所属进程共享一个进程地址空间，而这个轻量级进程独立的栈结构就可以通过第二个参数void* child_stack传递。pthread_create内部可能就调用了这个clone系统调用。 ChatGPT： 在Linux中，进程对不同线程进行资源划分的方法是：通过Clone系统调用创建线程。Clone系统调用允许父进程在创建子进程时，指定如何共享资源。 每个线程都有一个进程控制块(PCB)，该PCB记录了该线程的信息，如线程ID、执行状态等。线程之间可以共享进程的一些资源，如文件描述符表、地址空间、进程ID等。同时，每个线程还有一些独有的资源，如线程栈、寄存器状态等。 因此，在Linux中，线程可以通过Clone系统调用进行资源划分，使得多个线程可以共享一些公共资源，同时又有独立的资源。这种资源划分方法有助于提高程序的效率，同时又保证了线程之间的隔离性。 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:10:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"__thread ：线程的局部存储 全局变量在全局数据区，是所有线程共享的。如果希望每个线程都有一份独立的某全局变量，则可以用__thread修饰该全局变量。这称为线程的局部存储。 其实本质上就是存储在了pthread库中每个线程属性集合内的线程局部存储空间中 GPT ：__thread可以用于修饰全局变量和局部变量，它表示线程本地存储（Thread Local Storage），即每个线程都有自己独立的存储空间，不与其他线程共享。使用__thread修饰的变量，在同一线程中可以直接访问，不需要加锁，效率更高。 #include \u003ciostream\u003e #include \u003cstring\u003e #include \u003cthread\u003e #include \u003csignal.h\u003e #include \u003cunistd.h\u003e #include \u003cstring\u003e using namespace std; // 验证线程id的本质 __thread int num = 6; void* routine(void* arg) {} int main() { pthread_t tid; pthread_create(\u0026tid, nullptr, routine, nullptr); int a = 10; int *p = new int(10); printf(\"栈区 : %p\\n\", \u0026a); printf(\"tid : %p\\n\", tid); printf(\"堆区 : %p\\n\", p); printf(\"num : %p\\n\", \u0026num); return 0; } .[yzl@VM-4-5-centos Thread]$ ./mythread 栈区 : 0x7ffdf25d43cc tid : 0x7f0b310f8700 堆区 : 0x1f3d260 num : 0x60105c [yzl@VM-4-5-centos Thread]$ make g++ -o mythread mythread.cc -std=c++11 -lpthread .[yzl@VM-4-5-centos Thread]$ ./mythread 栈区 : 0x7ffed17feccc tid : 0x7f56c3fb1700 堆区 : 0x67a270 num : 0x7f56c4fce77c 第一次运行时num没有被__thread修饰 浅谈C++线程库 #include \u003ciostream\u003e #include \u003cstring\u003e #include \u003cthread\u003e #include \u003csignal.h\u003e #include \u003cunistd.h\u003e // #include \u003cpthread.h\u003e #include \u003cstring\u003e // using namespace std; // C++线程库 void routine() { std::cout \u003c\u003c \"haha\" \u003c\u003c std::endl; } int main() { std::thread t1(routine); t1.join(); return 0; } [yzl@VM-4-5-centos Thread]$ ldd mythread linux-vdso.so.1 =\u003e (0x00007ffdf5fbd000) libpthread.so.0 =\u003e /lib64/libpthread.so.0 (0x00007fe163154000) libstdc++.so.6 =\u003e /lib64/libstdc++.so.6 (0x00007fe162e4c000) libm.so.6 =\u003e /lib64/libm.so.6 (0x00007fe162b4a000) libgcc_s.so.1 =\u003e /lib64/libgcc_s.so.1 (0x00007fe162934000) libc.so.6 =\u003e /lib64/libc.so.6 (0x00007fe162566000) /lib64/ld-linux-x86-64.so.2 (0x00007fe163370000) 如上所示，虽然这个程序没有包pthread.h头文件，但是这个可执行依旧链接了pthread动态库。因为使用了C++线程库。 故，C++线程库底层一定使用了原生线程库。（虽然pthread库不是系统调用，但对于Linux用户来说，地位上几乎已经和系统调用等价了），任何一个语言，要想在Linux下跑多线程，一定要使用原生线程库。 语言上的线程库，底层必须使用原生线程库，一定是对原生线程库进行封装，目的是让用户更好地使用。类似于C语言的FILE结构体内部一定包含文件描述符fd字段。 补充 OS是可以做到使进程进行资源的细粒度划分的。(完善Linux线程的原理) Linux通过内存管理机制和进程管理机制实现了进程对内存资源的细粒度划分。 vm_area_struct是内核内存管理的一个数据结构，它表示一个进程的虚拟内存地址空间中的一段连续区域（例如代码段、堆、栈等）。通过这个数据结构，系统可以对进程的内存资源进行管理和分配。（例如将代码段中的某连续区域或堆区的某块区域划分给不同的线程） 因此，vm_area_struct与进程资源的细粒度划分具有密切关系，它是实现进程内存资源管理的关键数据结构。（进程/线程在堆区不断开辟空间，进程地址空间中堆区只有一个start和end，是无法准确管理每次申请的堆区空间的，底层就是使用vm_area_struct对每一段空间进行管理） vm_area_struct结构包含了以下关键字段 vm_start：虚拟内存区域的起始地址 vm_end：虚拟内存区域的结束地址 vm_flags：标识虚拟内存区域的特征，如是否可读、可写、可执行等 vm_page_prot：虚拟内存区域的页保护标志 vm_file：如果这个虚拟内存区域是从文件映射而来，则指向该文件的指针；否则为NULL vm_pgoff：文件映射的偏移量 vm_ops：操作虚拟内存区域的函数指针 这些字段是vm_area_struct的核心内容，它们共同描述了进程的虚拟内存地址空间，以及如何管理和使用这些虚拟内存。 (和进程对不同线程进行资源划分联系起来 ","date":"0001-01-01","objectID":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/:11:0","tags":null,"title":"","uri":"/linux8.0_linux%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"categories":null,"content":"线程互斥 ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:0:0","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"线程互斥及相关概念 **线程互斥（Mutual Exclusion）**是指在多线程环境下，同一时刻只能有一个线程访问共享资源，以避免对该资源的不正确访问，造成数据不一致等问题。 例如，如果有多个线程都要同时对同一个全局变量进行修改，那么就需要使用线程互斥来保证对该变量的访问是互斥的，也就是说，在任意时刻只能有一个线程对该变量进行访问。 **临界资源（Critical Resource）**是指在多线程环境下需要被多个线程共享访问的资源，对该资源的访问需要进行同步（如使用互斥进行同步）以避免出现不正确的访问。 临界区：每个线程内部，访问临界资源的代码，就叫做临界区 互斥：任何时刻，互斥保证有且只有一个执行流进入临界区，访问临界资源，通常对临界资源起保护作用。是对临界资源保护的一种手段。 原子性：不会被任何调度机制影响的操作，该操作只有两态，要么完成，要么未完成。 互斥量mutex： 大部分情况，线程使用的数据都是局部变量，变量的地址空间在线程栈空间内，这种情况，变量归属单个线程，其他线程无法获得这种变量。 但有时候，很多变量都需要在线程间共享，这样的变量称为共享变量，可以通过数据的共享，完成线程之间的交互。 多个线程并发的操作共享变量，会带来一些问题。 ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:1:0","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"多线程抢票 #include \u003ciostream\u003e #include \u003cpthread.h\u003e #include \u003cstring\u003e #include \u003cunistd.h\u003e using namespace std; // 共享资源，多线程同时访问(未来的临界资源) int tickets = 10000; void* getTickets(void* args) { string* ps = (string*)args; while(true) { if(tickets \u003e 0) // 未来的临界区 { usleep(1000); printf(\"%s : %d\\n\", ps-\u003ec_str(), tickets); // 未来的临界区 // cout \u003c\u003c *ps \u003c\u003c \" get ticket \" \u003c\u003c tickets \u003c\u003c endl; tickets--; // 未来的临界区 } else { break; } } delete ps; return nullptr; } // 多线程抢票程序 int main() { pthread_t tid[3]; for(int i = 0; i \u003c 3; ++i) { string* ps = new string(\"thread\"); ps-\u003eoperator+=(to_string(i+1)); pthread_create(tid + i, nullptr, getTickets, (void*)ps); } for(int i = 0; i \u003c 3; ++i) { pthread_join(tid[i], nullptr); // printf(\"主线程等待回收新线程%d成功\\n\", i + 1); // cout \u003c\u003c \"主线程等待回收新线程\" \u003c\u003c i+1 \u003c\u003c \"成功\" \u003c\u003c endl; } return 0; } 上方程序为多线程抢票程序，全局数据tickets为共享资源（未来的临界资源，此时还没有进行互斥保护)，多个线程对getTickets函数进行了重入，getTickets方法中对全局tickets变量访问和修改的代码都是未来的临界区，如if判断，printf打印及tickets–代码都是未来的临界区代码。 因为多线程并发执行，访问共享资源，因时序及线程切换等原因造成的数据不一致等问题。我们则需要对访问共享资源的代码进行加锁保护，进行线程互斥。 为什么多个线程并发访问共享资源时，因为线程切换就会造成数据不一致呢？下面举两点解释说明： if(tickets \u003e 0)：tickets \u003e 0判断的本质也是计算，则该代码执行时需要CPU进行逻辑运算，tickets全局数据存储在内存中，则需要将tickets数据load到CPU寄存器中，本质就是将数据load到当前线程的上下文中。执行if判断的后面代码块时，因为多线程并发执行，此时的执行线程随时可能被切换（此时寄存器中的线程上下文数据也会被线程保存起来），则就可能造成多个执行线程同时进入if判断内部。若此时tickets为1，则就会因为线程切换造成tickets减到0甚至-1。（上方程序中的usleep更加提高了这种情况发生的可能性） tickets–：这条C语句在不进行优化的情况下翻译为汇编时，最少会变为三条：1. load到CPU寄存器中 2. 对寄存器内容进行– 3. 将寄存器数据load回内存中。因此这个–操作并不是原子的，执行到哪一步时都有可能进行线程切换。则存在以下场景:两个线程，线程1和线程2接下来要进行tickets–操作，此时 tickets为10，线程1执行完load到寄存器之后，被切换了，此时线程1会保存它的上下文数据，比如此时保存tickets的寄存器值，其他临时数据，程序计数器eip，栈顶栈底指针的值等。线程2执行tickets–的过程中没有被切换，此时内存中的tickets的值成功被–到了9。再切换为线程1，线程1执行第二步和第三步。此时内存中的tickets又被重复写入到了9。 造成上方多线程抢票程序问题的主要原因其实是第一点，第二点也会存在，只是概率相对更低。实际的执行的情况会比上方所述复杂的多，总之这样的多线程并发访问共享资源的程序是有问题的。 **因此我们需要进行线程互斥，常见的保护措施就是互斥锁。**使得加锁和解锁之间的代码区域同一时刻只能有一个线程执行，这样的代码区域称为临界区，tickets数据称为临界资源。 Linux上提供的这把锁叫互斥量。 ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:2:0","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"pthread线程库mutex互斥锁（互斥量） // 初始化互斥锁 // 1. 静态分配，适用于全局或静态的互斥锁 pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER; // 2. 动态分配，适用于局部的互斥锁 int pthread_mutex_init(pthread_mutex_t *restrict mutex, const pthread_mutexattr_t *restrict attr); // 参数二设为nullptr即可 // 销毁互斥锁 // 使用PTHREAD_MUTEX_INITIALIZER初始化的互斥锁不需要销毁 int pthread_mutex_destroy(pthread_mutex_t *mutex); // 加锁，解锁 int pthread_mutex_lock(pthread_mutex_t *mutex); int pthread_mutex_unlock(pthread_mutex_t *mutex); // int pthread_mutex_trylock(pthread_mutex_t *mutex); 调用pthread_mutex_lock时，可能出现以下情况。 互斥锁处于未锁状态，该函数会将互斥锁锁定，同时返回0（成功 调用时，互斥锁已经被其他线程锁定，或者有其他线程同时申请互斥锁且此线程没有竞争到互斥锁。则pthread_mutex_lock会将调用线程进行阻塞等待，等待其他线程解锁该互斥锁。 #include \u003ciostream\u003e #include \u003cpthread.h\u003e #include \u003cstring\u003e #include \u003cunistd.h\u003e using namespace std; // 共享资源，多线程同时访问(未来的临界资源) int tickets = 3000; // pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER; struct ThreadData { public: ThreadData(const string\u0026 tname, pthread_mutex_t* pmtx) : tname_(tname), pmtx_(pmtx) {} string tname_; pthread_mutex_t* pmtx_; }; void* getTickets(void* args) { ThreadData* td = (ThreadData*)args; while(true) { pthread_mutex_lock(td-\u003epmtx_); if(tickets \u003e 0) // 未来的临界区 { usleep(1000); printf(\"%s : %d\\n\", td-\u003etname_.c_str(), tickets); // 未来的临界区 // cout \u003c\u003c *ps \u003c\u003c \" get ticket \" \u003c\u003c tickets \u003c\u003c endl; tickets--; // 未来的临界区 pthread_mutex_unlock(td-\u003epmtx_); } else { pthread_mutex_unlock(td-\u003epmtx_); break; } // usleep(1000); } delete td; pthread_exit(nullptr); // return nullptr; } // 多线程抢票程序 int main() { pthread_t tid[3]; pthread_mutex_t mtx; pthread_mutex_init(\u0026mtx, nullptr); for(int i = 0; i \u003c 3; ++i) { string s(\"thread\"); s += to_string(i+1); ThreadData* td = new ThreadData(s, \u0026mtx); pthread_create(tid + i, nullptr, getTickets, (void*)td); } for(int i = 0; i \u003c 3; ++i) { pthread_join(tid[i], nullptr); // printf(\"主线程等待回收新线程%d成功\\n\", i + 1); // cout \u003c\u003c \"主线程等待回收新线程\" \u003c\u003c i+1 \u003c\u003c \"成功\" \u003c\u003c endl; } pthread_mutex_destroy(\u0026mtx); return 0; } 不加锁时，多线程并发执行，效率较高。加锁之后，同一时刻只会有一个线程执行临界区代码，其他线程都会在pthread_mutex_lock这里阻塞等待，等待这个锁被解锁。效率会降低。因此加锁的粒度越小越好。 加锁之后，线程在临界区内依旧会被切换，但是不会造成之前的数据不一致等问题。因为执行线程切换时，是持有锁被切换的（调用过pthread_mutex_lock），其他线程要想进入临界区执行临界区代码，也要先申请锁，此时它是申请不成功的，会阻塞等待持有锁线程解锁。因此同一时刻只会有一个线程进入临界区执行临界区代码访问临界资源。从而保证了临界区中数据的一致性。 加锁之后，临界区代码是串行执行的。而不是之前的多线程并发执行。 要进入临界区访问临界资源，每个线程必须先调用pthread_mutex_lock申请锁，则每个线程必须看到同一把锁\u0026\u0026访问它（pthread_mutex_t）。则锁本身就是一个共享资源（类比上方的tickets），那么如何保证多线程加锁时，访问锁的安全呢？ ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:3:0","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"互斥锁mutex的实现原理 共识： 在汇编的角度，我们认为一条汇编语句的执行是原子的，也就是要么执行完成，要么未执行。没有中间态。 在执行流视角，CPU内部的寄存器，本质叫做当前执行流的上下文。这些寄存器，空间上是被所有执行流共享的，但是寄存器的内容，是每一个执行流执行时私有的，叫做执行流的上下文。 上图为pthread_mutex_lock和pthread_mutex_unlock的伪代码。 为了实现互斥锁操作,大多数体系结构都提供了swap或exchange指令,该指令的作用是把寄存器和内存单元的数据相交换,由于只有一条指令，因此该操作是原子的，保证了操作的原子性。 lock：mutex可以理解为pthread_mutex_t互斥锁，初始化后，在内存中它的值为1。假设现有两个线程，线程a执行movb，将CPU寄存器%al的值置为0，然后被切换了（注意此时%al寄存器的内容是线程a的上下文，切换时要保存起来），线程b执行movb，exchange，将内存中mutex内存块存储的1和%al的0相交换（注意此操作是原子的），至此，线程b申请锁成功，后面会执行return语句。此时内存中mutex的值为0。线程切换为线程a，根据程序计数器的值，继续执行exchange语句，将%al的0和内存中的0交换（表示线程b申请锁失败，此时互斥锁已经被其他线程锁定了，竞争锁失败）。之后就会执行挂起等待，等待申请锁的线程执行unlock：将内存中mutex的值置为1，下次线程a执行goto lock，如果是第一个执行exhcnage %al，mutex语句的线程，则线程a竞争锁成功。 竞争锁的原理：其实根本上就是竞争锁时，利用exchange这样的指令的原子性，谁先执行exchange，将内存中1这样的公有数据变为线程上下文数据（私有数据），则表示竞争锁成功。 重新理解，线程在临界区内也会被切换，但是它是持有锁被切换的（那个内存中的1）。其他线程要想进入临界区比如申请锁，此时会申请失败阻塞等待，等待持有锁线程解锁（执行unlock） ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:4:0","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"可重入VS线程安全 线程安全：多个线程并发同一段代码时，不会出现不同的结果。常见对全局变量或者静态变量进行操作， 并且没有锁保护的情况下，会出现该问题。 重入：同一个函数被不同的执行流调用，当前一个流程还没有执行完，就有其他的执行流再次进入，我们称之为重入。一个函数在重入的情况下，运行结果不会出现任何不同或者任何问题，则该函数被称为可重 入函数，否则，是不可重入函数。 常见的线程不安全的情况 不保护共享变量的函数 函数状态随着被调用，状态发生变化的函数 返回指向静态变量指针的函数 调用线程不安全函数的函数 常见的线程安全的情况 每个线程对全局变量或者静态变量只有读取的权限，而没有写入的权限，一般来说这些线程是安全的 类或者接口对于线程来说都是原子操作 多个线程之间的切换不会导致该接口的执行结果存在二义性 常见不可重入的情况 调用了malloc/free函数，因为malloc函数是用全局链表来管理堆的 调用了标准I/O库函数，标准I/O库的很多实现都以不可重入的方式使用全局数据结构 可重入函数体内使用了静态的数据结构（函数状态随着调用而变化） 常见可重入的情况 不使用全局变量或静态变量 不使用用malloc或者new开辟出的空间 不调用不可重入函数 不返回静态或全局数据，所有数据都有函数的调用者提供 使用本地数据，或者通过制作全局数据的本地拷贝来保护全局数据（数据的局部存储） 可重入与线程安全的联系 函数是可重入的，那就是线程安全的 函数是不可重入的，那就不能由多个线程使用，有可能引发线程安全问题 如果一个函数中有全局变量，那么这个函数既不是线程安全也不是可重入的。 可重入与线程安全区别 可重入函数是线程安全函数的一种 线程安全不一定是可重入的，而可重入函数则一定是线程安全的。 如果将对临界资源的访问加上锁，则这个函数是线程安全的，但如果这个重入函数若锁还未释放则会产生死锁，因此是不可重入的。（第二点的一种情况） ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:5:0","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"死锁 死锁（Deadlock）是指两个或多个进程（或线程）在执行过程中，因互相等待对方释放资源而陷入无限等待的一种状态。 死锁是指在一组进程中的各个进程均占有不会释放的资源，但因互相申请被其他进程所站用不会释放的资 源而处于的一种永久等待状态。 例如，如果线程A获取了锁1，正在申请锁2，需要等待线程B释放锁2才能继续执行，而线程B同时获取了锁2，正在申请锁1，但需要等待线程A释放锁1才能继续执行，那么两个线程就会陷入无限等待的状态，即死锁。 死锁是一种非常严重的问题，因为它会导致应用程序无法继续执行，并可能导致系统崩溃。因此，在编写多线程或多进程的应用程序时，必须小心处理锁的获取和释放顺序，以避免死锁的发生。 ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:6:0","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"死锁的四个必要条件 互斥条件：一个资源每次只能被一个执行流使用（多线程互斥场景下使用了互斥锁） 请求与保持条件：一个执行流因请求资源而阻塞时，对已获得的资源保持不放 不剥夺条件:一个执行流已获得的资源，在末使用完之前，不能强行剥夺 循环等待条件:若干执行流之间形成一种头尾相接的循环等待资源的关系 ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:6:1","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"避免死锁 破坏死锁的四个必要条件 加锁顺序一致 避免锁未释放的场景 资源一次性分配 线程同步 ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:6:2","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"线程同步解决的问题 一、例如上方的多线程抢票程序，会发生一段时间甚至整个程序运行过程中都是一个线程在抢票，也就是某一个线程因为调度器调度的缘故一直抢到了锁，获取了临界资源。导致其他线程长时间访问不到临界资源，造成其他线程的饥饿问题。 GPT：执行流饥饿（Starvation）问题是指某个线程或进程无法获得所需的系统资源，导致它无法继续执行的一种情况。在并发编程中，如果多个线程或进程同时竞争一些共享资源，可能会出现某些线程或进程一直得不到访问共享资源的机会，导致它们无法执行或执行效率非常低下，这就是执行流饥饿问题。 二、线程在访问临界资源前需要加锁，这是为了保护临界资源，避免多线程并发访问导致数据不一致的问题。同时，在某些情况下，例如一个线程访问队列时，发现队列为空，它只能等待，直到其它线程将一个节点添加到队列中。这种情况就需要用到条件变量进行线程同步。 此情况下，队列为临界资源，线程在获取临界资源前需要先判断临界资源是否就绪，是否满足获取条件，而这种判断本质也是对临界资源的一种访问，故需要在加锁和解锁之间的互斥条件下进行。因此可能造成一种情况：线程加锁，判断临界资源（比如此情况下的队列）是否就绪，未就绪，解锁。因为它并不知道什么时候就绪，即队列中有新增节点（由另一个线程完成此工作），则该线程就需重复加锁，判断，解锁的工作。这个工作无疑是浪费锁资源，不合理的。 解决上方两种问题，我们就可以利用条件变量进行线程同步。 ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:7:0","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"线程同步 同步：在保证数据安全的前提下，让线程能够按照某种特定的顺序访问临界资源，从而有效避免饥饿问题，叫做同步。 竞态条件：因为时序问题，而导致程序异常，我们称之为竞态条件。在线程场景下，这种问题也不难理解。 当一个线程互斥地访问某个变量时，它可能发现在其它线程改变状态之前，它什么也做不了。 例如：一个线程访问队列时，发现队列为空，它只能等待，直到其它线程将一个节点添加到队列中。这种情况就需要用到条件变量。 ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:8:0","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"条件变量 // 条件变量的初始化与销毁 int pthread_cond_destroy(pthread_cond_t *cond); int pthread_cond_init(pthread_cond_t *restrict cond, const pthread_condattr_t *restrict attr); pthread_cond_t cond = PTHREAD_COND_INITIALIZER; // 使线程在cond条件变量下等待，mutex互斥锁 int pthread_cond_wait(pthread_cond_t *restrict cond, pthread_mutex_t *restrict mutex); // 唤醒在cond条件变量下等待的所有线程 int pthread_cond_broadcast(pthread_cond_t *cond); // 唤醒在cond条件变量下等待的一个线程 int pthread_cond_signal(pthread_cond_t *cond); pthread_cond_signal用于发送一个通知信号，唤醒等待在条件变量上的一个线程。如果有多个线程在等待，那么只有一个线程会被唤醒，并且系统并不保证哪个线程会被唤醒。因此pthread_cond_signal通常用于通知某个线程某资源已就绪，可以继续执行。 pthread_cond_broadcast用于发送广播通知信号，唤醒所有等待在条件变量上的线程。这意味着所有等待线程都会被唤醒，并且可以同时开始执行。因此，pthread_cond_broadcast通常用于通知多个线程可以同时开始执行。 #include \u003ciostream\u003e #include \u003cpthread.h\u003e #include \u003cstring\u003e #include \u003cfunctional\u003e #include \u003cunistd.h\u003e using namespace std; struct ThreadData; #define THREAD_NUM 5 typedef void (*func_t)(ThreadData*); // #define std::function\u003cvoid (ThreadData*)\u003e func bool quit = false; struct ThreadData { public: ThreadData(const string\u0026 name, pthread_mutex_t* pmtx, pthread_cond_t* pcond, func_t func) :name_(name), pmtx_(pmtx), pcond_(pcond), func_(func) {} string name_; pthread_mutex_t* pmtx_; pthread_cond_t* pcond_; func_t func_; }; void thread1(ThreadData* td) { while(!quit) { pthread_mutex_lock(td-\u003epmtx_); // 这里原本是要先判断临界资源是否就绪，若不就绪则wait等待 cout \u003c\u003c td-\u003ename_ \u003c\u003c \" wait\" \u003c\u003c endl; pthread_cond_wait(td-\u003epcond_, td-\u003epmtx_); cout \u003c\u003c td-\u003ename_ \u003c\u003c \" wait done\" \u003c\u003c endl; pthread_mutex_unlock(td-\u003epmtx_); sleep(1); } } void thread2(ThreadData* td) { while(!quit) { pthread_mutex_lock(td-\u003epmtx_); // 这里原本是要先判断临界资源是否就绪，若不就绪则wait等待 cout \u003c\u003c td-\u003ename_ \u003c\u003c \" wait\" \u003c\u003c endl; pthread_cond_wait(td-\u003epcond_, td-\u003epmtx_); cout \u003c\u003c td-\u003ename_ \u003c\u003c \" wait done\" \u003c\u003c endl; pthread_mutex_unlock(td-\u003epmtx_); sleep(1); } } void thread3(ThreadData* td) { while(!quit) { pthread_mutex_lock(td-\u003epmtx_); // 这里原本是要先判断临界资源是否就绪，若不就绪则wait等待 cout \u003c\u003c td-\u003ename_ \u003c\u003c \" wait\" \u003c\u003c endl; pthread_cond_wait(td-\u003epcond_, td-\u003epmtx_); cout \u003c\u003c td-\u003ename_ \u003c\u003c \" wait done\" \u003c\u003c endl; pthread_mutex_unlock(td-\u003epmtx_); sleep(1); } } void thread4(ThreadData* td) { while(!quit) { pthread_mutex_lock(td-\u003epmtx_); // 这里原本是要先判断临界资源是否就绪，若不就绪则wait等待 cout \u003c\u003c td-\u003ename_ \u003c\u003c \" wait\" \u003c\u003c endl; pthread_cond_wait(td-\u003epcond_, td-\u003epmtx_); cout \u003c\u003c td-\u003ename_ \u003c\u003c \" wait done\" \u003c\u003c endl; pthread_mutex_unlock(td-\u003epmtx_); sleep(1); } } void thread5(ThreadData* td) { while(!quit) { pthread_mutex_lock(td-\u003epmtx_); // 这里原本是要先判断临界资源是否就绪，若不就绪则wait等待 cout \u003c\u003c td-\u003ename_ \u003c\u003c \" wait\" \u003c\u003c endl; pthread_cond_wait(td-\u003epcond_, td-\u003epmtx_); cout \u003c\u003c td-\u003ename_ \u003c\u003c \" wait done\" \u003c\u003c endl; pthread_mutex_unlock(td-\u003epmtx_); sleep(1); } } void* entry(void* args) { ThreadData* td = (ThreadData*)args; td-\u003efunc_(td); delete td; pthread_exit(nullptr); } int main() { pthread_mutex_t mtx; pthread_cond_t cond; pthread_mutex_init(\u0026mtx, nullptr); pthread_cond_init(\u0026cond, nullptr); pthread_t tid[THREAD_NUM]; func_t funcs[THREAD_NUM] = {thread1, thread2, thread3, thread4, thread5}; for(int i = 0; i \u003c THREAD_NUM; ++i) { string name = \"thread\"; name += to_string(i + 1); ThreadData* td = new ThreadData(name, \u0026mtx, \u0026cond, funcs[i]); pthread_create(tid + i, nullptr, entry, (void*)td); } int cnt = 10; while(cnt != 0) { cnt--; sleep(2); pthread_cond_signal(\u0026cond); // pthread_cond_broadcast(\u0026cond); } cout \u003c\u003c \"main thread control done\" \u003c\u003c endl; quit = true; pthread_cond_broadcast(\u0026cond); for(int i = 0; i \u003c THREAD_NUM; ++i) { pthread_join(tid[i], nullptr); printf(\"main thread waits thread%d success\\n\", i+1); } pthread_mutex_destroy(\u0026mtx); pthread_cond_destroy(\u0026cond); return 0; } 现象结论 pthread_cond_siganl唤醒某条件变量下的一个线程时，并不是完全随机的，而是类似于在条件变量下组织了一个队列，按照等待顺序去唤醒。 pthread_cond_broadcast唤醒某条件变量下的全部线程时，确实是全部唤醒，但一次全部唤醒的内部也是有顺序的，也是按照等待顺序唤醒。 这就证实了利用条件变量进行线程同步时，可以让线程按照一定顺序访问临界资源，避免线程饥饿的问题。 上方这个条件变量的测试代码。仅仅是最简单的一种测试代码，实际上，这里的pthread_cond_wait和pthread_cond_signal的使用是完全生硬的使用测试。因为这里根本没有临界资源，更不要说判断临界资源是否就绪，若不就绪，则调用wait等待（正确的使用方式） 具体的条件变量的使用场景见生产者消费者模型，在恰当的场景下才能理解条件变量的正确使用方式。 ","date":"0001-01-01","objectID":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/:9:0","tags":null,"title":"","uri":"/linux8.1_%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5/"},{"categories":null,"content":"生产者消费者模型是什么 生产者消费者模型中有三个关键成员：生产者、消费者和缓冲区。 生产者（Producer）：生产者负责生产数据，并将数据存储到缓冲区中。在多线程场景中，生产者线程是用来执行生产者任务的线程。 消费者（Consumer）：消费者负责从缓冲区中取出数据，并消费数据。在多线程场景中，消费者线程是用来执行消费者任务的线程。 缓冲区（Buffer）：缓冲区是用来存储生产者生产的数据的地方，同时也是消费者从中取出数据的地方。缓冲区可以是一个数组、队列、链表等数据结构。在多线程场景中，缓冲区需要通过互斥和条件变量等同步机制来实现生产者和消费者之间的协调和同步。 生产者、消费者和缓冲区是生产者消费者模型中的三个关键成员，它们之间的协调和同步可以通过同步机制来实现。这种模型可以有效地实现资源的共享和利用，提高系统的效率。 便于记忆：321原则 一个交易场所：缓冲区 二个角色：生产者，消费者 三种关系：生产者和生产者：竞争，互斥关系。消费者和消费者：竞争，互斥关系。生产者和消费者：互斥，同步关系。（利用互斥锁和条件变量来实现。） 基于阻塞队列BlockingQueue的生产者消费者模型 在多线程编程中阻塞队列(Blocking Queue)是一种常用于实现生产者消费者模型的数据结构。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取， 其与普通的队列区别在于，当队列为空时，从队列获取元素的操作（消费者线程进行）将会被阻塞，直到队列中被放入了元素（生产者线程进行）；当队列满时，往队列里存放元素的操作（生产者线程进行）会被阻塞，直到有元素被从队列中取出（消费者线程进行）(以上的操作都是基于不同的线程来说的，线程在对阻塞队列进行操作时会被阻塞) 因为缓冲区实际上就是一个某种数据结构组织的内存空间，只是这里用队列来充当这个缓冲区，同时加了同步和互斥机制，所以是一个阻塞队列。 ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:0:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"阻塞队列生产消费模型代码实现 zzz ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:1:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"互斥锁\u0026共享变量 因为利用条件变量进行了线程同步，因此可以让线程能够按照某种特定的顺序访问临界资源，从而有效避免饥饿问题。（生产者和消费者调用pthread_cond_siganl时是按照条件变量的等待顺序进行唤醒的） 条件变量的使用需要环境，在生产者消费者模型中，生产者生产完数据之后，可以signal消费者，因为生产者知道缓冲区中新增数据了。消费者消费完数据之后，可以signal生产者，因为消费者知道缓冲区中有空间了。 为什么int pthread_cond_wait(pthread_cond_t *restrict cond, pthread_mutex_t *restrict mutex)的第二个参数是一个互斥锁：当一个线程调用此函数时，是因为它检测到了临界资源不就绪，不满足访问条件，因此它需要等待（例如消费者等待生产者生产数据），而对于临界资源的检测本身也是一种访问，故需要在互斥的条件下进行，也就是加锁和解锁之间。因此，当线程调用pthread_cond_wait时，需要传入一个互斥锁，pthread_cond_wait会先将互斥锁解锁，以便其它线程可以进入临界区访问临界资源。比如：消费者检测阻塞队列内没有数据，需要等待，但是它必须先把互斥锁进行解锁，生产者才能进入临界区生产数据。 pthread_cond_wait第二个参数是一个锁，当成功调用wait之后，传入的锁，会被自动释放！当线程被唤醒的时候，pthread_cond_wait，会自动帮助线程获取锁（要竞争锁）。由此可见，条件变量本身就是和互斥锁配合起来使用的。 pthread_cond_wait是一个函数，可能调用失败，也可能存在伪唤醒的情况，因此编码规范为：while (临界资源不就绪） pthread_cond_wait(cond, mutex); ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:2:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"为什么 pthread_cond_wait 需要互斥量（上方的第三点） 条件等待是线程间同步的一种手段，如果只有一个线程，条件不满足，一直等下去都不会满足，所以必须要有一个线程通过某些操作，改变共享变量，使原先不满足的条件变得满足，并且友好的通知等待在条件 变量上的线程。 条件不会无缘无故的突然变得满足了，必然会牵扯到共享数据的变化。所以一定要用互斥锁来保护。没有互斥锁就无法安全的获取和修改共享数据。 按照上面的说法，我们设计出如下的代码：先上锁，发现条件不满足，解锁，然后等待在条件变量上不就行了~（等待临界资源条件就绪，这个状态变化一定是由其他线程来完成的） 生产者消费者模型的优点 分离生产和消费，提高系统的解耦性和可维护性。 充分利用多核CPU的并行处理能力，从而提高系统的处理性能和响应速度。 注意：生产消费的过程并非只是生产者往缓冲区放数据，消费者从缓冲区拿数据。更重要和耗时的是生产者生产数据和消费者处理数据的过程。因为缓冲区放数据和拿数据是互斥的，所以这里的执行是串行执行的，并没有提高效率。真正提高效率的是，提高生产者生产数据和消费者处理数据的并发度。以避免出现生产过剩或消费不及的情况，从而使系统的负载保持在一个合理的范围内。 设计多生产者多消费者的目的也是为了提高生产数据和消费数据的并发度，当然，如果生产数据和消费数据的过程很简单，则多生产多消费的意义也就不大了。若过程是大量IO的过程，则可以提高整体效率。(IO密集型) 生产者消费者模式就是通过一个容器来解决生产者和消费者的强耦合问题。 **阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。**支持忙闲不均。 前面的生产消费代码，可以把生产者输入整数和消费者消费整数稍微修改一下，体现出生产者生产数据和消费者处理数据的过程… 略略略 信号量（POSIX信号量） ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:3:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"信号量的理解 在Linux下，POSIX信号量是一种线程同步机制，用于控制多个线程之间的访问顺序。POSIX信号量可以用于实现线程之间的互斥与同步。 在之前的阻塞队列生产者消费者模型中，阻塞队列是一个共享资源，不管是生产者还是消费者，任何时刻只允许一个线程访问共享资源，所以对阻塞队列进行了加锁保护，使阻塞队列成为为临界资源。在这里，是将整个共享资源（阻塞队列）当作一个整体使用的。 信号量使用思路与场景：一个共享资源，不当作整体使用，将其划分为若干个小的共享资源，使得不同的执行流访问不同的小共享资源，本质上，当不同执行流访问不同的小共享资源时，不需要进行加锁保护，可以并发执行。而如果两个执行流访问到了同一个小的共享资源时，再进行互斥保护共享资源。 POSIX信号量提供了两种类型的信号量 二进制信号量：只有两个取值：0和1，它用于表示资源是否被占用。 二进制信号量可以充当互斥锁的作用。因为信号量的初始值为1，p相当于加锁，v相当于解锁。 计数信号量：可以有多个取值，它表示可用资源的数量。 计数信号量的值通常被称为“资源计数器”，它记录了当前可用的共享资源数量。当一个线程需要使用共享资源时，它会尝试获取计数信号量，如果当前可用资源数量大于0，则线程可以获取资源并将计数信号量的值减1。如果当前可用资源数量为0，则表示共享资源已被占用，线程需要等待直到有资源可用。 ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:4:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"POSIX信号量接口 int sem_init(sem_t *sem, int pshared, unsigned int value); // 初始化 int sem_destroy(sem_t *sem); // 销毁 int sem_wait(sem_t *sem); // 申请信号量，P操作 int sem_post(sem_t *sem); // 释放信号量，V操作 因为信号量本质就是一个计数器，记录着当前的资源数目，其中初始化时的value值就表示初始时的资源数目。 在使用POSIX信号量时，线程可以通过调用sem_wait()函数来请求获取信号量，如果当前信号量的值大于0，则线程可以继续执行，同时信号量的值会减1；否则线程将被阻塞，等待信号量的值变为大于0。当线程释放资源时，可以通过调用sem_post()函数来释放信号量，使得其它线程可以获取资源。 基于环形队列的生产者消费者模型 基于环形队列的生产者消费者模型，对比之前的基于阻塞队列的生产者消费者模型，只是缓冲区的具体数据结构变了。 如上图，环节队列是逻辑抽象结构，底层的物理结构是数组。要想达到环形队列的效果，只需在访问数组的最后一个元素的下一个元素时将下标 %= size即可。再者，环形队列有一个判空和判满的问题，因为这里的容量是有上限的，从数据结构的角度来说，一般会有一个start和end下标，当start == end时为空，而当数据为满时，不能也将start == end标定为满，因为这样空和满的判断条件就相同了。解决方法：1. 空出一个位置，当(end + 1) % size == start时，判定为满。 2. 加一个计数器，当计数器 == size时为满，计数器 == 0时为空。 但是，因为信号量的作用，在环形队列生产消费模型中，我们不需要考虑判空判满的问题。见下文。 ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:5:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"结合生产者消费者 先考虑单生产者，单消费者的环形队列生产消费模型，有以下前提： 通过给生产者线程和消费者线程分别设定一个下标，从而标定它们当前生产和消费的下标位置。 我们不需要通过空出一个位置或者添加计数器的方式来避免队列为空和为满时生产消费线程的下标相同这样的判定条件重复。因为有信号量的存在。 因此，队列为空和为满时，生产者线程的下标一定等于消费者线程的下标。一种情况是当前缓冲区中没有数据，一种是当前缓冲区中数据已满。 目的期望： 当生产线程和消费线程指向了环形队列的同一个位置时（此时队列为空or为满），也就是访问了同一个共享资源，此时需要进行多线程互斥与同步。 当生产线程和消费线程不指向环形队列的同一个位置时，此时不需要进行互斥与同步，可以并发执行，因为没有访问同一个共享资源。 也就是： 当环形队列为空时，必须让生产者先执行，消费者必须等待生产者生产完至少一个数据。这也就是第一点中的互斥与同步。 当环形队列为满时，必须让消费者先执行，生产者必须等待消费者消费完至少一个数据。这也就是第一点中的互斥与同步。 当环形队列不为空不为满时，生产线程与消费线程并发执行。 ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:6:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"结合信号量 信号量表征的是共享资源的数量，在环形队列生产消费模型中，设定两个信号量，分别表征数据资源数目和空间资源数目。生产者关注空间资源信号量，初始为N。消费者关注数据资源信号量，初始为0。 按照上面生产者和消费者的执行逻辑来看，每次在生产或者消费之前必须先申请信号量，**申请信号量的本质是一种对资源的预定机制。**若信号量大于0，则–信号量，继续执行。若为0，则阻塞等待信号量被其他线程释放。 这样一来，起始时，或其他时刻当环形队列为空时，dataSem为0，消费者没有数据资源可以申请，故必须阻塞等待。这也就是我们期望的，即当生产和消费的下标相等时，且队列为空时，必须让生产者先执行，消费者阻塞等待。 当环形队列为满时，spaceSem为0，dataSem为N，生产者没有空间资源可以申请，故必须阻塞等待。这也就是我们期望的，当生产和消费下标相等时，且队列为满时，必须让消费者先执行，生产线程必须阻塞等待。 （注意，上方两种互斥情况下，只有当另一方生产/消费完，执行完V操作之后，阻塞方才能P成功，这也就实现了互斥和同步 当环形队列不为空时，生产和消费的下标不同，此时并没有访问同一个资源，故可以并发执行。 ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:7:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"结合多生产多消费 多生产多消费时，任何一个生产者和一个消费者之间都已经通过信号量达到了应有的互斥与同步，现在需要考虑的是生产生产之间和消费消费之间的互斥关系。 生产生产之间，消费消费之间，需要保护的共享资源为下标：c_index_或者p_index_，环形队列：ring_queue_（STL：vector，不是线程安全的），故，需要在push和pop内部，进行加锁，构建临界区，保护临界资源。 因为这里生产和消费之间的互斥，已经在当下标相等时，为空或为满时，由信号量维护了，所以这里用两把锁，分别维护生产生产之间和消费消费之间。（见代码实现） ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:8:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"环形队列生产消费模型代码实现 [RingQueueCP](Linux_System_Programming/Thread/RingQueueCP at main · DaysOfExperience/Linux_System_Programming (github.com)) ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:9:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"为什么效率高，优势（基于环形队列的生产消费模型） 我们可以看到，多个生产之间，在对环形队列push时，内部是互斥的，也就是串型执行的。多个消费之间，在对环形队列pop时，内部是互斥的，也是串行执行的。对比阻塞队列的生产消费模型来说，改进就是，当缓冲区，环形队列不为空且不为满时，生产和消费的push和pop可以并发执行。 还是那句话，生产消费的过程并非只是pushpop的过程（放数据和拿数据的过程），还有生产者放数据之前生产数据和消费者拿数据之后处理数据的过程（这里才是最耗时间的）。多生产多消费的意义是，可以让生产数据和处理数据时多个线程并发执行，从而提高效率。 （和基于阻塞队列的一样） ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:10:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"信号量的理解 信号量的本质就是一个计数器，表示当前资源数目。计数器的意义是什么呢？ 在之前使用互斥锁，条件变量的使用场景下，常规流程是：申请锁-\u003e判断临界资源是否就绪-\u003e就绪则访问，不就绪则释放锁并在条件变量下等待-\u003e资源就绪获取锁访问临界资源-\u003e释放锁。 **而信号量这个计数器的意义就是：可以不进入临界区就得知资源情况，减少了临界区内的资源是否就绪的判断。**信号量就是一种资源预定机制，当P操作成功时，就代表着此时有一个资源可以供你获取/访问（比如环形队列中的空间资源/数据资源）。 ","date":"0001-01-01","objectID":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:11:0","tags":null,"title":"","uri":"/linux8.2_%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":null,"content":"线程池: 一种线程使用模式。线程过多会带来调度开销，进而影响缓存局部性和整体性能。而线程池维护着多个线程，等待着监督管理者分配可并发执行的任务。**这避免了在处理短时间任务时创建与销毁线程的代价。****线程池不仅能够保证内核的充分利用，还能防止过分调度。**可用线程数量应该取决于可用的并发处理器、处理器内核、内存、网络sockets等的数量。 线程池的应用场景: 需要大量的线程来完成任务，且完成任务的时间比较短。 WEB服务器完成网页请求这样的任务，使用线程池技术是非常合适的。因为单个任务小，而任务数量巨大，你可以想象一个热门网站的点击次数。 但对于长时间的任务，比如一个Telnet连接请求，线程池的优点就不明显了。因为Telnet会话时间比线程的创建时间大多了。 对性能要求苛刻的应用 比如要求服务器迅速响应客户请求。 接受突发性的大量请求，但不至于使服务器因此产生大量线程的应用。 突发性大量客户请求，在没有线程池情况下，将产生大量线程，虽然理论上大部分操作系统线程数目最大值不是问题。但短时间内产生大量线程可能使内存到达极限， 出现错误 线程池的种类: 线程池示例: 创建固定数量线程的线程池，循环从任务队列中获取任务对象。 获取到任务对象后，执行任务对象中的任务接口 C++实现简单的线程池 tiny线程池 log.hpp就是一个打印日志的功能。mutex.hpp实现了一个RAII式的加锁解锁类，参数是一个锁指针。 task.hpp就是对线程池内线程执行的任务的一个简单模拟。main函数生产task，调用线程池的pushTask方法，将task放入线程池的任务队列中，线程池内等待任务的线程被signal然后执行任务。 thread.hpp对于线程的封装，线程的数据成员有线程名，执行例程方法，参数，tid。就是一个很简单的封装。并不是构造时直接pthread_create，而是需要调用一个create方法。线程池构造时构造若干个thread对象。线程池的run方法中调用线程的create方法，使线程执行routine等待任务队列中的任务。 线程池的基本原理就是生产消费模型，main函数充当生产者，放入线程池的任务队列缓冲区中，然后利用条件变量唤醒pthread_cond_wait的线程，线程执行任务。这里需要加锁保护临界资源，在生产消费之间，消费消费之间都需要加锁实现互斥(因为此处仅main在生产，故没有生产生产之间的关系，但是也实现了互斥)，还需要条件变量实现同步机制。 对于routine函数，因为类的普通成员方法有一个隐藏的this指针参数，因此不能直接作为线程的执行例程方法（void*(*p)(void*)），因此需要将其定为static，但是static方法不能访问非static数据成员。也就导致了不能直接访问task_queue_，mutex，cond等数据成员，如果想直接访问，需要加static。但是如果这样的话，所有线程池都共用一个任务列表，锁，条件变量是不合适的。 解决方法是：将routine设为static方法，然后线程传参时传一个线程池对象的this指针过去，static成员方法不能访问非static数据成员的原因就是因为没有this指针。现在传一个过去，这个routine就可以访问任何非static数据成员和非static方法了。 有关加锁，和条件变量的使用，就是常规的生产消费模型中锁和条件变量的使用。 单例模式 ","date":"0001-01-01","objectID":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/:0:0","tags":null,"title":"","uri":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"categories":null,"content":"单例模式是什么 单例模式是一种 “经典的, 常用的, 常考的” 设计模式. ","date":"0001-01-01","objectID":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/:1:0","tags":null,"title":"","uri":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"categories":null,"content":"单例模式的特点 某些类, 只应该具有一个对象(实例), 就称之为单例.例如一个男人只能有一个媳妇. 在很多服务器开发场景中, 经常需要让服务器加载很多的数据 (上百G) 到内存中. 此时往往要用一个单例的类来管理这些数据. ","date":"0001-01-01","objectID":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/:2:0","tags":null,"title":"","uri":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"categories":null,"content":"饿汉模式与懒汉模式 分为懒汉模式和饿汉模式。懒汉方式最核心的思想是 “延时加载”. 从而能够优化服务器的启动速度. ","date":"0001-01-01","objectID":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/:3:0","tags":null,"title":"","uri":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"categories":null,"content":"饿汉方式实现单例模式 template \u003ctypename T\u003e class Singleton { private: static T data; public: static T* GetInstance() { return \u0026data; } }; 只要通过 Singleton 这个包装类来使用 T 对象, 则一个进程中只有一个T对象的实例。 ","date":"0001-01-01","objectID":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/:3:1","tags":null,"title":"","uri":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"categories":null,"content":"懒汉方式实现单例模式 template \u003ctypename T\u003e class Singleton { static T* inst; public: static T* GetInstance() { if (inst == NULL) { inst = new T(); // 此时再进行构造此单例对象 } return inst; } }; 存在一个严重的问题, 线程不安全. 第一次调用 GetInstance 的时候, 如果两个线程同时调用, 可能会创建出两份 T 对象的实例. 但是后续再次调用, 就没有问题了. ","date":"0001-01-01","objectID":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/:3:2","tags":null,"title":"","uri":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"categories":null,"content":"懒汉方式实现单例模式(线程安全版本) // 懒汉模式, 线程安全 template \u003ctypename T\u003e class Singleton { volatile static T* inst; // 需要设置 volatile 关键字, 否则可能被编译器优化. static std::mutex lock; public: static T* GetInstance() { if (inst == NULL) { // 双重判定空指针, 降低锁冲突的概率, 提高性能. lock.lock(); // 使用互斥锁, 保证多线程情况下也只调用一次 new. if (inst == NULL) { inst = new T(); } lock.unlock(); } return inst; } }; 注意事项: 加锁解锁的位置 双重 if 判定, 避免不必要的锁竞争 volatile关键字防止过度优化 ","date":"0001-01-01","objectID":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/:3:3","tags":null,"title":"","uri":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"categories":null,"content":"线程池的单例模式 将构造设为private，拷贝和赋值设为delete。对外提供一个static方法，内部有一个static对象指针（设为单例模式的类的对象指针）。static方法用于获取这个static对象指针。也就是static ThreadPool* threadpool_ptr。在static接口getThreadPool中，实现懒汉模式逻辑。 getThreadPool有一个线程安全问题，也就是多线程同时调用此方法时，可能出现内存中创建出多份单例对象的情况。后面再调用时，因为threadpool_ptr指针不为nullptr，就不会出现这种情况了。 为了避免上述情况发生，需要加锁保护。 加锁保护之后，可以有一个优化。也就是再套一层判断。因为只有第一批同时进入这个函数的线程可能有安全问题，后面当对象创建出来之后，单例指针不为nullptr，再进入方法中不再需要加锁解锁这样的过程。因此又加了一层判断，减少后期无意义的竞争锁，提高效率。 STL,智能指针和线程安全 ","date":"0001-01-01","objectID":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/:4:0","tags":null,"title":"","uri":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"categories":null,"content":"STL中的容器是否是线程安全的? 不是. 原因是, STL的设计初衷是将性能挖掘到极致, 而一旦涉及到加锁保证线程安全, 会对性能造成巨大的影响. 而且对于不同的容器, 加锁方式的不同, 性能可能也不同(例如hash表的锁表和锁桶). 因此 STL 默认不是线程安全. 如果需要在多线程环境下使用, 往往需要调用者自行保证线程安全. ","date":"0001-01-01","objectID":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/:5:0","tags":null,"title":"","uri":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"categories":null,"content":"智能指针是否是线程安全的? 对于 unique_ptr, 由于只是在当前代码块范围内生效, 因此不涉及线程安全问题. 对于 shared_ptr, 多个对象需要共用一个引用计数变量, 所以会存在线程安全问题. 但是标准库实现的时候考虑到了这个问题, 基于原子操作(CAS)的方式保证shared_ptr能够高效, 原子的操作引用计数. 其他常见的各种锁 读者写者问题［选学］ ","date":"0001-01-01","objectID":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/:6:0","tags":null,"title":"","uri":"/linux8.3_%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"categories":null,"content":"随便看看喽~ 局域网 - 广域网 所谓 “局域网” 和 “广域网” 只是一个相对的概念. 比如, 我们有 “天朝特色” 的广域网, 也可以看做一个比较大的局域网. 协议 “协议” 是一种约定. 计算机之间的传输媒介是光信号和电信号. 通过 “频率” 和 “强弱” 来表示 0 和 1 这样的信息. 要想传递各种不同的信 息, 就需要约定好双方的数据格式. 计算机生产厂商有很多; 计算机操作系统, 也有很多; 计算机网络硬件设备, 还是有很多; 如何让这些不同厂商之间生产的计算机能够相互顺畅的通信? 就需要有人站出来, 约定一个共同的标准, 大家都来遵守, 这就是 网络协议; 网络协议初识 ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/:0:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"协议分层 分层最大的好处在于 “封装” ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/:1:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"OSI七层模型 OSI（Open System Interconnection，开放系统互连）七层网络模型称为开放式系统互联参考模型， 是一个逻辑上的定义和规范; 把网络从逻辑上分为了7层. 每一层都有相关、相对应的物理设备，比如路由器，交换机; OSI 七层模型是一种框架性的设计方法，其最主要的功能就是帮助不同类型的主机实现数据传输; 它的最大优点是将服务、接口和协议这三个概念明确地区分开来，概念清楚，理论也比较完整. 通过七个层次化的结构模型使不同的系统不同的网络之间实现可靠的通讯; 但是, 它既复杂又不实用; 所以我们按照TCP/IP四层模型来讲解. ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/:2:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"TCP/IP五层(或四层)模型 TCP/IP是一组协议的代名词，它还包括许多协议，组成了TCP/IP协议簇. TCP/IP通讯协议采用了5层的层级结构，每一层都呼叫它的下一层所提供的网络来完成自己的需求. 物理层: 负责光/电信号的传递方式. 比如现在以太网通用的网线(双绞线)、早期以太网采用的的同轴电缆 (现在主要用于有线电视)、光纤, 现在的wifi无线网使用电磁波等都属于物理层的概念。物理层的能力决定了最大传输速率、传输距离、抗干扰性等. 集线器(Hub)工作在物理层. 数据链路层: 负责设备之间的数据帧的传送和识别. 例如网卡设备的驱动、帧同步(就是说从网线上检测到什么信号算作新帧的开始)、冲突检测(如果检测到冲突就自动重发)、数据差错校验等工作. 有以太网、令牌环网, 无线LAN等标准. 交换机(Switch)工作在数据链路层. 网络层: 负责地址管理和路由选择. 例如在IP协议中, 通过IP地址来标识一台主机, 并通过路由表的方式规划出两台主机之间的数据传输的线路(路由). 路由器(Router)工作在网路层. 传输层: 负责两台主机之间的数据传输. 如传输控制协议 (TCP), 能够确保数据可靠的从源主机发送到目标主机. 应用层: 负责应用程序间沟通，如简单电子邮件传输（SMTP）、文件传输协议（FTP）、网络远程访问协议（Telnet）等. 我们的网络编程主要就是针对应用层. 网络传输基本流程 ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/:3:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"网络传输流程图 同一个网段内的两台主机进行文件传输 跨网段的主机的文件传输. 数据从一台计算机到另一台计算机传输过程中要经过一个或多个路由器 数据包封装和分用 不同的协议层对数据包有不同的称谓，在传输层叫做段(segment)，在网络层叫做数据报 (datagram)，在链路层叫做帧(frame). 应用层数据通过协议栈发到网络上时，每层协议都要加上一个数据首部(header)，称为封装 (Encapsulation)。首部信息中包含了一些类似于首部有多长, 载荷(payload)有多长, 上层协议是什么等信息 数据封装成帧后发到传输介质上（以太网/令牌环），到达目的主机后每层协议再剥掉相应的首部，根据首部中的 “上层协议 字段” 将数据交给对应的上层协议处理。 ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/:4:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"数据封装的过程 ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/:5:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"数据分用的过程 网络中的地址管理 ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/:6:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"认识IP地址 IP协议有两个版本, IPv4和IPv6. 我们整个的课程, 凡是提到IP协议, 没有特殊说明的, 默认都是指IPv4 IP地址是在IP协议中, 用来标识网络中不同主机的地址; 对于IPv4来说, IP地址是一个4字节, 32位的整数; 我们通常也使用 “点分十进制” 的字符串表示IP地址, 例如 192.168.0.1; 用点分割的每一个数字表示一个 字节, 范围是 0 - 255; ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/:7:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"认识MAC地址 MAC地址用来识别数据链路层中相连的节点; MAC地址长度为48位, 即6个字节. 一般用16进制数字加上冒号的形式来表示(例如: 08:00:27:03:fb:19) 在网卡出厂时就确定了, 不能修改. mac地址通常是唯一的(虚拟机中的mac地址不是真实的mac地址, 可能会冲突; 也有些网卡支持用户配置mac地址). ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/:8:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"预备知识 ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/:0:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/"},{"categories":null,"content":"理解源IP地址和目的IP地址 在IP数据包头部中, 有两个IP地址, 分别叫做源IP地址, 和目的IP地址. 思考: 我们光有IP地址就可以完成通信了嘛? 想象一下发qq消息的例子, 有了IP地址能够把消息发送到对方的机器上, 但是还需要有一个其他的标识来区分出, 这个数据要给哪个程序（进程）进行解析. ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/:1:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/"},{"categories":null,"content":"认识端口号 端口号(port)是传输层协议的内容. 端口号是一个2字节16位的整数; 端口号用来标识一个进程, 告诉操作系统, 当前的这个数据要交给哪一个进程来处理; IP地址 + 端口号能够标识网络上的某一台主机的某一个进程; 同一台主机上，一个端口号只能被一个进程占用. 另外, 一个进程可以绑定多个端口号; 但是一个端口号不能被多个进程绑定; 理解 “端口号” 和 “进程ID”：解耦 理解源端口号和目的端口号：传输层协议(TCP和UDP)的数据段中有两个端口号, 分别叫做源端口号和目的端口号. 就是在描述 “数据是哪个进程发的, 要发给哪个进程”; ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/:2:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/"},{"categories":null,"content":"认识TCP、UDP协议 此处我们先对TCP(Transmission Control Protocol 传输控制协议)有一个直观的认识; 后面我们再详细讨论TCP的一 些细节问题. 传输层协议 有连接 可靠传输 面向字节流 此处我们也是对UDP(User Datagram Protocol 用户数据报协议)有一个直观的认识; 后面再详细讨论. 传输层协议 无连接 不可靠传输 面向数据报 ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/:3:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/"},{"categories":null,"content":"网络字节序 我们已经知道,内存中的多字节数据相对于内存地址有大端和小端之分, 磁盘文件中的多字节数据相对于文件中的偏移地址也有大端小端之分, 网络数据流同样有大端小端之分. 那么如何定义网络数据流的地址呢? 发送主机通常将发送缓冲区中的数据按内存地址从低到高的顺序发出; 接收主机把从网络上接到的字节依次保存在接收缓冲区中,也是按内存地址从低到高的顺序保存; 因此,网络数据流的地址应这样规定:先发出的数据是低地址,后发出的数据是高地址. TCP/IP协议规定,网络数据流应采用大端字节序,即低地址高字节. 不管这台主机是大端机还是小端机, 都会按照这个TCP/IP规定的网络字节序来发送/接收数据; 如果当前发送主机是小端, 就需要先将数据转成大端; 否则就忽略, 直接发送即可; 为使网络程序具有可移植性,使同样的C代码在大端和小端计算机上编译后都能正常运行,可以调用以下库函数做网络字节序和主机字节序的转换。 这些函数名很好记,h表示host,n表示network,l表示32位长整数,s表示16位短整数。 例如htonl表示将32位的长整数从主机字节序转换为网络字节序,例如将IP地址转换后准备发送。 如果主机是小端字节序,这些函数将参数做相应的大小端转换然后返回; 如果主机是大端字节序,这些函数不做转换,将参数原封不动地返回。 socket编程接口 ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/:4:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/"},{"categories":null,"content":"socket 常见API // 创建 socket 文件描述符 (TCP/UDP, 客户端 + 服务器) int socket(int domain, int type, int protocol); // 绑定端口号 (TCP/UDP, 服务器) int bind(int socket, const struct sockaddr *address, socklen_t address_len); // 开始监听socket (TCP, 服务器) int listen(int socket, int backlog); // 接收请求 (TCP, 服务器) int accept(int socket, struct sockaddr* address, socklen_t* address_len); // 建立连接 (TCP, 客户端) int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); 前面讲过,struct sockaddr *是一个通用指针类型,myaddr参数实际上可以接受多种协议的sockaddr结构体,而它们的长度各不相同,所以需要第三个参数addrlen指定结构体的长度; ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/:5:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/"},{"categories":null,"content":"sockaddr结构 socket API是一层抽象的网络编程接口（程序通用性，兼容性比较好）,适用于各种底层网络协议,如IPv4、IPv6,以及后面要讲的UNIX Domain Socket. 然而, 各种网络协议的地址格式并不相同. IPv4和IPv6的地址格式定义在netinet/in.h中,IPv4地址用sockaddr_in结构体表示,包括16位地址类型, 16 位端口号和32位IP地址. IPv4、IPv6地址类型分别定义为常数AF_INET、AF_INET6. 这样,只要取得某种sockaddr结构体的首地址, 不需要知道具体是哪种类型的sockaddr结构体,就可以根据地址类型字段确定结构体中的内容. socket API可以都用struct sockaddr *类型表示, 在使用的时候需要强制转化成sockaddr_in; 这样的好 处是程序的通用性, 可以接收IPv4, IPv6, 以及UNIX Domain Socket各种类型的sockaddr结构体指针做为参数; ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/:6:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/"},{"categories":null,"content":"地址转换函数 本节只介绍基于IPv4的socket网络编程,sockaddr_in中的成员struct in_addr sin_addr表示32位 的IP 地址 但是我们通常用点分十进制的字符串表示IP 地址,以下函数可以在字符串表示和in_addr表示之间转换; inet_addr(_ip.c_str()) 服务端bind inet_ntoa(client.sin_addr) 服务端accept获取客户端连接 …………. 简单的UDP/TCP网络程序 hhhhhh 结合代码稍微一看就行了，不难且大概率不考~ ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/:7:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/"},{"categories":null,"content":"TCP socket API 详解 int socket(int domain, int type, int protocol) socket()打开一个网络通讯端口,如果成功的话,就像open()一样返回一个文件描述符;应用程序可以像读写文件一样用read/write在网络上收发数据。（一切皆文件） int bind(int socket, const struct sockaddr *address, socklen_t address_len) 服务器程序所监听的网络地址和端口号通常是固定不变的,客户端程序得知服务器程序的地址和端口号后 就可以向服务器发起连接; 服务器需要调用bind绑定一个固定的网络地址和端口号; bind()的作用是将参数sockfd和myaddr绑定在一起, 使sockfd这个用于网络通讯的文件描述符监听myaddr所描述的地址和端口号; 网络地址为INADDR_ANY, 这个宏表示本地的任意IP地址,因为服务器可能有多个网卡,每个网卡也可能绑定多个IP 地址, 这样设置可以在所有的IP地址上监听,直到与某个客户端建立了连接时才确定下来到底用哪个IP地址; int listen(int socket, int backlog) listen()声明sockfd处于监听状态, 并且最多允许有backlog个客户端处于连接等待状态, 如果接收到更多的连接请求就忽略, 具体细节同学们课后深入研究; int accept(int socket, struct sockaddr* address, socklen_t* address_len) 三次握手完成后, 服务器调用accept()接受连接; 如果服务器调用accept()时还没有客户端的连接请求,就阻塞等待直到有客户端连接上来; addr是一个输出型参数,accept()返回时传出客户端的地址和端口号; 如果给addr参数传NULL,表示不关心客户端的地址; addrlen参数是一个传入传出参数(value-result argument), 传入的是调用者提供的, 缓冲区addr的长度 以避免缓冲区溢出问题, 传出的是客户端地址结构体的实际长度(有可能没有占满调用者提供的缓冲区); int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); 客户端需要调用connect()连接服务器 connect和bind的参数形式一致, 区别在于bind的参数是自己的地址, 而connect的参数是对方的地址; 补充说明：关于bind函数的调用 由于客户端不需要固定的端口号,因此不必调用bind(),客户端的端口号由内核自动分配. 客户端不是不允许调用bind(), 只是没有必要调用bind()固定一个端口号. 否则如果在同一台机器上启动 多个客户端, 就会出现端口号被占用导致不能正确建立连接; 服务器也不是必须调用bind(), 但如果服务器不调用bind(), 内核会自动给服务器分配监听端口, 每次启动 服务器时端口号都不一样, 客户端要连接服务器就会遇到麻烦; ","date":"0001-01-01","objectID":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/:8:0","tags":null,"title":"","uri":"/linux%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/"}]